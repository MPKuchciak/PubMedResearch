{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def read_parquet_in_batches_with_progress(file_path, batch_size):\n",
    "    \"\"\"\n",
    "    Read a Parquet file in fixed-size row batches with a progress bar and per-chunk logging.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the Parquet file.\n",
    "        batch_size (int): Number of rows per batch.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame after processing all batches.\n",
    "    \"\"\"\n",
    "    # Open the Parquet file\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    \n",
    "    # Total number of rows in the file\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    \n",
    "    # Initialize a list to store DataFrame chunks\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    with tqdm(total=total_rows, desc=\"Processing Batches\", unit=\"rows\") as pbar:\n",
    "        # Enumerate batches for logging\n",
    "        for batch_number, batch in enumerate(parquet_file.iter_batches(batch_size=batch_size), start=1):\n",
    "            # Convert the batch to a Pandas DataFrame\n",
    "            df_batch = batch.to_pandas()\n",
    "            \n",
    "            # Simulate processing (add your custom logic here)\n",
    "            all_chunks.append(df_batch)\n",
    "            \n",
    "            # Update the progress bar\n",
    "            pbar.update(len(df_batch))\n",
    "            \n",
    "            # Print per-chunk information\n",
    "            print(f\"Processed Chunk {batch_number}: {len(df_batch)} rows\")\n",
    "    \n",
    "    # Combine all chunks into a single DataFrame\n",
    "    combined_df = pd.concat(all_chunks, ignore_index=True)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a4f06283c448d8904ee1e963001681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/1057871 [00:00<?, ?rows/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 1: 100000 rows\n",
      "Processed Chunk 2: 100000 rows\n",
      "Processed Chunk 3: 100000 rows\n",
      "Processed Chunk 4: 100000 rows\n",
      "Processed Chunk 5: 100000 rows\n",
      "Processed Chunk 6: 100000 rows\n",
      "Processed Chunk 7: 100000 rows\n",
      "Processed Chunk 8: 100000 rows\n",
      "Processed Chunk 9: 100000 rows\n",
      "Processed Chunk 10: 100000 rows\n",
      "Processed Chunk 11: 57871 rows\n",
      "\n",
      "Final DataFrame with 1057871 rows:\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"Data/2.Processed/ModellingData/P5_final_new.parquet\"\n",
    "    batch_size = 100_000  # Define your desired chunk size\n",
    "    \n",
    "    df = read_parquet_in_batches_with_progress(file_path, batch_size)\n",
    "    \n",
    "    print(f\"\\nFinal DataFrame with {len(df)} rows:\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_and_merge_in_batches(\n",
    "    df: pd.DataFrame,\n",
    "    batch_size: int,\n",
    "    output_folder: str,\n",
    "    final_filename: str = \"final_merged.parquet\",\n",
    "    temp_batch_prefix: str = \"temp_batch_\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits 'df' into multiple batches (size = batch_size), writes each batch to a Parquet file,\n",
    "    then merges them into one final Parquet, with a progress bar showing how many batches are done.\n",
    "\n",
    "    Steps:\n",
    "    ------\n",
    "    1) Creates subfolder 'temp_batches' in output_folder for batch files.\n",
    "    2) For each chunk of rows:\n",
    "       - Writes it to 'temp_batch_X.parquet'\n",
    "       - Increments a progress bar\n",
    "    3) Reads & merges all batch files into 'final_filename', then removes them.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str -> path to the final merged Parquet file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Subfolder for temporary batch files\n",
    "    temp_folder = os.path.join(output_folder, \"temp_batches\")\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "    total_rows = len(df)\n",
    "    batch_count = (total_rows + batch_size - 1) // batch_size\n",
    "    print(f\"Splitting DataFrame of {total_rows} rows into {batch_count} batches (size={batch_size}).\")\n",
    "\n",
    "    temp_files = []\n",
    "    current_row = 0\n",
    "    batch_index = 1\n",
    "\n",
    "    # -- 1) SAVE IN MULTIPLE BATCHES WITH A PROGRESS BAR FOR THE BATCHES --\n",
    "    with tqdm(total=batch_count, desc=\"Saving Batches\", unit=\"batch\") as pbar:\n",
    "        while current_row < total_rows:\n",
    "            end_row = min(current_row + batch_size, total_rows)\n",
    "            df_batch = df.iloc[current_row:end_row]\n",
    "\n",
    "            temp_file_name = f\"{temp_batch_prefix}{batch_index}.parquet\"\n",
    "            temp_file_path = os.path.join(temp_folder, temp_file_name)\n",
    "\n",
    "            # Write the chunk (one shot for each batch)\n",
    "            df_batch.to_parquet(temp_file_path, index=False, compression=\"snappy\")\n",
    "\n",
    "            temp_files.append(temp_file_path)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Optional: Print log\n",
    "            print(f\"  -> Batch {batch_index} rows [{current_row}:{end_row}] saved to {temp_file_path}\")\n",
    "\n",
    "            current_row = end_row\n",
    "            batch_index += 1\n",
    "\n",
    "    # -- 2) MERGE ALL BATCH FILES INTO A SINGLE PARQUET --\n",
    "    final_file_path = os.path.join(output_folder, final_filename)\n",
    "    print(f\"\\nMerging {len(temp_files)} batch files into {final_file_path}...\")\n",
    "\n",
    "    merged_parts = []\n",
    "    # Another progress bar for reading merges (optional)\n",
    "    with tqdm(total=len(temp_files), desc=\"Merging Batches\", unit=\"file\") as pbar_merge:\n",
    "        for file_path in temp_files:\n",
    "            merged_parts.append(pd.read_parquet(file_path))\n",
    "            pbar_merge.update(1)\n",
    "\n",
    "    df_merged = pd.concat(merged_parts, ignore_index=True)\n",
    "    df_merged.to_parquet(final_file_path, index=False, compression=\"snappy\")\n",
    "    print(f\"Final merged DataFrame saved as: {final_file_path}\\n\")\n",
    "\n",
    "    # -- 3) CLEAN UP TEMPORARY FILES --\n",
    "    for path in temp_files:\n",
    "        os.remove(path)\n",
    "    os.rmdir(temp_folder)\n",
    "\n",
    "    print(\"Temporary batch files removed. All done!\")\n",
    "    return final_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>keywords</th>\n",
       "      <th>coi_statement</th>\n",
       "      <th>parsed_date</th>\n",
       "      <th>...</th>\n",
       "      <th>cleaned_title_tokens_hf</th>\n",
       "      <th>cleaned_abstract_tokens_simple</th>\n",
       "      <th>cleaned_abstract_tokens_hf</th>\n",
       "      <th>disease_title_tokens_simple</th>\n",
       "      <th>disease_title_tokens_hf</th>\n",
       "      <th>disease_abstract_tokens_simple</th>\n",
       "      <th>disease_abstract_tokens_hf</th>\n",
       "      <th>disease_abstract_spacy</th>\n",
       "      <th>disease_title_spacy</th>\n",
       "      <th>disease_mesh_terms_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10186596</td>\n",
       "      <td>The potential impact of health care reform on ...</td>\n",
       "      <td>Journal of public health management and practi...</td>\n",
       "      <td>General: This article observes that, despite t...</td>\n",
       "      <td>Auerbach J; McGuire J</td>\n",
       "      <td>HIV/AIDS Bureau, Massachusetts Department of P...</td>\n",
       "      <td>Financing, Government; HIV Infections; Health ...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[[CLS], potential, impact, health, care, refor...</td>\n",
       "      <td>[general, article, observes, despite, clear, p...</td>\n",
       "      <td>[[CLS], general, article, observes, despite, c...</td>\n",
       "      <td>[hiv]</td>\n",
       "      <td>[hiv]</td>\n",
       "      <td>[hiv, aids]</td>\n",
       "      <td>[hiv, aids]</td>\n",
       "      <td>[human immunodeficiency virus (HIV) disease, a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[HIV Infections]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10186588</td>\n",
       "      <td>New Jersey health promotion and disease preven...</td>\n",
       "      <td>Journal of public health management and practi...</td>\n",
       "      <td>General: Health promotion is a major component...</td>\n",
       "      <td>Louria D B</td>\n",
       "      <td>Department of Preventive Medicine and Communit...</td>\n",
       "      <td>Female; Health Education; Health Promotion; Hu...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[[CLS], new, jersey, health, promotion, diseas...</td>\n",
       "      <td>[general, health, promotion, major, component,...</td>\n",
       "      <td>[[CLS], general, health, promotion, major, com...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10186587</td>\n",
       "      <td>Who will provide preventive services? The chan...</td>\n",
       "      <td>Journal of public health management and practi...</td>\n",
       "      <td>General: Health care reform in the United Stat...</td>\n",
       "      <td>Pearson T A; Spencer M; Jenkins P</td>\n",
       "      <td>Mary Imogene Bassett Research Institute, Coope...</td>\n",
       "      <td>Delivery of Health Care; Female; Health Care R...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[[CLS], provide, prevent, ##ive, services, ?, ...</td>\n",
       "      <td>[general, health, care, reform, united, states...</td>\n",
       "      <td>[[CLS], general, health, care, reform, united,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10163501</td>\n",
       "      <td>Cytoreduction of small intestine metastases us...</td>\n",
       "      <td>Journal of gynecologic surgery</td>\n",
       "      <td>General: The Cavitron Ultrasonic Surgical Aspi...</td>\n",
       "      <td>Adelson M D</td>\n",
       "      <td>Department of Obstetrics and Gynecology, Crous...</td>\n",
       "      <td>Adenocarcinoma; Fallopian Tube Neoplasms; Fema...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[[CLS], cy, ##tore, ##duction, small, int, ##e...</td>\n",
       "      <td>[general, cavitron, ultrasonic, surgical, aspi...</td>\n",
       "      <td>[[CLS], general, ca, ##vi, ##tron, ultra, ##so...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tumor]</td>\n",
       "      <td>[tumor]</td>\n",
       "      <td>[carcinoma of the ovary, and one each had, tub...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Adenocarcinoma, Neoplasms, Ovarian Neoplasms]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10157383</td>\n",
       "      <td>Racial differences in access to kidney transpl...</td>\n",
       "      <td>Health care financing review</td>\n",
       "      <td>General: Previous work has documented large di...</td>\n",
       "      <td>Eggers P W</td>\n",
       "      <td>Office of Research and Demonstrations, Health ...</td>\n",
       "      <td>Adolescent; Adult; Black or African American; ...</td>\n",
       "      <td>Empirical Approach; End Stage Renal Disease Pr...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[[CLS], racial, differences, access, kidney, t...</td>\n",
       "      <td>[general, previous, work, documented, large, d...</td>\n",
       "      <td>[[CLS], general, previous, work, documented, l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[renal failure, renal failure, end stage renal...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[American Kidney Failure]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid                                              title  \\\n",
       "0  10186596  The potential impact of health care reform on ...   \n",
       "1  10186588  New Jersey health promotion and disease preven...   \n",
       "2  10186587  Who will provide preventive services? The chan...   \n",
       "3  10163501  Cytoreduction of small intestine metastases us...   \n",
       "4  10157383  Racial differences in access to kidney transpl...   \n",
       "\n",
       "                                             journal  \\\n",
       "0  Journal of public health management and practi...   \n",
       "1  Journal of public health management and practi...   \n",
       "2  Journal of public health management and practi...   \n",
       "3                     Journal of gynecologic surgery   \n",
       "4                       Health care financing review   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  General: This article observes that, despite t...   \n",
       "1  General: Health promotion is a major component...   \n",
       "2  General: Health care reform in the United Stat...   \n",
       "3  General: The Cavitron Ultrasonic Surgical Aspi...   \n",
       "4  General: Previous work has documented large di...   \n",
       "\n",
       "                             authors  \\\n",
       "0              Auerbach J; McGuire J   \n",
       "1                         Louria D B   \n",
       "2  Pearson T A; Spencer M; Jenkins P   \n",
       "3                        Adelson M D   \n",
       "4                         Eggers P W   \n",
       "\n",
       "                                        affiliations  \\\n",
       "0  HIV/AIDS Bureau, Massachusetts Department of P...   \n",
       "1  Department of Preventive Medicine and Communit...   \n",
       "2  Mary Imogene Bassett Research Institute, Coope...   \n",
       "3  Department of Obstetrics and Gynecology, Crous...   \n",
       "4  Office of Research and Demonstrations, Health ...   \n",
       "\n",
       "                                          mesh_terms  \\\n",
       "0  Financing, Government; HIV Infections; Health ...   \n",
       "1  Female; Health Education; Health Promotion; Hu...   \n",
       "2  Delivery of Health Care; Female; Health Care R...   \n",
       "3  Adenocarcinoma; Fallopian Tube Neoplasms; Fema...   \n",
       "4  Adolescent; Adult; Black or African American; ...   \n",
       "\n",
       "                                            keywords coi_statement  \\\n",
       "0                                                              N/A   \n",
       "1                                                              N/A   \n",
       "2                                                              N/A   \n",
       "3                                                              N/A   \n",
       "4  Empirical Approach; End Stage Renal Disease Pr...           N/A   \n",
       "\n",
       "  parsed_date  ...                            cleaned_title_tokens_hf  \\\n",
       "0  1995-01-01  ...  [[CLS], potential, impact, health, care, refor...   \n",
       "1  1995-01-01  ...  [[CLS], new, jersey, health, promotion, diseas...   \n",
       "2  1995-01-01  ...  [[CLS], provide, prevent, ##ive, services, ?, ...   \n",
       "3  1995-01-01  ...  [[CLS], cy, ##tore, ##duction, small, int, ##e...   \n",
       "4  1995-01-01  ...  [[CLS], racial, differences, access, kidney, t...   \n",
       "\n",
       "                      cleaned_abstract_tokens_simple  \\\n",
       "0  [general, article, observes, despite, clear, p...   \n",
       "1  [general, health, promotion, major, component,...   \n",
       "2  [general, health, care, reform, united, states...   \n",
       "3  [general, cavitron, ultrasonic, surgical, aspi...   \n",
       "4  [general, previous, work, documented, large, d...   \n",
       "\n",
       "                          cleaned_abstract_tokens_hf  \\\n",
       "0  [[CLS], general, article, observes, despite, c...   \n",
       "1  [[CLS], general, health, promotion, major, com...   \n",
       "2  [[CLS], general, health, care, reform, united,...   \n",
       "3  [[CLS], general, ca, ##vi, ##tron, ultra, ##so...   \n",
       "4  [[CLS], general, previous, work, documented, l...   \n",
       "\n",
       "  disease_title_tokens_simple disease_title_tokens_hf  \\\n",
       "0                       [hiv]                   [hiv]   \n",
       "1                          []                      []   \n",
       "2                          []                      []   \n",
       "3                          []                      []   \n",
       "4                          []                      []   \n",
       "\n",
       "  disease_abstract_tokens_simple disease_abstract_tokens_hf  \\\n",
       "0                    [hiv, aids]                [hiv, aids]   \n",
       "1                             []                         []   \n",
       "2                             []                         []   \n",
       "3                        [tumor]                    [tumor]   \n",
       "4                             []                         []   \n",
       "\n",
       "                              disease_abstract_spacy disease_title_spacy  \\\n",
       "0  [human immunodeficiency virus (HIV) disease, a...                  []   \n",
       "1                                                 []                  []   \n",
       "2                                                 []                  []   \n",
       "3  [carcinoma of the ovary, and one each had, tub...                  []   \n",
       "4  [renal failure, renal failure, end stage renal...                  []   \n",
       "\n",
       "                         disease_mesh_terms_spacy  \n",
       "0                                [HIV Infections]  \n",
       "1                                              []  \n",
       "2                                              []  \n",
       "3  [Adenocarcinoma, Neoplasms, Ovarian Neoplasms]  \n",
       "4                       [American Kidney Failure]  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['uid', 'title', 'journal', 'abstract', 'authors', 'affiliations',\n",
       "       'mesh_terms', 'keywords', 'coi_statement', 'parsed_date',\n",
       "       'cleaned_title_tokens_simple', 'cleaned_title_tokens_hf',\n",
       "       'cleaned_abstract_tokens_simple', 'cleaned_abstract_tokens_hf',\n",
       "       'disease_title_tokens_simple', 'disease_title_tokens_hf',\n",
       "       'disease_abstract_tokens_simple', 'disease_abstract_tokens_hf',\n",
       "       'disease_abstract_spacy', 'disease_title_spacy',\n",
       "       'disease_mesh_terms_spacy'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"uid\",\"abstract\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10186596</td>\n",
       "      <td>General: This article observes that, despite t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10186588</td>\n",
       "      <td>General: Health promotion is a major component...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10186587</td>\n",
       "      <td>General: Health care reform in the United Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10163501</td>\n",
       "      <td>General: The Cavitron Ultrasonic Surgical Aspi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10157383</td>\n",
       "      <td>General: Previous work has documented large di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid                                           abstract\n",
       "0  10186596  General: This article observes that, despite t...\n",
       "1  10186588  General: Health promotion is a major component...\n",
       "2  10186587  General: Health care reform in the United Stat...\n",
       "3  10163501  General: The Cavitron Ultrasonic Surgical Aspi...\n",
       "4  10157383  General: Previous work has documented large di..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   9%|▉         | 100000/1057871 [00:21<03:29, 4578.60rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 1: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  19%|█▉        | 200000/1057871 [00:22<01:19, 10838.60rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 2: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  28%|██▊       | 300000/1057871 [00:22<00:39, 19344.51rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 3: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  38%|███▊      | 400000/1057871 [00:22<00:21, 30581.19rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 4: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  47%|████▋     | 500000/1057871 [00:23<00:12, 44734.72rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 5: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  57%|█████▋    | 600000/1057871 [00:23<00:07, 62032.36rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 6: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  66%|██████▌   | 700000/1057871 [00:24<00:04, 81757.41rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 7: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  76%|███████▌  | 800000/1057871 [00:24<00:02, 100706.17rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 8: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  85%|████████▌ | 900000/1057871 [00:25<00:01, 121595.46rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 9: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  95%|█████████▍| 1000000/1057871 [00:25<00:00, 140618.83rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 10: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 1057871/1057871 [00:26<00:00, 40521.84rows/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 11: 57871 rows\n",
      "\n",
      "Final DataFrame with 1057871 rows:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"Data/2.Processed/ModellingData/P2_abstract.parquet\"\n",
    "    batch_size = 100_000  # Define your desired chunk size\n",
    "    \n",
    "    df = read_parquet_in_batches_with_progress(file_path, batch_size)\n",
    "    \n",
    "    print(f\"\\nFinal DataFrame with {len(df)} rows:\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10186596</td>\n",
       "      <td>General: This article observes that, despite t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10186588</td>\n",
       "      <td>General: Health promotion is a major component...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10186587</td>\n",
       "      <td>General: Health care reform in the United Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10163501</td>\n",
       "      <td>General: The Cavitron Ultrasonic Surgical Aspi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10157383</td>\n",
       "      <td>General: Previous work has documented large di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid                                           abstract\n",
       "0  10186596  General: This article observes that, despite t...\n",
       "1  10186588  General: Health promotion is a major component...\n",
       "2  10186587  General: Health care reform in the United Stat...\n",
       "3  10163501  General: The Cavitron Ultrasonic Surgical Aspi...\n",
       "4  10157383  General: Previous work has documented large di..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING MODELLING APPROACH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing in batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting DF with 1057871 rows into 106 batches of size 10000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:   1%|          | 1/106 [00:00<00:24,  4.20batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 1 rows [0:10000] to Data/2.Processed/SentimentAnalysis\\chunk_1.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:   2%|▏         | 2/106 [00:00<00:25,  4.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 2 rows [10000:20000] to Data/2.Processed/SentimentAnalysis\\chunk_2.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:   3%|▎         | 3/106 [00:00<00:25,  4.04batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 3 rows [20000:30000] to Data/2.Processed/SentimentAnalysis\\chunk_3.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:   4%|▍         | 4/106 [00:00<00:24,  4.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 4 rows [30000:40000] to Data/2.Processed/SentimentAnalysis\\chunk_4.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:   5%|▍         | 5/106 [00:01<00:24,  4.17batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 5 rows [40000:50000] to Data/2.Processed/SentimentAnalysis\\chunk_5.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:   6%|▌         | 6/106 [00:01<00:24,  4.02batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 6 rows [50000:60000] to Data/2.Processed/SentimentAnalysis\\chunk_6.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:   7%|▋         | 7/106 [00:01<00:23,  4.13batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 7 rows [60000:70000] to Data/2.Processed/SentimentAnalysis\\chunk_7.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:   8%|▊         | 8/106 [00:01<00:23,  4.17batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 8 rows [70000:80000] to Data/2.Processed/SentimentAnalysis\\chunk_8.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:   8%|▊         | 9/106 [00:02<00:23,  4.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 9 rows [80000:90000] to Data/2.Processed/SentimentAnalysis\\chunk_9.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:   9%|▉         | 10/106 [00:02<00:23,  4.06batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 10 rows [90000:100000] to Data/2.Processed/SentimentAnalysis\\chunk_10.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  10%|█         | 11/106 [00:02<00:23,  4.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 11 rows [100000:110000] to Data/2.Processed/SentimentAnalysis\\chunk_11.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  11%|█▏        | 12/106 [00:02<00:22,  4.17batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 12 rows [110000:120000] to Data/2.Processed/SentimentAnalysis\\chunk_12.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  12%|█▏        | 13/106 [00:03<00:22,  4.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 13 rows [120000:130000] to Data/2.Processed/SentimentAnalysis\\chunk_13.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  13%|█▎        | 14/106 [00:03<00:22,  4.17batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 14 rows [130000:140000] to Data/2.Processed/SentimentAnalysis\\chunk_14.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  14%|█▍        | 15/106 [00:03<00:21,  4.17batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 15 rows [140000:150000] to Data/2.Processed/SentimentAnalysis\\chunk_15.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  15%|█▌        | 16/106 [00:03<00:21,  4.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 16 rows [150000:160000] to Data/2.Processed/SentimentAnalysis\\chunk_16.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  16%|█▌        | 17/106 [00:04<00:22,  3.98batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 17 rows [160000:170000] to Data/2.Processed/SentimentAnalysis\\chunk_17.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  17%|█▋        | 18/106 [00:04<00:21,  4.06batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 18 rows [170000:180000] to Data/2.Processed/SentimentAnalysis\\chunk_18.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  18%|█▊        | 19/106 [00:04<00:21,  4.01batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 19 rows [180000:190000] to Data/2.Processed/SentimentAnalysis\\chunk_19.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  19%|█▉        | 20/106 [00:04<00:21,  4.08batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 20 rows [190000:200000] to Data/2.Processed/SentimentAnalysis\\chunk_20.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  20%|█▉        | 21/106 [00:05<00:21,  3.93batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 21 rows [200000:210000] to Data/2.Processed/SentimentAnalysis\\chunk_21.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  21%|██        | 22/106 [00:05<00:21,  3.90batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 22 rows [210000:220000] to Data/2.Processed/SentimentAnalysis\\chunk_22.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  22%|██▏       | 23/106 [00:05<00:21,  3.90batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 23 rows [220000:230000] to Data/2.Processed/SentimentAnalysis\\chunk_23.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  23%|██▎       | 24/106 [00:05<00:20,  4.01batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 24 rows [230000:240000] to Data/2.Processed/SentimentAnalysis\\chunk_24.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  24%|██▎       | 25/106 [00:06<00:19,  4.08batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 25 rows [240000:250000] to Data/2.Processed/SentimentAnalysis\\chunk_25.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  25%|██▍       | 26/106 [00:06<00:19,  4.15batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 26 rows [250000:260000] to Data/2.Processed/SentimentAnalysis\\chunk_26.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  25%|██▌       | 27/106 [00:06<00:18,  4.18batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 27 rows [260000:270000] to Data/2.Processed/SentimentAnalysis\\chunk_27.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  26%|██▋       | 28/106 [00:06<00:18,  4.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 28 rows [270000:280000] to Data/2.Processed/SentimentAnalysis\\chunk_28.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  27%|██▋       | 29/106 [00:07<00:18,  4.11batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 29 rows [280000:290000] to Data/2.Processed/SentimentAnalysis\\chunk_29.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  28%|██▊       | 30/106 [00:07<00:19,  3.97batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 30 rows [290000:300000] to Data/2.Processed/SentimentAnalysis\\chunk_30.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  29%|██▉       | 31/106 [00:07<00:18,  4.03batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 31 rows [300000:310000] to Data/2.Processed/SentimentAnalysis\\chunk_31.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  30%|███       | 32/106 [00:07<00:18,  4.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 32 rows [310000:320000] to Data/2.Processed/SentimentAnalysis\\chunk_32.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  31%|███       | 33/106 [00:08<00:17,  4.15batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 33 rows [320000:330000] to Data/2.Processed/SentimentAnalysis\\chunk_33.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  32%|███▏      | 34/106 [00:08<00:17,  4.14batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 34 rows [330000:340000] to Data/2.Processed/SentimentAnalysis\\chunk_34.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  33%|███▎      | 35/106 [00:08<00:17,  4.10batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 35 rows [340000:350000] to Data/2.Processed/SentimentAnalysis\\chunk_35.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  34%|███▍      | 36/106 [00:08<00:17,  4.01batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 36 rows [350000:360000] to Data/2.Processed/SentimentAnalysis\\chunk_36.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  35%|███▍      | 37/106 [00:09<00:17,  3.93batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 37 rows [360000:370000] to Data/2.Processed/SentimentAnalysis\\chunk_37.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  36%|███▌      | 38/106 [00:09<00:17,  3.99batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 38 rows [370000:380000] to Data/2.Processed/SentimentAnalysis\\chunk_38.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  37%|███▋      | 39/106 [00:09<00:16,  4.04batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 39 rows [380000:390000] to Data/2.Processed/SentimentAnalysis\\chunk_39.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  38%|███▊      | 40/106 [00:09<00:16,  4.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 40 rows [390000:400000] to Data/2.Processed/SentimentAnalysis\\chunk_40.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  39%|███▊      | 41/106 [00:10<00:15,  4.11batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 41 rows [400000:410000] to Data/2.Processed/SentimentAnalysis\\chunk_41.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  40%|███▉      | 42/106 [00:10<00:15,  4.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 42 rows [410000:420000] to Data/2.Processed/SentimentAnalysis\\chunk_42.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  41%|████      | 43/106 [00:10<00:16,  3.92batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 43 rows [420000:430000] to Data/2.Processed/SentimentAnalysis\\chunk_43.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  42%|████▏     | 44/106 [00:10<00:16,  3.83batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 44 rows [430000:440000] to Data/2.Processed/SentimentAnalysis\\chunk_44.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  42%|████▏     | 45/106 [00:11<00:15,  3.92batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 45 rows [440000:450000] to Data/2.Processed/SentimentAnalysis\\chunk_45.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  43%|████▎     | 46/106 [00:11<00:15,  3.98batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 46 rows [450000:460000] to Data/2.Processed/SentimentAnalysis\\chunk_46.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  44%|████▍     | 47/106 [00:11<00:14,  4.00batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 47 rows [460000:470000] to Data/2.Processed/SentimentAnalysis\\chunk_47.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  45%|████▌     | 48/106 [00:11<00:14,  4.00batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 48 rows [470000:480000] to Data/2.Processed/SentimentAnalysis\\chunk_48.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  46%|████▌     | 49/106 [00:12<00:14,  3.87batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 49 rows [480000:490000] to Data/2.Processed/SentimentAnalysis\\chunk_49.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  47%|████▋     | 50/106 [00:12<00:14,  3.76batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 50 rows [490000:500000] to Data/2.Processed/SentimentAnalysis\\chunk_50.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  48%|████▊     | 51/106 [00:12<00:14,  3.81batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 51 rows [500000:510000] to Data/2.Processed/SentimentAnalysis\\chunk_51.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  49%|████▉     | 52/106 [00:12<00:13,  3.86batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 52 rows [510000:520000] to Data/2.Processed/SentimentAnalysis\\chunk_52.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  50%|█████     | 53/106 [00:13<00:13,  3.91batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 53 rows [520000:530000] to Data/2.Processed/SentimentAnalysis\\chunk_53.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  51%|█████     | 54/106 [00:13<00:13,  3.86batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 54 rows [530000:540000] to Data/2.Processed/SentimentAnalysis\\chunk_54.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  52%|█████▏    | 55/106 [00:13<00:13,  3.67batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 55 rows [540000:550000] to Data/2.Processed/SentimentAnalysis\\chunk_55.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  53%|█████▎    | 56/106 [00:14<00:14,  3.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 56 rows [550000:560000] to Data/2.Processed/SentimentAnalysis\\chunk_56.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  54%|█████▍    | 57/106 [00:14<00:13,  3.60batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 57 rows [560000:570000] to Data/2.Processed/SentimentAnalysis\\chunk_57.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  55%|█████▍    | 58/106 [00:14<00:13,  3.68batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 58 rows [570000:580000] to Data/2.Processed/SentimentAnalysis\\chunk_58.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  56%|█████▌    | 59/106 [00:14<00:12,  3.72batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 59 rows [580000:590000] to Data/2.Processed/SentimentAnalysis\\chunk_59.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  57%|█████▋    | 60/106 [00:15<00:12,  3.80batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 60 rows [590000:600000] to Data/2.Processed/SentimentAnalysis\\chunk_60.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  58%|█████▊    | 61/106 [00:15<00:12,  3.65batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 61 rows [600000:610000] to Data/2.Processed/SentimentAnalysis\\chunk_61.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  58%|█████▊    | 62/106 [00:15<00:12,  3.50batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 62 rows [610000:620000] to Data/2.Processed/SentimentAnalysis\\chunk_62.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  59%|█████▉    | 63/106 [00:15<00:11,  3.64batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 63 rows [620000:630000] to Data/2.Processed/SentimentAnalysis\\chunk_63.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  60%|██████    | 64/106 [00:16<00:11,  3.67batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 64 rows [630000:640000] to Data/2.Processed/SentimentAnalysis\\chunk_64.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  61%|██████▏   | 65/106 [00:16<00:11,  3.68batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 65 rows [640000:650000] to Data/2.Processed/SentimentAnalysis\\chunk_65.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  62%|██████▏   | 66/106 [00:16<00:10,  3.67batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 66 rows [650000:660000] to Data/2.Processed/SentimentAnalysis\\chunk_66.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  63%|██████▎   | 67/106 [00:17<00:11,  3.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 67 rows [660000:670000] to Data/2.Processed/SentimentAnalysis\\chunk_67.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  64%|██████▍   | 68/106 [00:17<00:11,  3.36batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 68 rows [670000:680000] to Data/2.Processed/SentimentAnalysis\\chunk_68.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  65%|██████▌   | 69/106 [00:17<00:10,  3.44batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 69 rows [680000:690000] to Data/2.Processed/SentimentAnalysis\\chunk_69.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  66%|██████▌   | 70/106 [00:17<00:10,  3.54batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 70 rows [690000:700000] to Data/2.Processed/SentimentAnalysis\\chunk_70.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  67%|██████▋   | 71/106 [00:18<00:09,  3.53batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 71 rows [700000:710000] to Data/2.Processed/SentimentAnalysis\\chunk_71.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  68%|██████▊   | 72/106 [00:18<00:09,  3.56batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 72 rows [710000:720000] to Data/2.Processed/SentimentAnalysis\\chunk_72.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  69%|██████▉   | 73/106 [00:18<00:09,  3.34batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 73 rows [720000:730000] to Data/2.Processed/SentimentAnalysis\\chunk_73.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  70%|██████▉   | 74/106 [00:19<00:10,  3.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 74 rows [730000:740000] to Data/2.Processed/SentimentAnalysis\\chunk_74.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  71%|███████   | 75/106 [00:19<00:09,  3.26batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 75 rows [740000:750000] to Data/2.Processed/SentimentAnalysis\\chunk_75.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  72%|███████▏  | 76/106 [00:19<00:09,  3.32batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 76 rows [750000:760000] to Data/2.Processed/SentimentAnalysis\\chunk_76.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  73%|███████▎  | 77/106 [00:20<00:08,  3.38batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 77 rows [760000:770000] to Data/2.Processed/SentimentAnalysis\\chunk_77.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  74%|███████▎  | 78/106 [00:20<00:08,  3.46batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 78 rows [770000:780000] to Data/2.Processed/SentimentAnalysis\\chunk_78.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  75%|███████▍  | 79/106 [00:20<00:08,  3.28batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 79 rows [780000:790000] to Data/2.Processed/SentimentAnalysis\\chunk_79.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  75%|███████▌  | 80/106 [00:20<00:07,  3.30batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 80 rows [790000:800000] to Data/2.Processed/SentimentAnalysis\\chunk_80.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  76%|███████▋  | 81/106 [00:21<00:07,  3.35batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 81 rows [800000:810000] to Data/2.Processed/SentimentAnalysis\\chunk_81.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  77%|███████▋  | 82/106 [00:21<00:07,  3.35batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 82 rows [810000:820000] to Data/2.Processed/SentimentAnalysis\\chunk_82.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  78%|███████▊  | 83/106 [00:21<00:06,  3.40batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 83 rows [820000:830000] to Data/2.Processed/SentimentAnalysis\\chunk_83.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  79%|███████▉  | 84/106 [00:22<00:06,  3.47batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 84 rows [830000:840000] to Data/2.Processed/SentimentAnalysis\\chunk_84.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  80%|████████  | 85/106 [00:22<00:06,  3.41batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 85 rows [840000:850000] to Data/2.Processed/SentimentAnalysis\\chunk_85.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  81%|████████  | 86/106 [00:22<00:05,  3.35batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 86 rows [850000:860000] to Data/2.Processed/SentimentAnalysis\\chunk_86.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  82%|████████▏ | 87/106 [00:22<00:05,  3.42batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 87 rows [860000:870000] to Data/2.Processed/SentimentAnalysis\\chunk_87.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  83%|████████▎ | 88/106 [00:23<00:05,  3.42batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 88 rows [870000:880000] to Data/2.Processed/SentimentAnalysis\\chunk_88.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  84%|████████▍ | 89/106 [00:23<00:04,  3.45batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 89 rows [880000:890000] to Data/2.Processed/SentimentAnalysis\\chunk_89.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  85%|████████▍ | 90/106 [00:23<00:04,  3.49batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 90 rows [890000:900000] to Data/2.Processed/SentimentAnalysis\\chunk_90.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  86%|████████▌ | 91/106 [00:24<00:04,  3.33batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 91 rows [900000:910000] to Data/2.Processed/SentimentAnalysis\\chunk_91.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  87%|████████▋ | 92/106 [00:24<00:04,  3.30batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 92 rows [910000:920000] to Data/2.Processed/SentimentAnalysis\\chunk_92.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  88%|████████▊ | 93/106 [00:24<00:03,  3.33batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 93 rows [920000:930000] to Data/2.Processed/SentimentAnalysis\\chunk_93.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  89%|████████▊ | 94/106 [00:25<00:03,  3.37batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 94 rows [930000:940000] to Data/2.Processed/SentimentAnalysis\\chunk_94.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  90%|████████▉ | 95/106 [00:25<00:03,  3.32batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 95 rows [940000:950000] to Data/2.Processed/SentimentAnalysis\\chunk_95.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  91%|█████████ | 96/106 [00:25<00:02,  3.37batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 96 rows [950000:960000] to Data/2.Processed/SentimentAnalysis\\chunk_96.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  92%|█████████▏| 97/106 [00:25<00:02,  3.24batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 97 rows [960000:970000] to Data/2.Processed/SentimentAnalysis\\chunk_97.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  92%|█████████▏| 98/106 [00:26<00:02,  3.19batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 98 rows [970000:980000] to Data/2.Processed/SentimentAnalysis\\chunk_98.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  93%|█████████▎| 99/106 [00:26<00:02,  3.24batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 99 rows [980000:990000] to Data/2.Processed/SentimentAnalysis\\chunk_99.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  94%|█████████▍| 100/106 [00:26<00:01,  3.27batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 100 rows [990000:1000000] to Data/2.Processed/SentimentAnalysis\\chunk_100.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  95%|█████████▌| 101/106 [00:27<00:01,  3.32batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 101 rows [1000000:1010000] to Data/2.Processed/SentimentAnalysis\\chunk_101.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  96%|█████████▌| 102/106 [00:27<00:01,  3.32batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 102 rows [1010000:1020000] to Data/2.Processed/SentimentAnalysis\\chunk_102.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  97%|█████████▋| 103/106 [00:27<00:00,  3.23batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 103 rows [1020000:1030000] to Data/2.Processed/SentimentAnalysis\\chunk_103.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  98%|█████████▊| 104/106 [00:28<00:00,  3.21batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 104 rows [1030000:1040000] to Data/2.Processed/SentimentAnalysis\\chunk_104.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  99%|█████████▉| 105/106 [00:28<00:00,  3.24batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 105 rows [1040000:1050000] to Data/2.Processed/SentimentAnalysis\\chunk_105.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches: 100%|██████████| 106/106 [00:28<00:00,  3.69batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Saved batch 106 rows [1050000:1057871] to Data/2.Processed/SentimentAnalysis\\chunk_106.parquet\n",
      "\n",
      "All done. Each batch is in its own .parquet file in: Data/2.Processed/SentimentAnalysis\n",
      "Done splitting!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# SCRIPT 1: split_into_parquet.py\n",
    "###############################################################################\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_df_into_parquet_batches(\n",
    "    df: pd.DataFrame,\n",
    "    batch_size: int,\n",
    "    output_folder: str,\n",
    "    file_prefix: str = \"chunk_\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into multiple Parquet files (batch_size rows each),\n",
    "    storing them in 'output_folder'. Each file is named like 'chunk_1.parquet',\n",
    "    'chunk_2.parquet', etc.\n",
    "\n",
    "    A progress bar shows how many batches are being saved.\n",
    "\n",
    "    This script does NOT merge them back into a single file. The idea is to keep\n",
    "    each chunk separate so you can process them individually later.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    total_rows = len(df)\n",
    "    batch_count = (total_rows + batch_size - 1) // batch_size\n",
    "    print(f\"Splitting DF with {total_rows} rows into {batch_count} batches of size {batch_size}.\")\n",
    "\n",
    "    current_row = 0\n",
    "    batch_idx = 1\n",
    "\n",
    "    with tqdm(total=batch_count, desc=\"Saving Batches\", unit=\"batch\") as pbar:\n",
    "        while current_row < total_rows:\n",
    "            end_row = min(current_row + batch_size, total_rows)\n",
    "            df_batch = df.iloc[current_row:end_row]\n",
    "\n",
    "            chunk_filename = f\"{file_prefix}{batch_idx}.parquet\"\n",
    "            chunk_path = os.path.join(output_folder, chunk_filename)\n",
    "\n",
    "            df_batch.to_parquet(chunk_path, index=False)\n",
    "            \n",
    "            pbar.update(1)\n",
    "            print(f\"  -> Saved batch {batch_idx} rows [{current_row}:{end_row}] to {chunk_path}\")\n",
    "\n",
    "            current_row = end_row\n",
    "            batch_idx += 1\n",
    "\n",
    "    print(\"\\nAll done. Each batch is in its own .parquet file in:\", output_folder)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Decide where to store chunked Parquet files\n",
    "    out_folder = \"Data/2.Processed/SentimentAnalysis\"\n",
    "    prefix = \"chunk_\"\n",
    "    b_size = 10_000\n",
    "\n",
    "    split_df_into_parquet_batches(\n",
    "        df=df,\n",
    "        batch_size=b_size,\n",
    "        output_folder=out_folder,\n",
    "        file_prefix=prefix\n",
    "    )\n",
    "\n",
    "    print(\"Done splitting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FURTHER MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############################################################################\n",
    "# # SCRIPT: domain_sentiment.py\n",
    "# ###############################################################################\n",
    "# import os\n",
    "# import glob\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Hugging Face Transformers\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# def domain_sentiment_on_parquet_chunks(\n",
    "#     input_folder: str,\n",
    "#     output_folder: str,\n",
    "#     file_pattern: str = \"chunk_*.parquet\",\n",
    "#     out_prefix: str = \"sentiment_\",\n",
    "#     model_name: str = \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "#     text_column: str = \"abstract\"\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     1) Finds Parquet files (e.g., chunk_1.parquet, chunk_2.parquet...) in `input_folder`.\n",
    "#     2) Loads a domain-specific (or generic) sentiment model from HF Transformers.\n",
    "#     3) For each chunk:\n",
    "#        - Read the chunk into a DataFrame\n",
    "#        - Predict sentiment on 'text_column' (abstract)\n",
    "#        - Save as new file in `output_folder`, e.g. 'sentiment_chunk_1.parquet'\n",
    "#     \"\"\"\n",
    "\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#     # 1) Initialize the pipeline with your domain-specific model\n",
    "#     print(f\"Loading model: {model_name}\")\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "#     sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "#     # 2) Gather chunk files\n",
    "#     chunk_files = glob.glob(os.path.join(input_folder, file_pattern))\n",
    "#     chunk_files.sort()  # optional, to process in numeric order\n",
    "\n",
    "#     print(f\"Found {len(chunk_files)} chunk files in '{input_folder}' with pattern '{file_pattern}'\")\n",
    "#     if not chunk_files:\n",
    "#         print(\"No files found. Exiting.\")\n",
    "#         return\n",
    "\n",
    "#     # 3) Process each chunk\n",
    "#     with tqdm(total=len(chunk_files), desc=\"Sentiment Chunks\", unit=\"file\") as pbar:\n",
    "#         for chunk_file in chunk_files:\n",
    "#             df_chunk = pd.read_parquet(chunk_file)\n",
    "\n",
    "#             # We'll gather texts\n",
    "#             texts = df_chunk[text_column].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "#             # Predict in a batch manner (pipeline automatically does internal batching)\n",
    "#             # If memory is still an issue, we can do smaller sub-batches. For now, we do the entire chunk.\n",
    "#             sentiments = sentiment_pipeline(texts)\n",
    "\n",
    "#             # sentiments is a list of dicts like [{\"label\": \"POSITIVE\", \"score\": 0.998}, ...]\n",
    "#             # We'll store them in new columns\n",
    "#             df_chunk[\"sent_label\"] = [s[\"label\"] for s in sentiments]\n",
    "#             df_chunk[\"sent_score\"] = [s[\"score\"] for s in sentiments]\n",
    "\n",
    "#             # 4) Save chunk\n",
    "#             base_name = os.path.basename(chunk_file)  # e.g. 'chunk_1.parquet'\n",
    "#             out_name = out_prefix + base_name         # e.g. 'sentiment_chunk_1.parquet'\n",
    "#             out_path = os.path.join(output_folder, out_name)\n",
    "#             df_chunk.to_parquet(out_path, index=False)\n",
    "\n",
    "#             print(f\"  -> Labeled sentiment for {chunk_file} -> {out_path}\")\n",
    "#             pbar.update(1)\n",
    "\n",
    "#     print(\"\\nAll done! Labeled chunk files are in:\", output_folder)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_dir = \"Data/2.Processed/SentimentAnalysis\"    # where your chunk_*.parquet are\n",
    "#     output_dir = \"Data/2.Processed/SentimentAnalysis/Snorkel\" # where you want to save labeled files\n",
    "#     model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\" # microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract # nlptown/bert-base-multilingual-uncased-sentiment\n",
    "#     # ^ Replace with your domain model, e.g. \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\"\n",
    "#     #   if it has a sentiment head. Otherwise you may do zero-shot classification.\n",
    "\n",
    "#     domain_sentiment_on_parquet_chunks(\n",
    "#         input_folder=input_dir,\n",
    "#         output_folder=output_dir,\n",
    "#         file_pattern=\"chunk_*.parquet\",\n",
    "#         out_prefix=\"sentiment_\",\n",
    "#         model_name=model_name,\n",
    "#         text_column=\"abstract\"\n",
    "#     )\n",
    "#     print(\"Sentiment analysis complete on chunked data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\")\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 106 chunk files in 'Data/2.Processed/SentimentAnalysis' matching 'chunk_*.parquet'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment Chunks:   0%|          | 0/106 [00:00<?, ?file/s]"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# SCRIPT: domain_sentiment_sliding.py\n",
    "###############################################################################\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def predict_long_text(\n",
    "    text: str,\n",
    "    pipe,\n",
    "    max_length=512,\n",
    "    stride=256,\n",
    "    aggregation=\"average\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Use a sliding-window approach to handle text longer than `max_length`.\n",
    "    - `pipe` is a Hugging Face pipeline for sentiment or sequence classification.\n",
    "    - We chunk the text into overlapping windows of size `max_length`, with `stride`.\n",
    "    - For each chunk, we get a sentiment label distribution.\n",
    "    - Then we combine them (e.g. by averaging \"positive\" probability).\n",
    "\n",
    "    Returns a dict with overall \"label\" and \"score\", e.g. {\"label\": \"POSITIVE\", \"score\": 0.87}.\n",
    "    \"\"\"\n",
    "\n",
    "    # The pipeline may do its own tokenization, but we want more control here:\n",
    "    # We'll manually tokenize with the same tokenizer used in the pipeline.\n",
    "    tokenizer = pipe.tokenizer  # must match pipe's tokenizer\n",
    "\n",
    "    # Tokenize the entire text, but don't do immediate truncation\n",
    "    # We rely on the sliding window approach below\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=False,  # We'll handle it ourselves\n",
    "        return_overflowing_tokens=True,\n",
    "        max_length=max_length,\n",
    "        stride=stride\n",
    "    )\n",
    "\n",
    "    # The above call might produce multiple \"overflowing\" item sets in encoding\n",
    "    # if text is bigger than max_length. But actually, the HF approach is a bit different.\n",
    "    # We can do a simpler approach: manually chunk the text ourselves. Let's do so.\n",
    "\n",
    "    # We'll do an alternative approach: break the text into sub-chunks ourselves,\n",
    "    # each up to `max_length` tokens, with overlap `stride`.\n",
    "    # Then run pipe on each sub-chunk of tokens -> gather results.\n",
    "\n",
    "    # We'll define a manual chunking:\n",
    "    input_ids = tokenizer(text, add_special_tokens=True)[\"input_ids\"]\n",
    "    # The length in tokens\n",
    "    total_tokens = len(input_ids)\n",
    "    # If it fits in max_length, just do one pass\n",
    "    if total_tokens <= max_length:\n",
    "        results = pipe(text)\n",
    "        return results[0]  # pipeline returns a list of dicts, so we take the first\n",
    "\n",
    "    # Otherwise, define sub-chunks\n",
    "    # We'll store probabilities per chunk, then average or combine\n",
    "    # pipeline returns label + score. Typically \"POSITIVE\"/\"NEGATIVE\" w/ a single score, or multiple classes if >2\n",
    "\n",
    "    subchunk_sentiments = []\n",
    "\n",
    "    # We'll do a loop over start positions:\n",
    "    start = 0\n",
    "    while start < total_tokens:\n",
    "        end = start + max_length\n",
    "        sub_ids = input_ids[start:end]\n",
    "        sub_text = tokenizer.decode(sub_ids, skip_special_tokens=True)\n",
    "        # run pipeline on that sub_text\n",
    "        r = pipe(sub_text)\n",
    "        # r is typically e.g. [{\"label\": \"POSITIVE\", \"score\": 0.98}]\n",
    "        subchunk_sentiments.append(r[0])\n",
    "        if end >= total_tokens:\n",
    "            break\n",
    "        start += max_length - stride  # move by stride\n",
    "\n",
    "    # Now combine subchunk_sentiments. We can do many approaches:\n",
    "    # 1) If it's a binary classifier (POS/NEG), we can average the \"positive\" score, etc.\n",
    "    # 2) If it's multi-class, we might do pipe.model.config.num_labels to handle multiple classes\n",
    "\n",
    "    # We'll assume a typical BERT sentiment with \"POSITIVE\"/\"NEGATIVE\"/\"NEUTRAL\" labels.\n",
    "    # We'll convert each chunk's \"label\" into a numeric distribution if possible.\n",
    "    # But the pipeline by default might just give top label. That's ambiguous.\n",
    "    # For better approach, we might define pipe as \"text-classification\" with return_all_scores=True.\n",
    "\n",
    "    # For demonstration, let's do the simplest approach:\n",
    "    # We'll count how many sub-chunks are \"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\", etc.\n",
    "    # Then pick the majority or pick whichever is most frequent.\n",
    "\n",
    "    label_counts = {}\n",
    "    for chunk_res in subchunk_sentiments:\n",
    "        lbl = chunk_res[\"label\"]\n",
    "        label_counts[lbl] = label_counts.get(lbl, 0) + 1\n",
    "\n",
    "    # pick the label that appears most frequently\n",
    "    overall_label = max(label_counts, key=label_counts.get)\n",
    "    # the \"score\" we can set to the fraction of sub-chunks with that label\n",
    "    overall_score = label_counts[overall_label] / len(subchunk_sentiments)\n",
    "\n",
    "    return {\"label\": overall_label, \"score\": overall_score}\n",
    "\n",
    "def domain_sentiment_on_parquet_chunks(\n",
    "    input_folder: str,\n",
    "    output_folder: str,\n",
    "    file_pattern: str = \"chunk_*.parquet\",\n",
    "    out_prefix: str = \"sentiment_\",\n",
    "    model_name: str = \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    text_column: str = \"abstract\",\n",
    "    max_length=512,\n",
    "    stride=256\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Finds Parquet chunk files in `input_folder`.\n",
    "    2) Loads the specified model from HF Transformers.\n",
    "    3) For each chunk, read => for each row's text => do sliding-window sentiment => store label + score => save.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    # We define the pipeline, but we'll do sub-chunk logic ourselves in predict_long_text\n",
    "    # The pipeline will be used for classification of sub-chunks\n",
    "    sentiment_pipeline = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        # We'll still pass truncation=False here because we're handling sub-chunk truncation manually\n",
    "        truncation=False\n",
    "    )\n",
    "\n",
    "    chunk_files = glob.glob(os.path.join(input_folder, file_pattern))\n",
    "    chunk_files.sort()\n",
    "    print(f\"Found {len(chunk_files)} chunk files in '{input_folder}' matching '{file_pattern}'.\")\n",
    "    if not chunk_files:\n",
    "        print(\"No chunk files found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    with tqdm(total=len(chunk_files), desc=\"Sentiment Chunks\", unit=\"file\") as pbar:\n",
    "        for chunk_file in chunk_files:\n",
    "            df_chunk = pd.read_parquet(chunk_file)\n",
    "            # We'll do row-by-row if the chunk size is not huge. If chunk is big, consider sub-batching.\n",
    "\n",
    "            all_labels = []\n",
    "            all_scores = []\n",
    "\n",
    "            # row-based loop\n",
    "            for text in tqdm(df_chunk[text_column].fillna(\"\").astype(str), desc=\"Rows\", leave=False):\n",
    "                # do sub-chunk approach\n",
    "                res = predict_long_text(\n",
    "                    text=text,\n",
    "                    pipe=sentiment_pipeline,\n",
    "                    max_length=max_length,\n",
    "                    stride=stride\n",
    "                )\n",
    "                all_labels.append(res[\"label\"])\n",
    "                all_scores.append(res[\"score\"])\n",
    "\n",
    "            df_chunk[\"sent_label\"] = all_labels\n",
    "            df_chunk[\"sent_score\"] = all_scores\n",
    "\n",
    "            base_name = os.path.basename(chunk_file)\n",
    "            out_name = out_prefix + base_name\n",
    "            out_path = os.path.join(output_folder, out_name)\n",
    "            df_chunk.to_parquet(out_path, index=False)\n",
    "\n",
    "            print(f\"  -> Labeled {chunk_file} -> {out_path}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(\"\\nAll done! Labeled chunk files are in:\", output_folder)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"Data/2.Processed/SentimentAnalysis\"    # where your chunk_*.parquet are\n",
    "    output_dir = \"Data/2.Processed/SentimentAnalysis/Snorkel\"\n",
    "    model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\" #microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\n",
    "    # If you have a domain model w/ a sentiment classification head, put it here.\n",
    "    # If not, consider zero-shot approach or see if the text is short enough to not exceed 512 tokens\n",
    "\n",
    "    domain_sentiment_on_parquet_chunks(\n",
    "        input_folder=input_dir,\n",
    "        output_folder=output_dir,\n",
    "        file_pattern=\"chunk_*.parquet\",\n",
    "        out_prefix=\"sentiment_\",\n",
    "        model_name=model_name,\n",
    "        text_column=\"abstract\",\n",
    "        max_length=496,\n",
    "        stride=256\n",
    "    )\n",
    "    print(\"Sentiment analysis complete on chunked data (sliding window).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  Cutting df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: nlptown/bert-base-multilingual-uncased-sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 chunk files in 'Data/2.Processed/SentimentAnalysis' matching 'chunk_*.parquet'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentiment Chunks:   0%|          | 0/11 [00:00<?, ?file/s]Token indices sequence length is longer than the specified maximum sequence length for this model (700 > 512). Running this sequence through the model will result in indexing errors\n",
      "Sentiment Chunks:   0%|          | 0/11 [00:21<?, ?file/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 190\u001b[0m\n\u001b[0;32m    186\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlptown/bert-base-multilingual-uncased-sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# If you have a domain model w/ a sentiment classification head, put it here.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# If not, consider zero-shot approach or see if the text is short enough to not exceed 512 tokens\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m \u001b[43mdomain_sentiment_on_parquet_chunks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_pattern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchunk_*.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentiment_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m510\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\n\u001b[0;32m    199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment analysis complete on chunked data (sliding window).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 160\u001b[0m, in \u001b[0;36mdomain_sentiment_on_parquet_chunks\u001b[1;34m(input_folder, output_folder, file_pattern, out_prefix, model_name, text_column, max_length, stride)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# row-based loop\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(df_chunk[text_column]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRows\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# do sub-chunk approach\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_long_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msentiment_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     all_labels\u001b[38;5;241m.\u001b[39mappend(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    167\u001b[0m     all_scores\u001b[38;5;241m.\u001b[39mappend(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[1;32mIn[17], line 78\u001b[0m, in \u001b[0;36mpredict_long_text\u001b[1;34m(text, pipe, max_length, stride, aggregation)\u001b[0m\n\u001b[0;32m     76\u001b[0m sub_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(sub_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# run pipeline on that sub_text\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# r is typically e.g. [{\"label\": \"POSITIVE\", \"score\": 0.98}]\u001b[39;00m\n\u001b[0;32m     80\u001b[0m subchunk_sentiments\u001b[38;5;241m.\u001b[39mappend(r[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:159\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[1;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1301\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[0;32m   1295\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1298\u001b[0m         )\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[0;32m   1300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1308\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m   1307\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m-> 1308\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1309\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[0;32m   1310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\base.py:1208\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1207\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1208\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:190\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    189\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1665\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1657\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1662\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1665\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1677\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1679\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\pytorch_utils.py:258\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# SCRIPT: domain_sentiment_sliding.py\n",
    "###############################################################################\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def predict_long_text(\n",
    "    text: str,\n",
    "    pipe,\n",
    "    max_length=512,\n",
    "    stride=256,\n",
    "    aggregation=\"average\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Use a sliding-window approach to handle text longer than `max_length`.\n",
    "    - `pipe` is a Hugging Face pipeline for sentiment or sequence classification.\n",
    "    - We chunk the text into overlapping windows of size `max_length`, with `stride`.\n",
    "    - For each chunk, we get a sentiment label distribution.\n",
    "    - Then we combine them (e.g. by averaging \"positive\" probability).\n",
    "\n",
    "    Returns a dict with overall \"label\" and \"score\", e.g. {\"label\": \"POSITIVE\", \"score\": 0.87}.\n",
    "    \"\"\"\n",
    "\n",
    "    # The pipeline may do its own tokenization, but we want more control here:\n",
    "    # We'll manually tokenize with the same tokenizer used in the pipeline.\n",
    "    tokenizer = pipe.tokenizer  # must match pipe's tokenizer\n",
    "\n",
    "    # Tokenize the entire text, but don't do immediate truncation\n",
    "    # We rely on the sliding window approach below\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=False,  # We'll handle it ourselves\n",
    "        return_overflowing_tokens=True,\n",
    "        max_length=max_length,\n",
    "        stride=stride\n",
    "    )\n",
    "\n",
    "    # The above call might produce multiple \"overflowing\" item sets in encoding\n",
    "    # if text is bigger than max_length. But actually, the HF approach is a bit different.\n",
    "    # We can do a simpler approach: manually chunk the text ourselves. Let's do so.\n",
    "\n",
    "    # We'll do an alternative approach: break the text into sub-chunks ourselves,\n",
    "    # each up to `max_length` tokens, with overlap `stride`.\n",
    "    # Then run pipe on each sub-chunk of tokens -> gather results.\n",
    "\n",
    "    # We'll define a manual chunking:\n",
    "    input_ids = tokenizer(text, add_special_tokens=True)[\"input_ids\"]\n",
    "    # The length in tokens\n",
    "    total_tokens = len(input_ids)\n",
    "    # If it fits in max_length, just do one pass\n",
    "    if total_tokens <= max_length:\n",
    "        results = pipe(text)\n",
    "        return results[0]  # pipeline returns a list of dicts, so we take the first\n",
    "\n",
    "    # Otherwise, define sub-chunks\n",
    "    # We'll store probabilities per chunk, then average or combine\n",
    "    # pipeline returns label + score. Typically \"POSITIVE\"/\"NEGATIVE\" w/ a single score, or multiple classes if >2\n",
    "\n",
    "    subchunk_sentiments = []\n",
    "\n",
    "    # We'll do a loop over start positions:\n",
    "    start = 0\n",
    "    while start < total_tokens:\n",
    "        end = start + max_length\n",
    "        sub_ids = input_ids[start:end]\n",
    "        sub_text = tokenizer.decode(sub_ids, skip_special_tokens=True)\n",
    "        # run pipeline on that sub_text\n",
    "        r = pipe(sub_text)\n",
    "        # r is typically e.g. [{\"label\": \"POSITIVE\", \"score\": 0.98}]\n",
    "        subchunk_sentiments.append(r[0])\n",
    "        if end >= total_tokens:\n",
    "            break\n",
    "        start += max_length - stride  # move by stride\n",
    "\n",
    "    # Now combine subchunk_sentiments. We can do many approaches:\n",
    "    # 1) If it's a binary classifier (POS/NEG), we can average the \"positive\" score, etc.\n",
    "    # 2) If it's multi-class, we might do pipe.model.config.num_labels to handle multiple classes\n",
    "\n",
    "    # We'll assume a typical BERT sentiment with \"POSITIVE\"/\"NEGATIVE\"/\"NEUTRAL\" labels.\n",
    "    # We'll convert each chunk's \"label\" into a numeric distribution if possible.\n",
    "    # But the pipeline by default might just give top label. That's ambiguous.\n",
    "    # For better approach, we might define pipe as \"text-classification\" with return_all_scores=True.\n",
    "\n",
    "    # For demonstration, let's do the simplest approach:\n",
    "    # We'll count how many sub-chunks are \"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\", etc.\n",
    "    # Then pick the majority or pick whichever is most frequent.\n",
    "\n",
    "    label_counts = {}\n",
    "    for chunk_res in subchunk_sentiments:\n",
    "        lbl = chunk_res[\"label\"]\n",
    "        label_counts[lbl] = label_counts.get(lbl, 0) + 1\n",
    "\n",
    "    # pick the label that appears most frequently\n",
    "    overall_label = max(label_counts, key=label_counts.get)\n",
    "    # the \"score\" we can set to the fraction of sub-chunks with that label\n",
    "    overall_score = label_counts[overall_label] / len(subchunk_sentiments)\n",
    "\n",
    "    return {\"label\": overall_label, \"score\": overall_score}\n",
    "\n",
    "def domain_sentiment_on_parquet_chunks(\n",
    "    input_folder: str,\n",
    "    output_folder: str,\n",
    "    file_pattern: str = \"chunk_*.parquet\",\n",
    "    out_prefix: str = \"sentiment_\",\n",
    "    model_name: str = \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    text_column: str = \"abstract\",\n",
    "    max_length=512,\n",
    "    stride=256\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Finds Parquet chunk files in `input_folder`.\n",
    "    2) Loads the specified model from HF Transformers.\n",
    "    3) For each chunk, read => for each row's text => do sliding-window sentiment => store label + score => save.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    # We define the pipeline, but we'll do sub-chunk logic ourselves in predict_long_text\n",
    "    # The pipeline will be used for classification of sub-chunks\n",
    "    sentiment_pipeline = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        # We'll still pass truncation=False here because we're handling sub-chunk truncation manually\n",
    "        truncation=False\n",
    "    )\n",
    "\n",
    "    chunk_files = glob.glob(os.path.join(input_folder, file_pattern))\n",
    "    chunk_files.sort()\n",
    "    print(f\"Found {len(chunk_files)} chunk files in '{input_folder}' matching '{file_pattern}'.\")\n",
    "    if not chunk_files:\n",
    "        print(\"No chunk files found. Exiting.\")\n",
    "        return\n",
    "\n",
    "    with tqdm(total=len(chunk_files), desc=\"Sentiment Chunks\", unit=\"file\") as pbar:\n",
    "        for chunk_file in chunk_files:\n",
    "            df_chunk = pd.read_parquet(chunk_file)\n",
    "            # We'll do row-by-row if the chunk size is not huge. If chunk is big, consider sub-batching.\n",
    "\n",
    "            all_labels = []\n",
    "            all_scores = []\n",
    "\n",
    "            # row-based loop\n",
    "            for text in tqdm(df_chunk[text_column].fillna(\"\").astype(str), desc=\"Rows\", leave=False):\n",
    "                # do sub-chunk approach\n",
    "                res = predict_long_text(\n",
    "                    text=text,\n",
    "                    pipe=sentiment_pipeline,\n",
    "                    max_length=max_length,\n",
    "                    stride=stride\n",
    "                )\n",
    "                all_labels.append(res[\"label\"])\n",
    "                all_scores.append(res[\"score\"])\n",
    "\n",
    "            df_chunk[\"sent_label\"] = all_labels\n",
    "            df_chunk[\"sent_score\"] = all_scores\n",
    "\n",
    "            base_name = os.path.basename(chunk_file)\n",
    "            out_name = out_prefix + base_name\n",
    "            out_path = os.path.join(output_folder, out_name)\n",
    "            df_chunk.to_parquet(out_path, index=False)\n",
    "\n",
    "            print(f\"  -> Labeled {chunk_file} -> {out_path}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(\"\\nAll done! Labeled chunk files are in:\", output_folder)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"Data/2.Processed/SentimentAnalysis\"    # where your chunk_*.parquet are\n",
    "    output_dir = \"Data/2.Processed/SentimentAnalysis/Snorkel\"\n",
    "    model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "    # If you have a domain model w/ a sentiment classification head, put it here.\n",
    "    # If not, consider zero-shot approach or see if the text is short enough to not exceed 512 tokens\n",
    "\n",
    "    domain_sentiment_on_parquet_chunks(\n",
    "        input_folder=input_dir,\n",
    "        output_folder=output_dir,\n",
    "        file_pattern=\"chunk_*.parquet\",\n",
    "        out_prefix=\"sentiment_\",\n",
    "        model_name=model_name,\n",
    "        text_column=\"abstract\",\n",
    "        max_length=510,\n",
    "        stride=256\n",
    "    )\n",
    "    print(\"Sentiment analysis complete on chunked data (sliding window).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: snorkel\n",
      "Version: 0.10.0\n",
      "Summary: A system for quickly generating training data with weak supervision\n",
      "Home-page: https://github.com/snorkel-team/snorkel\n",
      "Author: \n",
      "Author-email: \n",
      "License: Apache License 2.0\n",
      "Location: c:\\Users\\macie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\n",
      "Requires: munkres, networkx, numpy, pandas, protobuf, scikit-learn, scipy, tensorboard, torch, tqdm\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show snorkel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function, PandasLFApplier\n",
    "import pandas as pd\n",
    "\n",
    "@labeling_function()\n",
    "def lf_positive_terms(x):\n",
    "    return 1 if any(term in x.text.lower() for term in [\"effective\", \"promising\", \"successful\"]) else -1\n",
    "\n",
    "@labeling_function()\n",
    "def lf_negative_terms(x):\n",
    "    return 0 if any(term in x.text.lower() for term in [\"adverse\", \"ineffective\", \"failure\"]) else -1\n",
    "\n",
    "# Combine and apply labeling functions\n",
    "lfs = [lf_positive_terms, lf_negative_terms]\n",
    "df = pd.DataFrame({\"text\": [\"Effective treatment.\", \"Adverse effects noted.\", \"No clear results.\"]})\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "label_matrix = applier.apply(df)\n",
    "print(label_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go for batches beacause this model won't work with this much data also maybe create new dataset without most columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Lexicon-Based Approach**\n",
    "\n",
    "1.1 Why a Lexicon-Based Method?\n",
    "No training data required. It uses a dictionary (lexicon) of words mapped to sentiment scores (positive, negative, neutral).\n",
    "\n",
    "Quick to implement, can provide a baseline or unsupervised vantage.\n",
    "\n",
    "1.2 Commonly used library: \n",
    "`VADER` (suitable for short social media–style text but can be adapted) or \n",
    "`SentiWordNet` (a more general WordNet-based approach).\n",
    "\n",
    "`VADER` \n",
    "Works decently on short, informal text (abstracts, titles).\n",
    "If your text is more scientific (like PubMed titles/abstracts), you may find many neutral or domain words not recognized by VADER.\n",
    "\n",
    "`SentiWordNet`\n",
    "If your text is more formal or domain-based, you might want to explore SentiWordNet or a domain-specific lexicon. The logic is similar, but you’d look up each word’s positivity/negativity in the SentiWordNet dictionary, summing or averaging them.\n",
    "\n",
    "`Justification`\n",
    "Lexicon-based methods are quick for an unsupervised sentiment estimate.\n",
    "They can fail in domain-specific contexts (e.g., biomedical text might mention “cancer” or “infection,” which are negative in a lay sense but may be neutral from a purely scientific standpoint).\n",
    "\n",
    "This is why a **supervised** approach may be more accurate if you have labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\macie\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 1) LEXICON-BASED (VADER) EXAMPLE\n",
    "###############################################################################\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# 1) Download the VADER lexicon if not installed\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def lexicon_based_vader(text):\n",
    "    \"\"\"\n",
    "    Return sentiment scores for the given text using VADER.\n",
    "    \"\"\"\n",
    "    # If text is None or not a string, convert to empty string\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores\n",
    "\n",
    "def get_vader_label(scores):\n",
    "    compound = scores[\"compound\"]\n",
    "    if compound >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif compound <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# 2) Suppose your DataFrame is 'df' with column \"abstract\" or \"title\"\n",
    "#    We'll do it on \"abstract\"\n",
    "df[\"vader_scores\"] = df[\"abstract\"].apply(lexicon_based_vader)\n",
    "df[\"vader_label\"] = df[\"vader_scores\"].apply(get_vader_label)\n",
    "\n",
    "df[[\"title\", \"abstract\", \"vader_scores\", \"vader_label\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Alternative: SentiWordNet\n",
    "\n",
    "If your text is more formal or domain-based, you might want to explore SentiWordNet or a domain-specific lexicon. The logic is similar, but you’d look up each word’s positivity/negativity in the SentiWordNet dictionary, summing or averaging them.\n",
    "\n",
    "Justification\n",
    "Lexicon-based methods are quick for an unsupervised sentiment estimate.\n",
    "They can fail in domain-specific contexts (e.g., biomedical text might mention “cancer” or “infection,” which are negative in a lay sense but may be neutral from a purely scientific standpoint).\n",
    "This is why a supervised approach may be more accurate if you have labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Supervised Machine Learning Approach**\n",
    "\n",
    "**2.1 Why a Supervised Method?**\n",
    "\n",
    "You’ll train a model on labeled examples of text → sentiment (pos/neg/neu).\n",
    "\n",
    "This typically yields better results than lexicon-based if you have enough labeled data.\n",
    "\n",
    "Common supervised classifiers: Logistic Regression, SVM, Naive Bayes, or even a fine-tuned BERT.\n",
    "\n",
    "**2.2 Example with Logistic Regression**\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Gather labeled data: \n",
    "\n",
    "You need text + a sentiment label. Perhaps you label 1000+ random samples as positive/negative/neutral.\n",
    "2. Feature extraction:\n",
    "\n",
    "A simple approach uses TF-IDF or CountVectorizer on the text.\n",
    "\n",
    "3. More advanced: \n",
    "\n",
    "use pretrained embeddings (e.g., BERT) as features.\n",
    "Train a scikit-learn classifier (LogReg or SVM).\n",
    "Predict on unseen text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Suppose df_labeled has columns: \"abstract\", \"label\"\n",
    "# label in {pos, neg, neu}, curated or partially annotated\n",
    "\n",
    "train_df, test_df = train_test_split(df_labeled, test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df[\"abstract\"])\n",
    "y_train = train_df[\"label\"]\n",
    "\n",
    "X_test = vectorizer.transform(test_df[\"abstract\"])\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification:\n",
    "\n",
    "Logistic Regression is a common baseline supervised approach for sentiment analysis.\n",
    "TF-IDF is quick to implement and typically effective for textual classification if domain-specific embeddings aren’t available.\n",
    "2.3 Possible Upgrades\n",
    "SVM or RandomForest: Slight variations in performance.\n",
    "Neural approach with BERT-based embeddings: If you have enough data/time.\n",
    "If your text is domain-specific, consider a SciBERT or BioBERT for embeddings or fine-tuning.\n",
    "3. Combining or Reporting\n",
    "You’re “obliged to perform sentiment analysis using at least one lexicon-based approach and at least one supervised technique.” So in your final report, you can:\n",
    "\n",
    "Implement VADER for the lexicon-based method.\n",
    "Implement Logistic Regression (or SVM) for the supervised method.\n",
    "Compare performance on the same test set (requires labeled data for the supervised approach).\n",
    "Justify choices:\n",
    "VADER is quick, well-known, but might not handle domain terms well.\n",
    "Logistic Regression is a standard baseline for text classification, interpretable, typically robust.\n",
    "If your text is heavily domain-specific (scientific, biomedical), mention that both methods might have limitations:\n",
    "\n",
    "Lexicon: might incorrectly classify domain words.\n",
    "Logistic Regression: requires a domain-labeled dataset.\n",
    "Still, this approach meets the stated requirement: one lexicon-based, one supervised approach, plus a reasoned justification.\n",
    "\n",
    "4. Considering Neutral vs. Non-Neutral\n",
    "If your data is primarily neutral (like many scientific abstracts), the distribution of sentiment might be heavily skewed. You can either:\n",
    "\n",
    "Keep a 3-class system (pos/neg/neu).\n",
    "Merge pos/neg into non-neutral vs. neutral.\n",
    "Provide an analysis of how many are likely to be neutral, if that’s the main interest.\n",
    "Either approach is valid, but note that heavily neutral data can reduce your classifier’s performance if you have few positive/negative examples.\n",
    "\n",
    "5. Potential Models Summarized\n",
    "Lexicon-based:\n",
    "\n",
    "VADER if text is general or social media–like.\n",
    "SentiWordNet or other dictionary for more formal text.\n",
    "Bio domain: Possibly no major out-of-the-box lexicon for sentiment, so VADER is a fallback.\n",
    "Supervised:\n",
    "\n",
    "Logistic Regression or SVM with TF-IDF → easy to implement, relatively fast.\n",
    "If large labeled data + domain complexity → fine-tuned BERT (e.g., SciBERT or BioBERT). But that’s more advanced in setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "Yes, you can approach your data with two methods:\n",
    "Lexicon-based (like VADER) for an unsupervised baseline,\n",
    "Supervised (like Logistic Regression) for better domain accuracy if you have labeled data.\n",
    "Keep in mind domain-specific challenges if your text is specialized.\n",
    "If your data is mostly neutral, analyzing pos/neg signals might require a large labeled set or more sophisticated domain lexicons.\n",
    "This satisfies the requirement to use “at least one lexicon-based approach and at least one supervised machine learning technique”, plus justification for each choice.\n",
    "With these code blocks and the rationale above, you can implement both methods, compare them, and then summarize in your final report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Takeaways\n",
    "Use raw text (not tokenized) for VADER, because it relies on punctuation, capitalization, etc.\n",
    "Expect mostly neutral results for biomedical articles.\n",
    "For your supervised approach, a standard logistic regression or SVM with TF-IDF is straightforward. Or you can do advanced domain-based or neural embeddings.\n",
    "You’ll include both approaches in your final deliverable, explaining their rationale and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHATT GO GO GO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Acquisition & Initial Storage\n",
    "Gather the Articles\n",
    "\n",
    "You likely have data from PubMed or a similar source, with columns like uid, title, abstract, parsed_date, etc.\n",
    "If you have over a million records, store them in a batch-oriented or chunk-based format (e.g., multiple Parquet files). This ensures you don’t exceed memory constraints.\n",
    "Ensure You Have Enough Labeled Data (for the supervised part)\n",
    "\n",
    "If sentiment labels do not exist, you must create at least a small labeled set (e.g., 1,000–2,000 randomly sampled abstracts/titles).\n",
    "Alternatively, if there is no direct sentiment labeling, consider a small manual annotation or partially auto-labeled approach to bootstrap your supervised classifier.\n",
    "Because biomedical text is typically neutral, you’ll need to carefully define “positive” vs. “negative” in a domain context (e.g., describing successful treatments, hopeful outcomes → positive; describing severe complications or mortality → negative).\n",
    "2. Data Cleaning & Preprocessing\n",
    "Check for Missing / Incomplete Rows\n",
    "\n",
    "Titles or abstracts that are empty → handle them (they might default to “neutral” in lexicon-based, or be dropped for supervised training).\n",
    "Normalize (Optional)\n",
    "\n",
    "Typically, for lexicon-based approaches (like VADER), punctuation and case matter. So keep them.\n",
    "For the supervised approach (TF-IDF or embeddings), you might do normal lowercasing/punctuation removal.\n",
    "You can keep a separate column for raw text if your lexicon-based approach benefits from punctuation, capitalization, etc.\n",
    "Tokenization (for supervised approach)\n",
    "\n",
    "If you plan to do a simpler TF-IDF approach, you can use standard tokenization (like nltk.word_tokenize) plus lowercasing.\n",
    "If you plan advanced embeddings (e.g., BioBERT), you feed raw text into the model’s tokenizer.\n",
    "Optional Stopword Removal\n",
    "\n",
    "For typical sentiment tasks, removing standard English stopwords can help in a supervised approach. But in the medical domain, certain “stop” words might have sentiment implications, so proceed with caution.\n",
    "Chunking (Memory Management)\n",
    "\n",
    "If your DataFrame is huge (1M+ rows), you might do a chunk-based approach for certain tasks (reading/writing). For lexicon-based and supervised inference, you can process in smaller chunks and then store results.\n",
    "3. Lexicon-Based Sentiment Analysis\n",
    "3.1. Why Lexicon?\n",
    "Requirement: At least one lexicon-based approach.\n",
    "Pros: No training data needed, quick to set up.\n",
    "Cons: Domain mismatch likely → many biomedical terms not in the default sentiment dictionary.\n",
    "3.2. Implementation Steps\n",
    "Choose a Lexicon\n",
    "\n",
    "VADER (if text is somewhat plain English).\n",
    "SentiWordNet or another approach if you prefer more general coverage.\n",
    "For domain-specific expansions: Possibly incorporate medical synonyms with negative connotations (e.g., “adverse event,” “fatal,” etc.) if you have a custom dictionary.\n",
    "Apply Lexicon\n",
    "\n",
    "For VADER, install nltk → download 'vader_lexicon' → SentimentIntensityAnalyzer().\n",
    "For each row’s text (preferably the raw title or abstract, not tokenized), get the polarity scores.\n",
    "Decide a threshold: e.g., compound >= 0.05 → positive, compound <= -0.05 → negative, else neutral.\n",
    "Store & Summarize\n",
    "\n",
    "Create columns vader_scores (dictionary) and vader_label (pos/neg/neu).\n",
    "Expect many “neutral” labels. This is normal for scientific text.\n",
    "Review\n",
    "\n",
    "Possibly analyze a small sample to see if domain words are missed.\n",
    "In your final report, note the likely mismatch and the expected neutral skew.\n",
    "4. Supervised Machine Learning Approach\n",
    "4.1. Why Supervised?\n",
    "Typically yields better results if you have labeled data, because the classifier learns domain context.\n",
    "You meet the requirement: “at least one supervised technique.”\n",
    "4.2. Steps\n",
    "Obtain / Create Labeled Data\n",
    "\n",
    "You need a set of text samples with sentiment labels (pos/neg/neu).\n",
    "If you have no pre-labeled set, you might do a small manual annotation or an auto-labeled heuristic.\n",
    "Keep class distributions in mind: if your domain is heavily neutral, consider more examples for positive/negative to balance.\n",
    "Feature Extraction\n",
    "\n",
    "Basic: Use TfidfVectorizer on the text (title/abstract).\n",
    "Intermediate: Use word embeddings (e.g., GloVe) or domain embeddings (like scispaCy vectors).\n",
    "Advanced: Fine-tune a BioBERT or SciBERT model for sentiment classification. This is more complex but can handle domain terms better.\n",
    "Model Choice\n",
    "\n",
    "Logistic Regression or SVM: common baseline.\n",
    "Possibly a RandomForest or XGBoost if you suspect non-linear relationships.\n",
    "Or a transformer-based approach (BioBERT) if you have enough labeled data/time/GPU resources.\n",
    "Training & Evaluation\n",
    "\n",
    "Split data into train/dev/test sets (e.g., 80/10/10).\n",
    "Train on train set, tune hyperparameters on dev set, evaluate final on test set.\n",
    "Use metrics: accuracy, F1, or a confusion matrix to see how well each class is predicted.\n",
    "Because you might have a mostly neutral dataset, track class distribution carefully.\n",
    "Interpretation\n",
    "\n",
    "If using Logistic Regression, you can examine top coefficients for each class.\n",
    "If using advanced embeddings, you might do some error analysis to see if certain domain terms always lead to “negative.”\n",
    "5. Final Assembly & Reporting\n",
    "Implementation\n",
    "\n",
    "Code each pipeline (lexicon-based & supervised) in a modular way.\n",
    "Possibly store sentiment results in new columns: vader_label, supervised_label.\n",
    "Compare or Correlate\n",
    "\n",
    "You can see how often your lexicon-based label matches your supervised label. Probably a large portion is “neutral” from both.\n",
    "The difference might be in detecting minor positivity or negativity that VADER missed.\n",
    "Summarize\n",
    "\n",
    "Provide a short table: e.g., how many articles each approach labeled as pos/neg/neu.\n",
    "If you have test labels for the supervised approach, show confusion matrix & F1-scores.\n",
    "Justify\n",
    "\n",
    "Lexicon-based: easy to set up, no training data. Good for a quick baseline or if no labeling is possible.\n",
    "Supervised: required labeled data but typically more accurate for domain-specific text.\n",
    "Conclusions\n",
    "\n",
    "Probably you’ll find the text is predominantly neutral. This is your real-world domain outcome.\n",
    "If “positive” or “negative” emerges, it likely correlates with language describing strong beneficial outcomes or severe adverse events.\n",
    "6. Additional Options for Medical Field\n",
    "Domain-Specific Lexicon\n",
    "\n",
    "Some resources provide medical synonyms for “risk,” “complication,” “fatal,” etc. If integrated into your lexicon-based approach, you might capture more domain negativity.\n",
    "scispaCy\n",
    "\n",
    "For biomedical text processing: tokenization, NER. Not specifically sentiment, but can help with domain text.\n",
    "BioBERT / SciBERT\n",
    "\n",
    "If you have a large labeled dataset for sentiment, you can fine-tune a domain BERT model. This can yield state-of-the-art results for domain text. But it’s more advanced in terms of GPU usage and setup.\n",
    "Error Analysis\n",
    "\n",
    "Because sentiment in medical text is subtle, examine misclassifications carefully. Terms like “cancer” might appear negative to a typical lexicon, but the article might be neutral if it’s just describing a procedure.\n",
    "7. Proposed Pipeline (Summary)\n",
    "Putting it all in a step-by-step bullet:\n",
    "\n",
    "Data Preparation\n",
    "\n",
    "Load your ~1M articles (title, abstract, etc.). Possibly store in chunked Parquet if memory is an issue.\n",
    "Data Cleaning\n",
    "\n",
    "Remove or handle missing abstracts. Keep punctuation/case for VADER.\n",
    "Lexicon-Based Approach (VADER)\n",
    "\n",
    "For each row’s raw text (abstract or title), get VADER scores → finalize vader_label (pos/neg/neu).\n",
    "Expect mostly neutral. Store the results.\n",
    "Supervised Approach\n",
    "\n",
    "a) Obtain labeled data (pos/neg/neu). Possibly label a random subset.\n",
    "b) Split train/test.\n",
    "c) Feature Extraction (TF-IDF) or advanced embeddings.\n",
    "d) Train a classifier (LogReg, SVM, or a BERT-based approach).\n",
    "e) Evaluate on test set → get accuracy, F1, confusion matrix.\n",
    "Merge & Compare\n",
    "\n",
    "Optionally compare vader_label vs. supervised_label to see alignment. Summarize stats.\n",
    "Analysis & Report\n",
    "\n",
    "Summarize method, limitations, domain mismatch issues.\n",
    "Conclude that domain text is largely neutral, but show some examples where a small subset might appear positive or negative (e.g., “successful outcome,” “mortality,” etc.).\n",
    "8. Final Notes & Best Practices\n",
    "Because your domain is heavily neutral, your supervised model might need a class re-balancing technique (e.g. oversampling the minority classes).\n",
    "If you do find that 90%+ of your text is “neutral” from both methods, report that as a finding. The domain might truly not exhibit strong sentiment.\n",
    "For large-scale data (~1 million + rows), efficiency matters:\n",
    "Use chunk-based reading/writing to avoid memory overflows.\n",
    "For lexicon-based scoring, you can chunk the DataFrame (e.g., 50k at a time), apply VADER, store results.\n",
    "For the supervised approach, ensure you have enough labeled examples. If you only label 100 or 200 items in a domain with 1 million rows, it may not generalize well.\n",
    "This pipeline addresses the entire journey:\n",
    "\n",
    "At least one lexicon-based approach: VADER.\n",
    "At least one supervised technique: Logistic Regression or SVM, with justification.\n",
    "Careful about domain mismatch and the high neutrality of the medical field.\n",
    "You’ll produce two sets of labels (vader_label, model_label) and a discussion of their alignment, limitations, and next steps (like domain expansions or advanced embeddings). This thoroughly satisfies your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHAT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why You Need Labeled Data\n",
    "A supervised approach (like Logistic Regression, SVM, or BERT-based classification) requires labeled examples of your text mapped to sentiment labels (positive/negative/neutral, or some variant). If you have zero labeled data, you cannot directly train or evaluate a supervised model.\n",
    "\n",
    "However, there are ways to generate or approximate labeled data:\n",
    "\n",
    "Manual annotation of a small subset (ideal but might be time-consuming).\n",
    "Heuristic or “weak supervision” methods that auto-label some data with certain rules.\n",
    "Use an external, relevant corpus that is already labeled for sentiment.\n",
    "Crowdsourcing or domain-expert partial labeling.\n",
    "Below are some approaches in more detail.\n",
    "\n",
    "2. Small Manual Annotation (A Minimal, “Gold” Sample)\n",
    "If you have even a small capacity to label, you might:\n",
    "\n",
    "Sample ~500–2,000 articles’ titles or short abstracts randomly.\n",
    "Spend time (or ask a small group of domain experts) to label them as “positive,” “negative,” or “neutral.”\n",
    "Positive = text describing beneficial outcomes, improvements, success, etc.\n",
    "Negative = describing severe risk, complications, mortality, adverse events.\n",
    "Neutral = purely descriptive or objective with no implied positivity/negativity.\n",
    "This small labeled set can train a baseline classifier. You can’t handle the entire million-row dataset manually, but a small subset is often enough to get started.\n",
    "Pros:\n",
    "\n",
    "Real domain data, labeled by humans.\n",
    "Even 500–1,000 labeled examples can allow a basic supervised model.\n",
    "Cons:\n",
    "\n",
    "Takes time (especially if domain experts are needed).\n",
    "Still might be skewed “neutral.”\n",
    "3. Heuristic / “Weak Supervision” Approaches\n",
    "If no direct labeling is possible, consider:\n",
    "\n",
    "Keyword-based heuristics:\n",
    "\n",
    "Mark documents containing specific terms (“success,” “improved,” “effective”) as “positive.”\n",
    "Mark those mentioning severe complications, “fatal,” “mortality,” “adverse event” as “negative.”\n",
    "Mark everything else “neutral.”\n",
    "Domain dictionary expansions:\n",
    "\n",
    "If you can gather a small set of “negative” medical synonyms (e.g., “death,” “severe,” “complication,” “failure,” “risk,” “hazard,” “worsen,” etc.), plus “positive” synonyms (“success,” “improve,” “beneficial,” “safe,” etc.), you can label text that clearly uses these words.\n",
    "Auto-labeled data from external cues:\n",
    "\n",
    "If your text has meta-data that indicates outcomes or conclusions (like “Conclusion: The approach was successful…”), you might parse that as a positive label.\n",
    "Then:\n",
    "\n",
    "All docs that match your “positive” rules are labeled “positive,” those matching “negative” rules are labeled “negative,” and everything else is “neutral.”\n",
    "This becomes a “silver standard” or “weakly supervised” dataset.\n",
    "You can then train a supervised classifier on these auto-labeled examples.\n",
    "Caveat:\n",
    "\n",
    "The classifier will learn from potentially noisy labels. But it’s better than no supervision at all, and you can refine the rules if you see major errors.\n",
    "4. Use an External Labeled Corpus\n",
    "In some cases, you might:\n",
    "\n",
    "Find a public medical-sentiment dataset (though these are rare).\n",
    "Pre-train or fine-tune your model on that external dataset.\n",
    "Optionally adapt it to your domain using a small subset of your data labeled via heuristics or manual sampling.\n",
    "Example: If there’s a small medical tweet dataset with sentiment labeled, you might train or at least partially adapt that model. The domain mismatch might not be ideal, but it’s a start.\n",
    "\n",
    "5. Crowdsourcing or Domain-Expert Partial Labeling\n",
    "If you have a small group of domain experts (e.g., med students, colleagues with subject knowledge), you can do a labeling sprint:\n",
    "\n",
    "Each person labels 100–200 examples.\n",
    "Combine them into a minimal but “gold quality” dataset.\n",
    "If the text is short enough, you might use a crowdsourcing platform (like Amazon Mechanical Turk). But for deeply biomedical text, crowd workers might not parse the domain-specific content properly—expert labeling is usually more accurate.\n",
    "\n",
    "6. Summarizing a Feasible Path\n",
    "Given you said: “I do not have a way to manually label,” you might choose:\n",
    "\n",
    "Heuristic-based labeling:\n",
    "Construct or gather a domain dictionary indicating “positive” vs. “negative” words.\n",
    "Auto-label ~10,000 articles.\n",
    "Accept that some fraction might be mislabeled (noise).\n",
    "Train a logistic regression or SVM on these auto-labeled (weakly supervised) examples.\n",
    "Evaluate it qualitatively or on a smaller manually-labeled sample if possible.\n",
    "This method fulfills the “supervised approach” requirement, even if your labels aren’t perfect. You can highlight that in your report:\n",
    "\n",
    "“Due to lack of manual labeling resources, we used a heuristic-based approach to label texts with certain keywords. The resulting dataset was used to train a logistic regression classifier. We acknowledge the potential noise in these labels but it demonstrates how a supervised pipeline can be employed with minimal human annotation.”\n",
    "7. In Conclusion\n",
    "Yes, you do need labeled data for a supervised approach.\n",
    "If you absolutely can’t label, consider a weak supervision scheme or minimal manual labeling.\n",
    "This will let you train a model and meet the requirement for a supervised method.\n",
    "In your final project, you can highlight the limitations of auto-labeling vs. a carefully curated gold standard.\n",
    "Key: The “point” is to show you can build a supervised pipeline. Even if your data is mostly neutral and you lack direct labels, you can approximate or gather a minimal labeled set. That’s typically how professional data scientists handle sentiment analysis in a domain with little or no labeled data: creative heuristics, partial annotation, or domain expansions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a detailed explanation of how to implement weak supervision (auto-labeling) for sentiment in biomedical text, plus whether scispaCy is usable for sentiment analysis in this domain. I’ll outline practical steps and tools you can use, especially if you have no labeled data, want to create a “silver standard,” or want to incorporate domain knowledge.\n",
    "\n",
    "1. What Is Weak Supervision (Auto-Labeling)?\n",
    "Weak supervision (sometimes called distant supervision or auto-labeling) is a technique to generate approximate or noisy labels for your data without manually labeling each example. You create a set of rules or heuristics that label each text as positive, negative, or neutral, effectively bootstrapping a training set for your supervised classifier. It’s not perfect, but it can be enough to train an initial model if you have no direct labeled data.\n",
    "\n",
    "1.1. Simple Heuristic Example\n",
    "Positive keywords: “successful,” “improved,” “beneficial,” “effective,” “safe,” “significant improvement,” etc.\n",
    "Negative keywords: “fatal,” “adverse,” “complication,” “increased risk,” “worse outcome,” “failure,” “severe,” “mortality,” etc.\n",
    "If the text contains ≥1 strongly negative phrase → label “negative.”\n",
    "If the text contains ≥1 strongly positive phrase → label “positive.”\n",
    "Otherwise → label “neutral.”\n",
    "You can refine the rules, e.g., if the text matches both negative and positive keywords, maybe neutral or whichever is “stronger.” This is your “weak supervision” approach.\n",
    "\n",
    "Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "positive_terms = [\n",
    "    r\"successful\", r\"improved\", r\"beneficial\", r\"effective\", r\"safe\", r\"significant improvement\"\n",
    "]\n",
    "negative_terms = [\n",
    "    r\"fatal\", r\"adverse\", r\"complication\", r\"failure\", r\"severe\", r\"mortality\",\n",
    "    r\"increased risk\", r\"worse outcome\"\n",
    "]\n",
    "\n",
    "def weak_label_text(text):\n",
    "    text_lower = text.lower()  # simple approach\n",
    "    # Search for positive or negative keywords\n",
    "    found_pos = any(re.search(term, text_lower) for term in positive_terms)\n",
    "    found_neg = any(re.search(term, text_lower) for term in negative_terms)\n",
    "    \n",
    "    if found_pos and not found_neg:\n",
    "        return \"positive\"\n",
    "    elif found_neg and not found_pos:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "df[\"weak_label\"] = df[\"abstract\"].fillna(\"\").apply(weak_label_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce labels. They’ll be noisy because domain text might have complicated language. But it’s better than zero labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. More Advanced Approaches (Snorkel, etc.)\n",
    "Snorkel is a framework from Stanford for building more sophisticated label functions that can overlap or conflict, then it uses a label model to produce a more consistent final label. This is recommended if you want to scale up rule-based labeling with advanced conflict resolution.\n",
    "\n",
    "Snorkel can combine multiple heuristics:\n",
    "E.g., “If text mentions improve AND does not mention risk, label positive.”\n",
    "Weighted or conflict rules.\n",
    "The label model estimates the accuracy/overlap of each rule function.\n",
    "But at a high level, the concept is the same: you end up with “silver standard” labeled data to train a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Training a Supervised Classifier from Weak Labels\n",
    "Once you have these auto-labeled (weak-labeled) examples, you can:\n",
    "\n",
    "Split them into train/dev/test or do cross-validation.\n",
    "Use a standard approach like:\n",
    "TfidfVectorizer + Logistic Regression / SVM, or\n",
    "BERT-based approach (if you can handle the domain complexity and have enough examples).\n",
    "Your model will learn from these imperfect labels. The final classifier might generalize somewhat better than your original rules alone, because the classifier can pick up additional patterns.\n",
    "\n",
    "3. Using scispaCy for Sentiment?\n",
    "scispaCy provides:\n",
    "\n",
    "Domain tokenization\n",
    "POS tagging\n",
    "NER models for biomedical text (like en_ner_bc5cdr_md for diseases and chemicals).\n",
    "It does NOT provide an out-of-the-box sentiment pipeline. So you can’t just do nlp(text)._.sentiment in scispaCy. Instead, you can use scispaCy to get domain-aware tokens, lemma, etc., then feed those tokens or embeddings into your own sentiment model. Some suggestions:\n",
    "\n",
    "Domain Tokenization: Instead of standard spaCy English, load en_core_sci_sm or en_core_sci_md, which handle biomedical text better. Then your doc object might have more accurate token boundaries for domain terms.\n",
    "NER from scispaCy**: You can identify diseases, chemicals, genes, etc. If you see certain negative or positive disease mentions, that might feed into your labeling rules or supervised features.\n",
    "Vectors from en_core_sci_md**: This model has ~50k word vectors specialized for biomedical text. You can convert tokens to these vectors for your supervised classifier if you prefer that to TF-IDF.\n",
    "But scispaCy does not do sentiment classification on its own. You still must define or train a sentiment approach.\n",
    "\n",
    "4. Putting It All Together: Proposed Pipeline\n",
    "Data\n",
    "\n",
    "You have 1M+ biomedical articles (title, abstract).\n",
    "Possibly store them in chunked Parquet for memory reasons.\n",
    "Weak Labeling\n",
    "\n",
    "a) Define keyword-based rules for “positive,” “negative,” “neutral.”\n",
    "b) Label the entire dataset or a big chunk.\n",
    "c) Accept it’s noisy but large scale.\n",
    "Train a Classifier\n",
    "\n",
    "a) Use scispaCy to tokenize if you want domain tokenization.\n",
    "b) Extract features:\n",
    "Option A: TF-IDF on the text.\n",
    "Option B: scispaCy “md” or “lg” vectors as embeddings for each token. Possibly average them.\n",
    "c) Fit an SVM, Logistic Regression, or a simple neural net.\n",
    "d) Evaluate on a small manually-labeled set if possible, or do cross-validation on your weak-labeled data.\n",
    "Analysis\n",
    "\n",
    "See how many get labeled positive/negative. Possibly everything is mostly neutral.\n",
    "If you do find partial positives or negatives, that might mean your domain text specifically mentions improvements or severe complications.\n",
    "5. Additional Considerations\n",
    "5.1. Domain-Specific Dictionary\n",
    "You can improve your weak supervision rules by including medical synonyms:\n",
    "\n",
    "Negative: “adverse event,” “complications,” “mortality,” “fatal,” “toxic,” …\n",
    "Positive: “improved survival,” “beneficial,” “safe,” “well-tolerated,” “positive outcome,” …\n",
    "5.2. Combine Lexicon + scispaCy Entities\n",
    "If you see that an article mentions “severe disease progression,” you might treat that as negative. If it mentions “successful therapy,” treat as positive. scispaCy NER can help you identify DISEASE or CHEMICAL mentions, but you still need your own rules or classifier for sentiment.\n",
    "\n",
    "5.3. If You Obtain Some Labeled Data\n",
    "Even 100–200 manual labels can help you refine or check your heuristic approach:\n",
    "\n",
    "Compare how many times your auto-labeled data matches the small gold standard.\n",
    "Possibly reweight or re-engineer your rules to reduce false positives.\n",
    "6. Example Code Skeleton for Weak Labeling + scispaCy (High-Level)\n",
    "python\n",
    "Copy code\n",
    "import re\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1) Load scispaCy model for domain tokenization (optional)\n",
    "nlp = spacy.load(\"en_core_sci_md\")\n",
    "\n",
    "# 2) Define your negative/positive patterns\n",
    "pos_patterns = [r\"successful\", r\"improved\", r\"beneficial\", r\"safe\", r\"positive outcome\", ...]\n",
    "neg_patterns = [r\"fatal\", r\"adverse\", r\"complication\", r\"failure\", r\"severe\", r\"mortality\", ...]\n",
    "\n",
    "def auto_label_biomedical(text):\n",
    "    text_lower = text.lower()\n",
    "    found_pos = any(re.search(p, text_lower) for p in pos_patterns)\n",
    "    found_neg = any(re.search(p, text_lower) for p in neg_patterns)\n",
    "    if found_pos and not found_neg:\n",
    "        return \"positive\"\n",
    "    elif found_neg and not found_pos:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# 3) Suppose you read your data chunk by chunk\n",
    "# for chunk_df in read_in_chunks(...):\n",
    "#     chunk_df[\"weak_label\"] = chunk_df[\"abstract\"].apply(auto_label_biomedical)\n",
    "#     # Save or combine\n",
    "Then train a classifier:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df_weak = df[ df[\"weak_label\"] != \"neutral\" ]  # or keep all, depends on distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_weak[\"abstract\"], df_weak[\"weak_label\"], ...)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "clf = SVC()\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "print(\"Accuracy on test:\", clf.score(X_test_vec, y_test))\n",
    "Summary\n",
    "Weak supervision (auto-labeling) can be done with keyword-based or more advanced rule-based approaches.\n",
    "This yields a noisy but large labeled dataset, letting you train a supervised sentiment classifier.\n",
    "scispaCy doesn’t have a built-in sentiment pipeline—it’s for tokenization, domain NER, or embeddings in the biomedical domain. You can use it to create domain tokens, but you still must define or train your own sentiment model.\n",
    "For best results, combine a small set of actual manual labels (if possible) with your heuristic approach to refine or evaluate the auto-labeled data.\n",
    "Thus, you can handle no-labeled-data scenarios by:\n",
    "\n",
    "Writing heuristics or dictionary rules → produce “weak labels.”\n",
    "Using scispaCy for domain tokenization or NER if desired.\n",
    "Training a supervised method on your auto-labeled “silver standard.”\n",
    "That is how you achieve a practical sentiment classification pipeline in biomedical text without having to manually label large volumes from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# SCRIPT 1: split_into_parquet.py\n",
    "###############################################################################\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_df_into_parquet_batches(\n",
    "    df: pd.DataFrame,\n",
    "    batch_size: int,\n",
    "    output_folder: str,\n",
    "    file_prefix: str = \"chunk_\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into multiple Parquet files (batch_size rows each),\n",
    "    storing them in 'output_folder'. Each file is named like 'chunk_1.parquet',\n",
    "    'chunk_2.parquet', etc.\n",
    "\n",
    "    A progress bar shows how many batches are being saved.\n",
    "\n",
    "    This script does NOT merge them back into a single file. The idea is to keep\n",
    "    each chunk separate so you can process them individually later.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    total_rows = len(df)\n",
    "    batch_count = (total_rows + batch_size - 1) // batch_size\n",
    "    print(f\"Splitting DF with {total_rows} rows into {batch_count} batches of size {batch_size}.\")\n",
    "\n",
    "    current_row = 0\n",
    "    batch_idx = 1\n",
    "\n",
    "    with tqdm(total=batch_count, desc=\"Saving Batches\", unit=\"batch\") as pbar:\n",
    "        while current_row < total_rows:\n",
    "            end_row = min(current_row + batch_size, total_rows)\n",
    "            df_batch = df.iloc[current_row:end_row]\n",
    "\n",
    "            chunk_filename = f\"{file_prefix}{batch_idx}.parquet\"\n",
    "            chunk_path = os.path.join(output_folder, chunk_filename)\n",
    "\n",
    "            df_batch.to_parquet(chunk_path, index=False)\n",
    "            \n",
    "            pbar.update(1)\n",
    "            print(f\"  -> Saved batch {batch_idx} rows [{current_row}:{end_row}] to {chunk_path}\")\n",
    "\n",
    "            current_row = end_row\n",
    "            batch_idx += 1\n",
    "\n",
    "    print(\"\\nAll done. Each batch is in its own .parquet file in:\", output_folder)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # EXAMPLE USAGE:\n",
    "    # Suppose you have a big DataFrame 'df_final' already in memory\n",
    "    # or you read it from somewhere.\n",
    "\n",
    "    # For demonstration, let's just create a small DataFrame:\n",
    "    import numpy as np\n",
    "\n",
    "    df_final = pd.DataFrame({\n",
    "        \"colA\": np.random.randint(0, 1000, 350_000),\n",
    "        \"colB\": [f\"Row {i}\" for i in range(350_000)]\n",
    "    })\n",
    "\n",
    "    # Decide where to store chunked Parquet files\n",
    "    out_folder = \"Data/2.Processed/ParquetChunks\"\n",
    "    prefix = \"chunk_\"\n",
    "    b_size = 100_000\n",
    "\n",
    "    split_df_into_parquet_batches(\n",
    "        df=df_final,\n",
    "        batch_size=b_size,\n",
    "        output_folder=out_folder,\n",
    "        file_prefix=prefix\n",
    "    )\n",
    "\n",
    "    print(\"Done splitting!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNORKEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
