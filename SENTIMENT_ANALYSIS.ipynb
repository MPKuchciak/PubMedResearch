{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SENTIMENT ANALYSIS PLAN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def read_parquet_in_batches_with_progress(file_path, batch_size):\n",
    "    \"\"\"\n",
    "    Read a Parquet file in fixed-size row batches with a progress bar and per-chunk logging.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the Parquet file.\n",
    "        batch_size (int): Number of rows per batch.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame after processing all batches.\n",
    "    \"\"\"\n",
    "    # Open the Parquet file\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    \n",
    "    # Total number of rows in the file\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    \n",
    "    # Initialize a list to store DataFrame chunks\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    with tqdm(total=total_rows, desc=\"Processing Batches\", unit=\"rows\") as pbar:\n",
    "        # Enumerate batches for logging\n",
    "        for batch_number, batch in enumerate(parquet_file.iter_batches(batch_size=batch_size), start=1):\n",
    "            # Convert the batch to a Pandas DataFrame\n",
    "            df_batch = batch.to_pandas()\n",
    "            \n",
    "            # Simulate processing (add your custom logic here)\n",
    "            all_chunks.append(df_batch)\n",
    "            \n",
    "            # Update the progress bar\n",
    "            pbar.update(len(df_batch))\n",
    "            \n",
    "            # Print per-chunk information\n",
    "            print(f\"Processed Chunk {batch_number}: {len(df_batch)} rows\")\n",
    "    \n",
    "    # Combine all chunks into a single DataFrame\n",
    "    combined_df = pd.concat(all_chunks, ignore_index=True)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"Data/2.Processed/ModellingData/P5_final_new.parquet\"\n",
    "    batch_size = 100_000  # Define your desired chunk size\n",
    "    \n",
    "    df = read_parquet_in_batches_with_progress(file_path, batch_size)\n",
    "    \n",
    "    print(f\"\\nFinal DataFrame with {len(df)} rows:\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lexicon-Based Approach\n",
    "1.1 Why a Lexicon-Based Method?\n",
    "No training data required. It uses a dictionary (lexicon) of words mapped to sentiment scores (positive, negative, neutral).\n",
    "Quick to implement, can provide a baseline or unsupervised vantage.\n",
    "Commonly used library: VADER (suitable for short social media–style text but can be adapted) or SentiWordNet (a more general WordNet-based approach).\n",
    "VADER (if your text is not deeply domain-specific)\n",
    "Works decently on short, informal text.\n",
    "If your text is more scientific (like PubMed titles/abstracts), you may find many neutral or domain words not recognized by VADER. Still, it can serve as a demonstration approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CODE: LEXICON-BASED (VADER)\n",
    "###############################################################################\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# 1) Download the VADER lexicon if needed\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def lexicon_based_vader(text):\n",
    "    \"\"\"\n",
    "    Return sentiment scores for the given text using VADER.\n",
    "    \"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    scores = sid.polarity_scores(text)\n",
    "    # scores is dict with {compound, neg, neu, pos}\n",
    "    # For a single label, you might pick:\n",
    "    # 'positive' if compound >= 0.05, 'negative' if compound <= -0.05, else 'neutral'\n",
    "    return scores\n",
    "\n",
    "# Suppose df has a column \"title\" with your text\n",
    "# We'll create columns for VADER sentiment\n",
    "df[\"vader_scores\"] = df[\"title\"].apply(lexicon_based_vader)\n",
    "\n",
    "# Optionally parse them into one label\n",
    "def get_vader_label(scores):\n",
    "    compound = scores[\"compound\"]\n",
    "    if compound >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif compound <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "df[\"vader_label\"] = df[\"vader_scores\"].apply(get_vader_label)\n",
    "\n",
    "print(df[[\"title\", \"vader_scores\", \"vader_label\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Alternative: SentiWordNet\n",
    "If your text is more formal or domain-based, you might want to explore SentiWordNet or a domain-specific lexicon. The logic is similar, but you’d look up each word’s positivity/negativity in the SentiWordNet dictionary, summing or averaging them.\n",
    "Justification\n",
    "Lexicon-based methods are quick for an unsupervised sentiment estimate.\n",
    "They can fail in domain-specific contexts (e.g., biomedical text might mention “cancer” or “infection,” which are negative in a lay sense but may be neutral from a purely scientific standpoint).\n",
    "This is why a supervised approach may be more accurate if you have labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Supervised Machine Learning Approach\n",
    "2.1 Why a Supervised Method?\n",
    "You’ll train a model on labeled examples of text → sentiment (pos/neg/neu).\n",
    "This typically yields better results than lexicon-based if you have enough labeled data.\n",
    "Common supervised classifiers: Logistic Regression, SVM, Naive Bayes, or even a fine-tuned BERT.\n",
    "2.2 Example with Logistic Regression\n",
    "Steps:\n",
    "\n",
    "Gather labeled data: You need text + a sentiment label. Perhaps you label 1000+ random samples as positive/negative/neutral.\n",
    "Feature extraction:\n",
    "A simple approach uses TF-IDF or CountVectorizer on the text.\n",
    "More advanced: use pretrained embeddings (e.g., BERT) as features.\n",
    "Train a scikit-learn classifier (LogReg or SVM).\n",
    "Predict on unseen text.\n",
    "Example Code (Basic TF-IDF + Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# CODE: SUPERVISED (LOGREG + TF-IDF)\n",
    "###############################################################################\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppose df has columns: \"title\" (text) and \"label\" (pos/neg/neu)\n",
    "# This is your labeled dataset\n",
    "\n",
    "# 1) Split train/test\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2) TF-IDF on the text\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df[\"title\"])\n",
    "y_train = train_df[\"label\"]\n",
    "\n",
    "X_test = vectorizer.transform(test_df[\"title\"])\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# 3) Train a Logistic Regression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 4) Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justification:\n",
    "\n",
    "Logistic Regression is a common baseline supervised approach for sentiment analysis.\n",
    "TF-IDF is quick to implement and typically effective for textual classification if domain-specific embeddings aren’t available.\n",
    "2.3 Possible Upgrades\n",
    "SVM or RandomForest: Slight variations in performance.\n",
    "Neural approach with BERT-based embeddings: If you have enough data/time.\n",
    "If your text is domain-specific, consider a SciBERT or BioBERT for embeddings or fine-tuning.\n",
    "3. Combining or Reporting\n",
    "You’re “obliged to perform sentiment analysis using at least one lexicon-based approach and at least one supervised technique.” So in your final report, you can:\n",
    "\n",
    "Implement VADER for the lexicon-based method.\n",
    "Implement Logistic Regression (or SVM) for the supervised method.\n",
    "Compare performance on the same test set (requires labeled data for the supervised approach).\n",
    "Justify choices:\n",
    "VADER is quick, well-known, but might not handle domain terms well.\n",
    "Logistic Regression is a standard baseline for text classification, interpretable, typically robust.\n",
    "If your text is heavily domain-specific (scientific, biomedical), mention that both methods might have limitations:\n",
    "\n",
    "Lexicon: might incorrectly classify domain words.\n",
    "Logistic Regression: requires a domain-labeled dataset.\n",
    "Still, this approach meets the stated requirement: one lexicon-based, one supervised approach, plus a reasoned justification.\n",
    "\n",
    "4. Considering Neutral vs. Non-Neutral\n",
    "If your data is primarily neutral (like many scientific abstracts), the distribution of sentiment might be heavily skewed. You can either:\n",
    "\n",
    "Keep a 3-class system (pos/neg/neu).\n",
    "Merge pos/neg into non-neutral vs. neutral.\n",
    "Provide an analysis of how many are likely to be neutral, if that’s the main interest.\n",
    "Either approach is valid, but note that heavily neutral data can reduce your classifier’s performance if you have few positive/negative examples.\n",
    "\n",
    "5. Potential Models Summarized\n",
    "Lexicon-based:\n",
    "\n",
    "VADER if text is general or social media–like.\n",
    "SentiWordNet or other dictionary for more formal text.\n",
    "Bio domain: Possibly no major out-of-the-box lexicon for sentiment, so VADER is a fallback.\n",
    "Supervised:\n",
    "\n",
    "Logistic Regression or SVM with TF-IDF → easy to implement, relatively fast.\n",
    "If large labeled data + domain complexity → fine-tuned BERT (e.g., SciBERT or BioBERT). But that’s more advanced in setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "Yes, you can approach your data with two methods:\n",
    "Lexicon-based (like VADER) for an unsupervised baseline,\n",
    "Supervised (like Logistic Regression) for better domain accuracy if you have labeled data.\n",
    "Keep in mind domain-specific challenges if your text is specialized.\n",
    "If your data is mostly neutral, analyzing pos/neg signals might require a large labeled set or more sophisticated domain lexicons.\n",
    "This satisfies the requirement to use “at least one lexicon-based approach and at least one supervised machine learning technique”, plus justification for each choice.\n",
    "With these code blocks and the rationale above, you can implement both methods, compare them, and then summarize in your final report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
