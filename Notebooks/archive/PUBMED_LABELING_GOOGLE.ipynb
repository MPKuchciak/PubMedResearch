{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e703961-c67c-4555-8ed5-edab22306ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers tqdm gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e8ca64-dfd3-4c33-b2da-709eab513e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1+cu117\n",
      "CUDA available: True\n",
      "GPU detected: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU detected:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a57ea40-a9d3-4be0-9692-36e2000547c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c83436b-6d07-4820-9ff4-4d27b5961d75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1+cu117\n",
      "CUDA available: True\n",
      "GPU detected: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU detected:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb5db7b4-3261-4248-a881-f1eba65e352c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_long_text_biomed(\n",
    "    text: str,\n",
    "    pipe,\n",
    "    max_length=512,\n",
    "    stride=256\n",
    "):\n",
    "    tokenizer = pipe.tokenizer\n",
    "    input_ids = tokenizer(text, add_special_tokens=True)[\"input_ids\"]\n",
    "    total_tokens = len(input_ids)\n",
    "\n",
    "    if total_tokens <= max_length:\n",
    "        return pipe(text)[0]\n",
    "\n",
    "    subchunk_sentiments = []\n",
    "    start = 0\n",
    "    while start < total_tokens:\n",
    "        end = start + max_length\n",
    "        sub_ids = input_ids[start:end]\n",
    "        sub_text = tokenizer.decode(sub_ids, skip_special_tokens=True)\n",
    "\n",
    "        sub_res = pipe(sub_text)\n",
    "        subchunk_sentiments.append(sub_res[0])\n",
    "\n",
    "        if end >= total_tokens:\n",
    "            break\n",
    "        start += max_length - stride\n",
    "\n",
    "    label_counts = {}\n",
    "    for chunk_res in subchunk_sentiments:\n",
    "        lbl = chunk_res[\"label\"]\n",
    "        label_counts[lbl] = label_counts.get(lbl, 0) + 1\n",
    "\n",
    "    overall_label = max(label_counts, key=label_counts.get)\n",
    "    overall_score = label_counts[overall_label] / len(subchunk_sentiments)\n",
    "    return {\"label\": overall_label, \"score\": overall_score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a26e19aa-1063-4cf1-b2dd-aeba47beb28d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_chunk(\n",
    "    chunk_file: str,\n",
    "    output_folder: str,\n",
    "    biomedbert_model: str,\n",
    "    max_length: int = 512,\n",
    "    stride: int = 256,\n",
    "    batch_size: int = 500,  # Process in batches\n",
    "    text_column: str = \"abstract\",\n",
    "    uid_column: str = \"uid\"\n",
    "):\n",
    "    output_file = f\"{output_folder}/labeled_{os.path.basename(chunk_file)}\"\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    if fs.exists(output_file):\n",
    "        print(f\"Chunk {chunk_file} already processed. Skipping.\")\n",
    "        return\n",
    "\n",
    "    with fs.open(chunk_file, 'rb') as f:\n",
    "        df_chunk = pd.read_parquet(f)\n",
    "    print(f\"Processing {len(df_chunk)} rows from {chunk_file}...\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(biomedbert_model)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(biomedbert_model)\n",
    "    pipe = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        device=0  # Use GPU\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for start in tqdm(range(0, len(df_chunk), batch_size), desc=\"Batch Processing\"):\n",
    "        sub_batch = df_chunk.iloc[start:start + batch_size]\n",
    "        for _, row in sub_batch.iterrows():\n",
    "            prediction = predict_long_text_biomed(\n",
    "                text=row[text_column],\n",
    "                pipe=pipe,\n",
    "                max_length=max_length,\n",
    "                stride=stride\n",
    "            )\n",
    "            results.append({\n",
    "                \"UID\": row[uid_column],\n",
    "                \"label\": prediction[\"label\"],\n",
    "                \"score\": prediction[\"score\"]\n",
    "            })\n",
    "\n",
    "    output_df = pd.DataFrame(results)\n",
    "    with fs.open(output_file, 'wb') as f:\n",
    "        output_df.to_parquet(f, index=False)\n",
    "    print(f\"Saved labeled chunk to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a166ebc-1ba4-4049-b302-9d078d1bf34f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def biomedbert_sequential_processing(\n",
    "    input_folder: str,\n",
    "    output_folder: str,\n",
    "    biomedbert_model: str,\n",
    "    max_length: int = 512,\n",
    "    stride: int = 256,\n",
    "    text_column: str = \"abstract\",\n",
    "    uid_column: str = \"uid\",\n",
    "    specific_chunk: str = None\n",
    "):\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    chunk_files = [\n",
    "        f for f in fs.glob(f\"{input_folder}/*.parquet\")\n",
    "    ]\n",
    "\n",
    "    if specific_chunk:\n",
    "        process_chunk(\n",
    "            chunk_file=specific_chunk,\n",
    "            output_folder=output_folder,\n",
    "            biomedbert_model=biomedbert_model,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            text_column=text_column,\n",
    "            uid_column=uid_column\n",
    "        )\n",
    "    else:\n",
    "        for chunk_file in tqdm(chunk_files, desc=\"Processing Chunks\"):\n",
    "            process_chunk(\n",
    "                chunk_file=chunk_file,\n",
    "                output_folder=output_folder,\n",
    "                biomedbert_model=biomedbert_model,\n",
    "                max_length=max_length,\n",
    "                stride=stride,\n",
    "                text_column=text_column,\n",
    "                uid_column=uid_column\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5ca81-afda-4121-8b33-45c993a53974",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk pubmed_123/sent_chunks/chunk_1.parquet already processed. Skipping.\n",
      "Processing 100000 rows from pubmed_123/sent_chunks/chunk_10.parquet...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c852ef28d8942abb54c5d47e0a6074f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2f0692402249fe91160942ac71a1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch Processing:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[AYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "\n",
      "Batch Processing:   0%|          | 1/200 [00:12<42:06, 12.70s/it]\u001b[A\n",
      "Batch Processing:   1%|          | 2/200 [00:24<39:50, 12.07s/it]\u001b[A\n",
      "Batch Processing:   2%|▏         | 3/200 [00:35<38:25, 11.70s/it]\u001b[A\n",
      "Batch Processing:   2%|▏         | 4/200 [00:48<39:08, 11.98s/it]\u001b[A\n",
      "Batch Processing:   2%|▎         | 5/200 [01:00<39:17, 12.09s/it]\u001b[A\n",
      "Batch Processing:   3%|▎         | 6/200 [01:11<38:15, 11.83s/it]\u001b[A\n",
      "Batch Processing:   4%|▎         | 7/200 [01:23<37:51, 11.77s/it]\u001b[A\n",
      "Batch Processing:   4%|▍         | 8/200 [01:34<37:02, 11.58s/it]\u001b[A\n",
      "Batch Processing:   4%|▍         | 9/200 [01:46<37:02, 11.63s/it]\u001b[A\n",
      "Batch Processing:   5%|▌         | 10/200 [01:58<37:23, 11.81s/it]\u001b[A\n",
      "Batch Processing:   6%|▌         | 11/200 [02:10<37:37, 11.94s/it]\u001b[A\n",
      "Batch Processing:   6%|▌         | 12/200 [02:21<36:47, 11.74s/it]\u001b[A\n",
      "Batch Processing:   6%|▋         | 13/200 [02:34<37:19, 11.98s/it]\u001b[A\n",
      "Batch Processing:   7%|▋         | 14/200 [02:47<37:42, 12.17s/it]\u001b[A\n",
      "Batch Processing:   8%|▊         | 15/200 [02:59<37:23, 12.13s/it]\u001b[A\n",
      "Batch Processing:   8%|▊         | 16/200 [03:10<36:44, 11.98s/it]\u001b[A\n",
      "Batch Processing:   8%|▊         | 17/200 [03:22<36:39, 12.02s/it]\u001b[A\n",
      "Batch Processing:   9%|▉         | 18/200 [03:35<37:17, 12.29s/it]\u001b[A\n",
      "Batch Processing:  10%|▉         | 19/200 [03:48<37:03, 12.29s/it]\u001b[A\n",
      "Batch Processing:  10%|█         | 20/200 [03:59<35:54, 11.97s/it]\u001b[A\n",
      "Batch Processing:  10%|█         | 21/200 [04:12<36:29, 12.23s/it]\u001b[A\n",
      "Batch Processing:  11%|█         | 22/200 [04:25<37:16, 12.57s/it]\u001b[A\n",
      "Batch Processing:  12%|█▏        | 23/200 [04:37<36:24, 12.34s/it]\u001b[A\n",
      "Batch Processing:  12%|█▏        | 24/200 [04:49<35:45, 12.19s/it]\u001b[A\n",
      "Batch Processing:  12%|█▎        | 25/200 [05:01<35:40, 12.23s/it]\u001b[A\n",
      "Batch Processing:  13%|█▎        | 26/200 [05:13<35:38, 12.29s/it]\u001b[A\n",
      "Batch Processing:  14%|█▎        | 27/200 [05:26<35:35, 12.34s/it]\u001b[A\n",
      "Batch Processing:  14%|█▍        | 28/200 [05:38<35:21, 12.34s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_dir = \"gs://pubmed_123/sent_chunks\"\n",
    "    output_dir = \"gs://pubmed_123/labeled_chunks\"\n",
    "    biomedbert_model = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\"\n",
    "\n",
    "    specific_chunk = None  # Example: \"gs://pubmed_123/sent_chunks/sent_chunks_chunk_1.parquet\"\n",
    "\n",
    "    biomedbert_sequential_processing(\n",
    "        input_folder=input_dir,\n",
    "        output_folder=output_dir,\n",
    "        biomedbert_model=biomedbert_model,\n",
    "        max_length=512,\n",
    "        stride=256,\n",
    "        text_column=\"abstract\",\n",
    "        uid_column=\"uid\",\n",
    "        specific_chunk=specific_chunk\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2f264-1177-427c-9d73-4fb6b3804580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
