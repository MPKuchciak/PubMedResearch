{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data, Gaining Some Insights and Preparing Tokenizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "Sure, here is a summary of the entire Jupyter Notebook:\n",
    "\n",
    "1. **Markdown Cells:**\n",
    "    - Titles and section headers to organize the notebook.\n",
    "    - Descriptions and explanations of various steps and processes.\n",
    "\n",
    "2. **Imports and Setup:**\n",
    "    - Import necessary libraries such as `pandas`, `pyarrow.parquet`, `tqdm`, `matplotlib.pyplot`, `scispacy`, `spacy`, `transformers`, `nltk`, etc.\n",
    "    - Define utility functions for reading and saving parquet files in batches.\n",
    "    - Define functions for tokenization using simple and Hugging Face tokenizers.\n",
    "    - Define functions for disease detection using dictionary-based and SciSpacy approaches.\n",
    "\n",
    "3. **Data Loading and Initial Processing:**\n",
    "    - Load the main dataset from a parquet file.\n",
    "    - Perform initial data cleaning, such as removing rows with missing abstracts and filtering rows based on date.\n",
    "    - Select relevant columns for further processing.\n",
    "\n",
    "4. **Tokenization:**\n",
    "    - Tokenize the `title` and `abstract` columns using both simple and Hugging Face tokenizers.\n",
    "    - Remove stopwords and punctuation from the tokenized data.\n",
    "    - Analyze token frequencies and generate insights.\n",
    "\n",
    "5. **Disease Detection:**\n",
    "    - Implement dictionary-based disease detection by keeping only tokens present in a predefined disease dictionary.\n",
    "    - Implement SciSpacy-based disease detection using the `en_ner_bc5cdr_md` model to extract disease entities from the `title` and `abstract` columns.\n",
    "    - Analyze the results of disease detection, including counting the number of detected entities and categorizing rows based on the presence of entities.\n",
    "\n",
    "6. **Merging and Saving Results:**\n",
    "    - Merge the results of disease detection with the main dataset.\n",
    "    - Save the final merged dataset to a parquet file.\n",
    "\n",
    "7. **Additional Analysis:**\n",
    "    - Perform additional analysis on the merged dataset, such as counting the number of rows with entities in both `title` and `abstract` columns.\n",
    "    - Explore potential next steps, such as using UMLS or MeSH for further disease categorization.\n",
    "\n",
    "8. **Environment Management:**\n",
    "    - Reset the environment and release memory to ease computer work.\n",
    "\n",
    "Here is a summary of the key variables and their values:\n",
    "\n",
    "- `base_path`: `'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch'`\n",
    "- `batch_size`: `100000`\n",
    "- `col`: `'disease_abstract_spacy'`\n",
    "- `df`: DataFrame with 1,057,871 rows and 22 columns.\n",
    "- `file_path`: `'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch\\\\Data\\\\2.Processed\\\\ModellingData\\\\P5_final_new.parquet'`\n",
    "- `final_file`: `'P6_merged_tokens.parquet'`\n",
    "- `output_folder`: `'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch\\\\Data\\\\2.Processed\\\\ModellingData'`\n",
    "- `result_path`: `'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch\\\\Data\\\\2.Processed\\\\ModellingData\\\\P6_merged_tokens.parquet'`\n",
    "- `token_columns`: `['disease_title_tokens_simple', 'disease_title_tokens_hf', 'disease_abstract_tokens_simple', 'disease_abstract_tokens_hf', 'disease_title_spacy', 'disease_mesh_terms_spacy', 'disease_abstract_spacy']`\n",
    "\n",
    "This summary provides an overview of the notebook's structure, key steps, and important variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markdown Cells:\n",
    "\n",
    "Titles and section headers to organize the notebook.\n",
    "Descriptions and explanations of various steps and processes.\n",
    "Imports and Setup:\n",
    "\n",
    "Import necessary libraries such as pandas, pyarrow.parquet, tqdm, matplotlib.pyplot, scispacy, spacy, transformers, nltk, etc.\n",
    "Define utility functions for reading and saving parquet files in batches. These functions are used multiple times throughout the notebook to handle large datasets efficiently.\n",
    "Define functions for tokenization using simple and Hugging Face tokenizers. Tokenization is a crucial step for text processing and analysis.\n",
    "Define functions for disease detection using dictionary-based and SciSpacy approaches. SciSpacy is a specialized library for biomedical text processing, and it helps in extracting disease entities from text.\n",
    "Data Loading and Initial Processing:\n",
    "\n",
    "Load the main dataset from a parquet file. Parquet is a columnar storage file format optimized for large-scale data processing.\n",
    "Perform initial data cleaning, such as removing rows with missing abstracts and filtering rows based on date. This ensures the dataset is clean and relevant for further analysis.\n",
    "Select relevant columns for further processing to focus on the necessary data.\n",
    "Tokenization:\n",
    "\n",
    "Tokenize the title and abstract columns using both simple and Hugging Face tokenizers. Tokenization breaks down text into individual tokens (words or subwords).\n",
    "Remove stopwords and punctuation from the tokenized data to focus on meaningful words.\n",
    "Analyze token frequencies and generate insights to understand the most common terms in the dataset.\n",
    "Disease Detection:\n",
    "\n",
    "Implement dictionary-based disease detection by keeping only tokens present in a predefined disease dictionary. This method relies on a predefined list of disease terms.\n",
    "Implement SciSpacy-based disease detection using the en_ner_bc5cdr_md model to extract disease entities from the title and abstract columns. SciSpacy uses advanced NLP techniques to identify biomedical entities.\n",
    "Analyze the results of disease detection, including counting the number of detected entities and categorizing rows based on the presence of entities.\n",
    "Merging and Saving Results:\n",
    "\n",
    "Merge the results of disease detection with the main dataset to create a comprehensive dataset with both original and processed data.\n",
    "Save the final merged dataset to a parquet file for future use.\n",
    "Additional Analysis:\n",
    "\n",
    "Perform additional analysis on the merged dataset, such as counting the number of rows with entities in both title and abstract columns.\n",
    "Explore potential next steps, such as using UMLS or MeSH for further disease categorization. These are standardized vocabularies for biomedical terms.\n",
    "Environment Management:\n",
    "\n",
    "Reset the environment and release memory to ease computer work. This includes multiple instances of gc.collect() to manually trigger garbage collection and free up RAM.\n",
    "Key Variables and Their Values:\n",
    "\n",
    "base_path: 'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch'\n",
    "batch_size: 100000\n",
    "col: 'disease_abstract_spacy'\n",
    "df: DataFrame with 1,057,871 rows and 22 columns.\n",
    "file_path: 'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch\\\\Data\\\\2.Processed\\\\ModellingData\\\\P5_final_new.parquet'\n",
    "final_file: 'P6_merged_tokens.parquet'\n",
    "output_folder: 'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch\\\\Data\\\\2.Processed\\\\ModellingData'\n",
    "result_path: 'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch\\\\Data\\\\2.Processed\\\\ModellingData\\\\P6_merged_tokens.parquet'\n",
    "token_columns: ['disease_title_tokens_simple', 'disease_title_tokens_hf', 'disease_abstract_tokens_simple', 'disease_abstract_tokens_hf', 'disease_title_spacy', 'disease_mesh_terms_spacy', 'disease_abstract_spacy']\n",
    "This extended summary provides a comprehensive overview of the notebook's structure, key steps, tools used, and important variables, along with additional details on the purpose and usage of various components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "Sure, here is a summary of the entire Jupyter Notebook:\n",
    "\n",
    "1. **Markdown Cells:**\n",
    "    - Titles and section headers to organize the notebook.\n",
    "    - Descriptions and explanations of various steps and processes.\n",
    "\n",
    "2. **Imports and Setup:**\n",
    "    - Import necessary libraries such as `pandas`, `pyarrow.parquet`, `tqdm`, `matplotlib.pyplot`, `scispacy`, `spacy`, `transformers`, `nltk`, etc.\n",
    "    - Define utility functions for reading and saving parquet files in batches.\n",
    "    - Define functions for tokenization using simple and Hugging Face tokenizers.\n",
    "    - Define functions for disease detection using dictionary-based and SciSpacy approaches.\n",
    "\n",
    "3. **Data Loading and Initial Processing:**\n",
    "    - Load the main dataset from a parquet file.\n",
    "    - Perform initial data cleaning, such as removing rows with missing abstracts and filtering rows based on date.\n",
    "    - Select relevant columns for further processing.\n",
    "\n",
    "4. **Tokenization:**\n",
    "    - Tokenize the `title` and `abstract` columns using both simple and Hugging Face tokenizers.\n",
    "    - Remove stopwords and punctuation from the tokenized data.\n",
    "    - Analyze token frequencies and generate insights.\n",
    "\n",
    "5. **Disease Detection:**\n",
    "    - Implement dictionary-based disease detection by keeping only tokens present in a predefined disease dictionary.\n",
    "    - Implement SciSpacy-based disease detection using the `en_ner_bc5cdr_md` model to extract disease entities from the `title` and `abstract` columns.\n",
    "    - Analyze the results of disease detection, including counting the number of detected entities and categorizing rows based on the presence of entities.\n",
    "\n",
    "6. **Merging and Saving Results:**\n",
    "    - Merge the results of disease detection with the main dataset.\n",
    "    - Save the final merged dataset to a parquet file.\n",
    "\n",
    "7. **Additional Analysis:**\n",
    "    - Perform additional analysis on the merged dataset, such as counting the number of rows with entities in both `title` and `abstract` columns.\n",
    "    - Explore potential next steps, such as using UMLS or MeSH for further disease categorization.\n",
    "\n",
    "8. **Environment Management:**\n",
    "    - Reset the environment and release memory to ease computer work.\n",
    "\n",
    "Here is a summary of the key variables and their values:\n",
    "\n",
    "- `base_path`: `'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch'`\n",
    "- `batch_size`: `100000`\n",
    "- `col`: `'disease_abstract_spacy'`\n",
    "- `df`: DataFrame with 1,057,871 rows and 22 columns.\n",
    "- `file_path`: `'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch\\\\Data\\\\2.Processed\\\\ModellingData\\\\P5_final_new.parquet'`\n",
    "- `final_file`: `'P6_merged_tokens.parquet'`\n",
    "- `output_folder`: `'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch\\\\Data\\\\2.Processed\\\\ModellingData'`\n",
    "- `result_path`: `'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch\\\\Data\\\\2.Processed\\\\ModellingData\\\\P6_merged_tokens.parquet'`\n",
    "- `token_columns`: `['disease_title_tokens_simple', 'disease_title_tokens_hf', 'disease_abstract_tokens_simple', 'disease_abstract_tokens_hf', 'disease_title_spacy', 'disease_mesh_terms_spacy', 'disease_abstract_spacy']`\n",
    "\n",
    "This summary provides an overview of the notebook's structure, key steps, and important variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "Sure, here is an enhanced summary of the entire Jupyter Notebook, including the purposes of each section and the overall objective:\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of the Jupyter Notebook\n",
    "\n",
    "### Purpose:\n",
    "This Jupyter Notebook is designed to process a large dataset of PubMed abstracts, perform tokenization, detect disease entities, and analyze the results. The ultimate goal is to gain insights into the prevalence and distribution of diseases mentioned in the abstracts.\n",
    "\n",
    "### Structure and Key Steps:\n",
    "\n",
    "1. **Markdown Cells:**\n",
    "    - **Purpose:** Organize the notebook with titles and section headers.\n",
    "    - **Content:** Descriptions and explanations of various steps and processes to guide the reader.\n",
    "\n",
    "2. **Imports and Setup:**\n",
    "    - **Purpose:** Import necessary libraries and set up the environment.\n",
    "    - **Content:**\n",
    "        - Libraries: `pandas`, `pyarrow.parquet`, `tqdm`, `matplotlib.pyplot`, `scispacy`, `spacy`, `transformers`, `nltk`, etc.\n",
    "        - Utility Functions: Functions for reading and saving parquet files in batches.\n",
    "        - Tokenization Functions: Functions for tokenizing text using simple and Hugging Face tokenizers.\n",
    "        - Disease Detection Functions: Functions for detecting diseases using dictionary-based and SciSpacy approaches.\n",
    "\n",
    "3. **Data Loading and Initial Processing:**\n",
    "    - **Purpose:** Load and clean the main dataset.\n",
    "    - **Content:**\n",
    "        - Load the dataset from a parquet file.\n",
    "        - Perform initial data cleaning, such as removing rows with missing abstracts and filtering rows based on date.\n",
    "        - Select relevant columns for further processing.\n",
    "\n",
    "4. **Tokenization:**\n",
    "    - **Purpose:** Tokenize the text data for further analysis.\n",
    "    - **Content:**\n",
    "        - Tokenize the `title` and `abstract` columns using both simple and Hugging Face tokenizers.\n",
    "        - Remove stopwords and punctuation from the tokenized data.\n",
    "        - Analyze token frequencies to generate insights.\n",
    "\n",
    "5. **Disease Detection:**\n",
    "    - **Purpose:** Detect disease entities in the text data.\n",
    "    - **Content:**\n",
    "        - Dictionary-Based Detection: Keep only tokens present in a predefined disease dictionary.\n",
    "        - SciSpacy-Based Detection: Use the `en_ner_bc5cdr_md` model to extract disease entities from the `title` and `abstract` columns.\n",
    "        - Analyze the results, including counting the number of detected entities and categorizing rows based on the presence of entities.\n",
    "\n",
    "6. **Merging and Saving Results:**\n",
    "    - **Purpose:** Combine the results of disease detection with the main dataset and save the final dataset.\n",
    "    - **Content:**\n",
    "        - Merge the results of disease detection with the main dataset.\n",
    "        - Save the final merged dataset to a parquet file.\n",
    "\n",
    "7. **Additional Analysis:**\n",
    "    - **Purpose:** Perform further analysis on the merged dataset.\n",
    "    - **Content:**\n",
    "        - Count the number of rows with entities in both `title` and `abstract` columns.\n",
    "        - Explore potential next steps, such as using UMLS or MeSH for further disease categorization.\n",
    "\n",
    "8. **Environment Management:**\n",
    "    - **Purpose:** Manage the environment to optimize performance.\n",
    "    - **Content:**\n",
    "        - Reset the environment and release memory to ease computer work.\n",
    "\n",
    "### Key Variables and Their Values:\n",
    "\n",
    "- `base_path`: `'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch'`\n",
    "- `batch_size`: `100000`\n",
    "- `col`: `'disease_abstract_spacy'`\n",
    "- `df`: DataFrame with 1,057,871 rows and 22 columns.\n",
    "- `file_path`: `'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch\\\\Data\\\\2.Processed\\\\ModellingData\\\\P5_final_new.parquet'`\n",
    "- `final_file`: `'P6_merged_tokens.parquet'`\n",
    "- `output_folder`: `'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch\\\\Data\\\\2.Processed\\\\ModellingData'`\n",
    "- `result_path`: `'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch\\\\Data\\\\2.Processed\\\\ModellingData\\\\P6_merged_tokens.parquet'`\n",
    "- `token_columns`: `['disease_title_tokens_simple', 'disease_title_tokens_hf', 'disease_abstract_tokens_simple', 'disease_abstract_tokens_hf', 'disease_title_spacy', 'disease_mesh_terms_spacy', 'disease_abstract_spacy']`\n",
    "\n",
    "This enhanced summary provides a comprehensive overview of the notebook's structure, key steps, purposes, and important variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## Table Of Contents (ToC)\n",
    "\n",
    "1. [Utility Functions](#99-utility-functions)\n",
    "    - 1) [read_parquet_in_batches - function](#1-read_parquet_in_batches---function)\n",
    "    - 2) [save_batches_to_parquet - function](#2-save_batches_to_parquet---function)\n",
    "\n",
    "2. [Early Processing](#early-processing)\n",
    "    - Step 0: [Setup](#step-0-setup)\n",
    "    - Step 1: [Filtering rows + removal of missing records](#step-1-filtering-rows--removal-of-missing-records)\n",
    "\n",
    "3. [Further Processing](#further-processing)\n",
    "    - [Variable: `uid` + `parsed_date`](#variable-uid--parsed_date)\n",
    "    - [Title + Abstract](#title--abstract)\n",
    "        - [0) Insight into simple and Hugging Face tokenization](#0-insight-into-simple-and-hugging-face-tokenization)\n",
    "                - [Conclusions from 0)](#conclusions-from-0)\n",
    "\n",
    "4. [Variables](#variables)\n",
    "    - [1) Dictionary-Based Disease Detection](#1-dictionary-based-disease-detection)\n",
    "    - [2) SciSpacy (or spaCy) Biomedical NER Approach](#2-scispacy-or-spacy-biomedical-ner-approach)\n",
    "    - [WHAT'S NEXT?](#whats-next)\n",
    "    - [Further Analysis Of Abstract And Title](#further-analysis-of-abstract-and-title)\n",
    "    - [3) EMBEDDINGS - maybe to get into later, very heavy for computer](#3-embeddings)\n",
    "\n",
    "5. [Additional Variables](#additional-variables)\n",
    "    - [mesh_terms](#mesh_terms)\n",
    "    - [journal](#journal)\n",
    "    - [abstract](#abstract)\n",
    "    - [authors](#authors)\n",
    "    - [affiliations](#affiliations)\n",
    "    - [keywords](#keywords)\n",
    "    - [coi_statement](#coi_statement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## Table Of Contents (ToC)\n",
    "\n",
    "1. [Utility Functions](#99-utility-functions)\n",
    "    - 1) [read_parquet_in_batches - function](#1-read_parquet_in_batches---function)\n",
    "    - 2) [save_batches_to_parquet - function](#2-save_batches_to_parquet---function)\n",
    "\n",
    "2. [Early Processing](#early-processing)\n",
    "    - Step 0: [Setup](#step-0-setup)\n",
    "    - Step 1: [Filtering rows + removal of missing records](#step-1-filtering-rows--removal-of-missing-records)\n",
    "\n",
    "3. [Further Processing](#further-processing)\n",
    "    - [Variable: `uid` + `parsed_date`](#variable-uid--parsed_date)\n",
    "    - [Title + Abstract](#title--abstract)\n",
    "        - [0) Insight into simple and Hugging Face tokenization](#0-insight-into-simple-and-hugging-face-tokenization)\n",
    "                - [Conclusions from 0)](#conclusions-from-0)\n",
    "\n",
    "4. [Variables](#variables)\n",
    "    - [1) Dictionary-Based Disease Detection](#1-dictionary-based-disease-detection)\n",
    "    - [2) SciSpacy (or spaCy) Biomedical NER Approach](#2-scispacy-or-spacy-biomedical-ner-approach)\n",
    "    - [WHAT'S NEXT?](#whats-next)\n",
    "    - [Further Analysis Of Abstract And Title](#further-analysis-of-abstract-and-title)\n",
    "    - [3) EMBEDDINGS - maybe to get into later, very heavy for computer](#3-embeddings)\n",
    "\n",
    "5. [Additional Variables](#additional-variables)\n",
    "    - `mesh_terms`\n",
    "    - `journal`\n",
    "    - `abstract`\n",
    "    - `authors`\n",
    "    - `affiliations`\n",
    "    - `mesh_terms`\n",
    "    - `keywords`\n",
    "    - `coi_statement`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents (ToC) - to do later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Utility Functions](#99-utility-functions)\n",
    "   - 1) read_parquet_in_batches - function\n",
    "   - 2) save_batches_to_parquet - function\n",
    "\n",
    "2. [Early Processing](#early-processing)\n",
    "   - Step 0: Setup\n",
    "   - Step 1: Filtering rows + removal of missing records (missing abstracts are, after checking manually)\n",
    "\n",
    "3. [Further Processing](#further-processing)\n",
    "   - [Variable: `uid + parsed_date`](variable+)\n",
    "   - [Title + Abstract](#title-+-abstract)\n",
    "      - [0) Insight into simple and Hugging Face tokenization](#insight-into-simple-and-hugging-face-tokenization)\n",
    "            - Conclusions from 0)\n",
    "\n",
    "4. [Variables](#variables)\n",
    "\n",
    "     \n",
    "     - [1) Dictionary-Based Disease Detection](#1-dictionary-based-disease-detection)\n",
    "     - [2) SciSpacy (or spaCy) Biomedical NER Approach](#scispacy-or-spacy-biomedical-ner-approach)\n",
    "     - [WHAT'S NEXT?](#whats-next)\n",
    "     - [Further Analysis Of Abstract And Title](#further-analysis-of-abstract-and-title)\n",
    "     - [3) EMBEDDINGS - maybe to get into later, very heavy for computer](#embeddings)\n",
    "\n",
    "5. [Additional Variables](#additional-variables)\n",
    "   - `mesh_terms`\n",
    "   - `journal`\n",
    "   - `abstract`\n",
    "   - `authors`\n",
    "   - `affiliations`\n",
    "   - `mesh_terms`\n",
    "   - `keywords`\n",
    "   - `coi_statement`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `99)` Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) read_parquet_in_batches - function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def read_parquet_in_batches_with_progress(file_path, batch_size):\n",
    "    \"\"\"\n",
    "    Read a Parquet file in fixed-size row batches with a progress bar and per-chunk logging.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the Parquet file.\n",
    "        batch_size (int): Number of rows per batch.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame after processing all batches.\n",
    "    \"\"\"\n",
    "    # Open the Parquet file\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    \n",
    "    # Total number of rows in the file\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    \n",
    "    # Initialize a list to store DataFrame chunks\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    with tqdm(total=total_rows, desc=\"Processing Batches\", unit=\"rows\") as pbar:\n",
    "        # Enumerate batches for logging\n",
    "        for batch_number, batch in enumerate(parquet_file.iter_batches(batch_size=batch_size), start=1):\n",
    "            # Convert the batch to a Pandas DataFrame\n",
    "            df_batch = batch.to_pandas()\n",
    "            \n",
    "            # Simulate processing (add custom logic here if necessary)\n",
    "            all_chunks.append(df_batch)\n",
    "            \n",
    "            # Update the progress bar\n",
    "            pbar.update(len(df_batch))\n",
    "            \n",
    "            # Print per-chunk information\n",
    "            print(f\"Processed Chunk {batch_number}: {len(df_batch)} rows\")\n",
    "    \n",
    "    # Combine all chunks into a single DataFrame\n",
    "    combined_df = pd.concat(all_chunks, ignore_index=True)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "# EXAMPLE USAGE\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"Data/2.Processed/ModellingData/P1_all.parquet\"\n",
    "    batch_size = 100_000  # Define your desired chunk size\n",
    "    \n",
    "    df = read_parquet_in_batches_with_progress(file_path, batch_size)\n",
    "    \n",
    "    print(f\"\\nFinal DataFrame with {len(df)} rows:\")\n",
    "    df.head()\n",
    "    ````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) save_batches_to_parquet - function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_and_merge_in_batches(\n",
    "    df: pd.DataFrame,\n",
    "    batch_size: int,\n",
    "    output_folder: str,\n",
    "    final_filename: str = \"final_merged.parquet\",\n",
    "    temp_batch_prefix: str = \"temp_batch_\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits 'df' into multiple batches (size = batch_size), writes each batch to a Parquet file,\n",
    "    then merges them into one final Parquet, with a progress bar showing how many batches are done.\n",
    "\n",
    "    Steps:\n",
    "    ------\n",
    "    1) Creates subfolder 'temp_batches' in output_folder for batch files.\n",
    "    2) For each chunk of rows:\n",
    "       - Writes it to 'temp_batch_X.parquet'\n",
    "       - Increments a progress bar\n",
    "    3) Reads & merges all batch files into 'final_filename', then removes them.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str -> path to the final merged Parquet file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Subfolder for temporary batch files\n",
    "    temp_folder = os.path.join(output_folder, \"temp_batches\")\n",
    "    os.makedirs(temp_folder, exist_ok=True)\n",
    "\n",
    "    total_rows = len(df)\n",
    "    batch_count = (total_rows + batch_size - 1) // batch_size\n",
    "    print(f\"Splitting DataFrame of {total_rows} rows into {batch_count} batches (size={batch_size}).\")\n",
    "\n",
    "    temp_files = []\n",
    "    current_row = 0\n",
    "    batch_index = 1\n",
    "\n",
    "    # -- 1) SAVE IN MULTIPLE BATCHES WITH A PROGRESS BAR FOR THE BATCHES --\n",
    "    with tqdm(total=batch_count, desc=\"Saving Batches\", unit=\"batch\") as pbar:\n",
    "        while current_row < total_rows:\n",
    "            end_row = min(current_row + batch_size, total_rows)\n",
    "            df_batch = df.iloc[current_row:end_row]\n",
    "\n",
    "            temp_file_name = f\"{temp_batch_prefix}{batch_index}.parquet\"\n",
    "            temp_file_path = os.path.join(temp_folder, temp_file_name)\n",
    "\n",
    "            # Write the chunk (one shot for each batch)\n",
    "            df_batch.to_parquet(temp_file_path, index=False, compression=\"snappy\")\n",
    "\n",
    "            temp_files.append(temp_file_path)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Optional: Print log\n",
    "            print(f\"  -> Batch {batch_index} rows [{current_row}:{end_row}] saved to {temp_file_path}\")\n",
    "\n",
    "            current_row = end_row\n",
    "            batch_index += 1\n",
    "\n",
    "    # -- 2) MERGE ALL BATCH FILES INTO A SINGLE PARQUET --\n",
    "    final_file_path = os.path.join(output_folder, final_filename)\n",
    "    print(f\"\\nMerging {len(temp_files)} batch files into {final_file_path}...\")\n",
    "\n",
    "    merged_parts = []\n",
    "    # Another progress bar for reading merges (optional)\n",
    "    with tqdm(total=len(temp_files), desc=\"Merging Batches\", unit=\"file\") as pbar_merge:\n",
    "        for file_path in temp_files:\n",
    "            merged_parts.append(pd.read_parquet(file_path))\n",
    "            pbar_merge.update(1)\n",
    "\n",
    "    df_merged = pd.concat(merged_parts, ignore_index=True)\n",
    "    df_merged.to_parquet(final_file_path, index=False, compression=\"snappy\")\n",
    "    print(f\"Final merged DataFrame saved as: {final_file_path}\\n\")\n",
    "\n",
    "    # -- 3) CLEAN UP TEMPORARY FILES --\n",
    "    for path in temp_files:\n",
    "        os.remove(path)\n",
    "    os.rmdir(temp_folder)\n",
    "\n",
    "    print(\"Temporary batch files removed. All done!\")\n",
    "    return final_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "# ---------------------------\n",
    "# EXAMPLE USAGE\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    folder_path = \"Data/2.Processed/ModellingData\"\n",
    "    final_file = \"P4_final_merged.parquet\"\n",
    "    batch_size = 100_000  # e.g. if you want ~10 batches\n",
    "\n",
    "    result_path = save_and_merge_in_batches(\n",
    "        df=df_final,\n",
    "        batch_size=batch_size,\n",
    "        output_folder=folder_path,\n",
    "        final_filename=final_file,\n",
    "        temp_batch_prefix=\"temp_batch_\"\n",
    "    )\n",
    "\n",
    "    print(f\"All done. Merged file at: {result_path}\")\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*removing NA + filtering rows with wrong dates* (after 2024 and before 1995 articles will be excluded, as currently its 2024, and 1994 had low amount of articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"Data/1.EarlyCleaned/cleaned_parquet/final/PubMedAbstracts_final.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na per column \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1**: Filtering rows + removal of missing records "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(missing abstracts are, after checking manually, missing from articles itself, they are **NOT** due to mistakes in the processing or during phase of data gathering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_abstracts = df[df[\"abstract\"].isna()]\n",
    "print(\"Rows where 'abstract' is missing:\")\n",
    "missing_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (df.shape)\n",
    "\n",
    "# 1) Drop rows with missing abstract (14) -> We have looked into them and it's really like they are missing abstract not due to fault that comes from coding but they are just empty on website. they contribute to ~ 1% of dataset so it is not much of an issue\n",
    "df = df.dropna(subset=[\"abstract\"])\n",
    "\n",
    "print(\"Removed missing abstract rows:\")\n",
    "print(x[0]-df.shape[0])\n",
    "\n",
    "# 2) Drop rows with year == 2025\n",
    "# first ensure parsed_date is datetime\n",
    "df[\"parsed_date\"] = pd.to_datetime(df[\"parsed_date\"], errors=\"coerce\")\n",
    "\n",
    "#df = df[df[\"parsed_date\"].dt.year != 2025]\n",
    "# Exclude years 1994 and 2025 ; 1994 has low amount of articles\n",
    "df = df[(df[\"parsed_date\"].dt.year != 1994) & (df[\"parsed_date\"].dt.year != 2025)]\n",
    "\n",
    "print(\"Removed total rows:\")\n",
    "print(x[0] - df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only columns that we will be working with\n",
    "df = df[[\"uid\", \"title\", \"journal\", \"abstract\", \"authors\", \"affiliations\", \"mesh_terms\", \"keywords\", \"coi_statement\", \"parsed_date\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: `uid` + `parsed_date`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# parsed date into datetime format\n",
    "df[\"parsed_date\"] = pd.to_datetime(df[\"parsed_date\"], errors=\"coerce\")\n",
    "\n",
    "# Compute articles per year\n",
    "articles_per_year = df.groupby(df[\"parsed_date\"].dt.year)[\"uid\"].count().sort_index()\n",
    "\n",
    "# Compute Year-over-Year % change\n",
    "growth_pct = articles_per_year.pct_change().fillna(0) * 100\n",
    "growth_pct.iloc[0] = 0  # Set the first year's growth to 0 to avoid outliers\n",
    "\n",
    "# Ensure all years from 1995 to 2024 are represented (even if no data for some years)\n",
    "all_years = pd.Series(range(1995, 2025), name=\"Year\")\n",
    "articles_per_year = articles_per_year.reindex(all_years, fill_value=0)\n",
    "growth_pct = growth_pct.reindex(all_years, fill_value=0)\n",
    "\n",
    "# Compute cumulative number of articles\n",
    "cumulative_articles = articles_per_year.cumsum()\n",
    "\n",
    "# Compute Indexed Growth (base 1995 = 100)\n",
    "indexed_growth = (articles_per_year / articles_per_year.iloc[0]) * 100\n",
    "\n",
    "# --- Visualization 1: Articles per Year + YoY Growth ---\n",
    "fig, ax1 = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "# Adjust the bar width and opacity\n",
    "bar_width = 0.8\n",
    "bars = ax1.bar(articles_per_year.index, articles_per_year.values, color=\"skyblue\", alpha=0.8, width=bar_width, label=\"Article Count\")\n",
    "ax1.set_xlabel(\"Year\", fontsize=12)\n",
    "ax1.set_ylabel(\"Count of Articles\", color=\"blue\", fontsize=12)\n",
    "ax1.tick_params(axis='y', labelcolor=\"blue\")\n",
    "ax1.set_xticks(articles_per_year.index)\n",
    "ax1.set_xticklabels(articles_per_year.index, rotation=90, fontsize=10)\n",
    "\n",
    "# Annotate each bar with its count (formatted with commas or spaces), slightly raised\n",
    "for rect in bars:\n",
    "    height = rect.get_height()\n",
    "    if height > 0:  # Annotate only if count > 0\n",
    "        ax1.text(\n",
    "            rect.get_x() + rect.get_width() / 2, height - 1500,  # Slightly raised above bar tops\n",
    "            f\"{int(height):,}\".replace(\",\", \" \"), ha=\"center\", va=\"bottom\", fontsize=10\n",
    "        )\n",
    "\n",
    "# Secondary axis for growth percentage\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Year-over-Year Growth (%)\", fontsize=12)\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "# Custom Y-axis ticks with range -30 to 30\n",
    "ax2.set_ylim(-30, 30)\n",
    "ax2.set_yticks(range(-30, 35, 5))\n",
    "ax2.set_yticklabels(\n",
    "    [f\"{abs(t)}%\" if t == 0 else f\"{t}%\" for t in range(-30, 35, 5)],\n",
    "    color=\"gray\",\n",
    "    fontsize=10,\n",
    ")\n",
    "\n",
    "# Change color of Y-axis labels dynamically\n",
    "for label in ax2.get_yticklabels():\n",
    "    value = int(label.get_text().replace(\"%\", \"\"))\n",
    "    if value > 0:\n",
    "        label.set_color(\"green\")\n",
    "    elif value < 0:\n",
    "        label.set_color(\"red\")\n",
    "    else:\n",
    "        label.set_color(\"gray\")\n",
    "\n",
    "# Align the line chart with the center of bars\n",
    "line_x = articles_per_year.index + (bar_width / 2 - 0.4)\n",
    "colors = [\"green\" if y > 0 else \"red\" for y in growth_pct.values]\n",
    "\n",
    "# Plot colored growth lines based on percentage\n",
    "for i in range(1, len(articles_per_year)):\n",
    "    ax2.plot(\n",
    "        [line_x[i - 1], line_x[i]],\n",
    "        [growth_pct.values[i - 1], growth_pct.values[i]],\n",
    "        color=colors[i],\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "# Annotate growth percentage below/above dots in bold\n",
    "for x, y in zip(line_x, growth_pct.values):\n",
    "    if abs(y) > 0.5:  # Annotate only significant changes\n",
    "        offset = -2 if y < 0 else 2\n",
    "        ax2.text(\n",
    "            x, y + offset,\n",
    "            f\"{y:.1f}%\", color=\"green\" if y > 0 else \"red\",\n",
    "            ha=\"center\", va=\"bottom\" if y > 0 else \"top\", fontsize=10, fontweight=\"bold\"\n",
    "        )\n",
    "\n",
    "# Add a title and adjust layout\n",
    "ax1.set_title(\"Articles per Year and Year-over-Year Growth\", fontsize=14, pad=20)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Visualization 2: Indexed Growth + Cumulative Articles ---\n",
    "fig, ax1 = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "# Plot cumulative number of articles as bars\n",
    "bars = ax1.bar(cumulative_articles.index, cumulative_articles.values, color=\"lightblue\", alpha=0.9, width=bar_width, label=\"Cumulative Articles\")\n",
    "ax1.set_xlabel(\"Year\", fontsize=12)\n",
    "ax1.set_ylabel(\"Cumulative Articles\", color=\"blue\", fontsize=12)\n",
    "ax1.tick_params(axis='y', labelcolor=\"blue\")\n",
    "ax1.set_xticks(cumulative_articles.index)\n",
    "ax1.set_xticklabels(cumulative_articles.index, rotation=90, fontsize=10)\n",
    "\n",
    "# Set Y-axis to show absolute values (e.g., 100,000)\n",
    "ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{int(x):,}\"))\n",
    "\n",
    "# Annotate each bar with its cumulative count\n",
    "for rect in bars:\n",
    "    height = rect.get_height()\n",
    "    if height > 0:  # Annotate only if count > 0\n",
    "        ax1.text(\n",
    "            rect.get_x() + rect.get_width() / 2, height - 20000,  # Slightly above the bar tops\n",
    "            f\"{int(height):,}\".replace(\",\", \" \"), ha=\"center\", va=\"bottom\", fontsize=10\n",
    "        )\n",
    "\n",
    "# Secondary axis for Indexed Growth\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Indexed Growth (Base 1995 = 100)\", color=\"purple\", fontsize=12)\n",
    "ax2.plot(cumulative_articles.index, indexed_growth.values, color=\"purple\", marker=\"o\", linewidth=2, label=\"Indexed Growth\")\n",
    "\n",
    "# Annotate points on the purple line (indexed growth)\n",
    "for x, y in zip(cumulative_articles.index, indexed_growth.values):\n",
    "    ax2.text(\n",
    "        x, y + 5,  # Slightly above each point\n",
    "        f\"{y:.1f}\", color=\"purple\", fontsize=10, ha=\"center\"\n",
    "    )\n",
    "\n",
    "ax2.tick_params(axis='y', labelcolor=\"purple\")\n",
    "ax2.set_ylim(0, max(indexed_growth) * 1.1)\n",
    "\n",
    "# Add a title and adjust layout\n",
    "ax1.set_title(\"Cumulative Articles and Indexed Growth\", fontsize=14, pad=20)\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: `title` + `abstract`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # Import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hugging Face tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def huggingface_tokenize(text, max_len=512):\n",
    "    \"\"\"\n",
    "    Tokenize text with the Hugging Face tokenizer and truncate to max_len.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=False\n",
    "    )\n",
    "    return tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Simple whitespace and punctuation-based tokenizer.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    return text.split()\n",
    "\n",
    "# Batch processing with progress bar\n",
    "def process_in_batches(df, column, batch_size=1000, tokenizer_func=None, output_column=None, save_path=None):\n",
    "    tqdm.pandas()  # Enables progress_apply with tqdm\n",
    "\n",
    "    # Load existing processed results if the file exists\n",
    "    if save_path and Path(save_path).exists():\n",
    "        processed = pd.read_parquet(save_path)\n",
    "        print(f\"Loaded existing results from {save_path}.\")\n",
    "        return processed\n",
    "\n",
    "    num_batches = (len(df) + batch_size - 1) // batch_size  # Total number of batches\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "\n",
    "        # Process batch and avoid direct assignment to a slice\n",
    "        batch = df.iloc[start:end]\n",
    "        tokenized_data = batch[column].progress_apply(tokenizer_func)\n",
    "        tokenized_df = pd.DataFrame({output_column: tokenized_data}, index=batch.index)\n",
    "        results.append(tokenized_df)\n",
    "\n",
    "        # Save progress after each batch\n",
    "        if save_path:\n",
    "            pd.concat(results).to_parquet(save_path, index=True)\n",
    "\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So firstly we will get simple tokanization and tokanization using huggingface tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process title with both tokenizers\n",
    "df[\"title_tokens_simple\"] = process_in_batches(\n",
    "    df, \"title\", batch_size=100_000, tokenizer_func=simple_tokenize, output_column=\"title_tokens_simple\", save_path=\"Data/2.Processed/ModellingData/P0.simple_tokens_title.parquet\"\n",
    ")\n",
    "\n",
    "df[\"title_tokens_hf\"] = process_in_batches(\n",
    "    df, \"title\", batch_size=100_000, tokenizer_func=lambda txt: huggingface_tokenize(txt, max_len=512), output_column=\"title_tokens_hf\", save_path=\"Data/2.Processed/ModellingData/P0.hf_tokens_title.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process title with both tokenizers\n",
    "df[\"abstract_tokens_simple\"] = process_in_batches(\n",
    "    df, \"abstract\", batch_size=100_000, tokenizer_func=simple_tokenize, output_column=\"abstract_tokens_simple\", save_path=\"Data/2.Processed/ModellingData/P0.simple_tokens_abstract.parquet\"\n",
    ")\n",
    "\n",
    "df[\"abstract_tokens_hf\"] = process_in_batches(\n",
    "    df, \"abstract\", batch_size=100_000, tokenizer_func=lambda txt: huggingface_tokenize(txt, max_len=512), output_column=\"abstract_tokens_hf\", save_path=\"Data/2.Processed/ModellingData/P0.hf_tokens_abstract.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0) Insight into `simple` and `hugging face` tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten token lists and count token frequencies\n",
    "simple_token_flat = [token for tokens in df[\"title_tokens_simple\"] for token in tokens]\n",
    "hf_token_flat = [token for tokens in df[\"title_tokens_hf\"] for token in tokens]\n",
    "\n",
    "# Count frequencies\n",
    "simple_token_freq = Counter(simple_token_flat).most_common(20)\n",
    "hf_token_freq = Counter(hf_token_flat).most_common(20)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "freq_df = pd.DataFrame({\n",
    "    \"Simple Tokens\": [token for token, _ in simple_token_freq],\n",
    "    \"Simple Frequency\": [freq for _, freq in simple_token_freq],\n",
    "    \"HF Tokens\": [token for token, _ in hf_token_freq],\n",
    "    \"HF Frequency\": [freq for _, freq in hf_token_freq]\n",
    "})\n",
    "\n",
    "print(freq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten token lists and count token frequencies\n",
    "simple_token_flat = [token for tokens in df[\"abstract_tokens_simple\"] for token in tokens]\n",
    "hf_token_flat = [token for tokens in df[\"abstract_tokens_hf\"] for token in tokens]\n",
    "\n",
    "# Count frequencies\n",
    "simple_token_freq = Counter(simple_token_flat).most_common(20)\n",
    "hf_token_freq = Counter(hf_token_flat).most_common(20)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "freq_df = pd.DataFrame({\n",
    "    \"Simple Tokens\": [token for token, _ in simple_token_freq],\n",
    "    \"Simple Frequency\": [freq for _, freq in simple_token_freq],\n",
    "    \"HF Tokens\": [token for token, _ in hf_token_freq],\n",
    "    \"HF Frequency\": [freq for _, freq in hf_token_freq]\n",
    "})\n",
    "\n",
    "print(freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stopwords removal`\n",
    "\n",
    "an, a, the, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "# Get the list of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Get the list of stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Remove stopwords from the tokens titles\n",
    "df[\"cleaned_title_tokens_simple\"] = df[\"title_tokens_simple\"].apply(lambda tokens: [t for t in tokens if t not in stop_words])\n",
    "df[\"cleaned_title_tokens_hf\"] = df[\"title_tokens_hf\"].apply(lambda tokens: [t for t in tokens if t not in stop_words])\n",
    "\n",
    "# Remove stopwords from the tokens abstracts\n",
    "df[\"cleaned_abstract_tokens_simple\"] = df[\"abstract_tokens_simple\"].apply(lambda tokens: [t for t in tokens if t not in stop_words])\n",
    "df[\"cleaned_abstract_tokens_hf\"] = df[\"abstract_tokens_hf\"].apply(lambda tokens: [t for t in tokens if t not in stop_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`punctuations removal` \n",
    "\n",
    " \".\" , \",\" , \"(\" , etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_tokens = {\".\", \",\", \"-\", \":\", \";\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"`\", \"'\"}\n",
    "def remove_punctuation(tokens):\n",
    "    return [t for t in tokens if t not in punctuation_tokens]\n",
    "\n",
    "# Then:\n",
    "df[\"cleaned_title_tokens_simple\"] = df[\"cleaned_title_tokens_simple\"].apply(remove_punctuation).copy()\n",
    "df[\"cleaned_title_tokens_hf\"] = df[\"cleaned_title_tokens_hf\"].apply(remove_punctuation).copy()\n",
    "\n",
    "# Then:\n",
    "df[\"cleaned_abstract_tokens_simple\"] = df[\"cleaned_abstract_tokens_simple\"].apply(remove_punctuation).copy()\n",
    "df[\"cleaned_abstract_tokens_hf\"] = df[\"cleaned_abstract_tokens_hf\"].apply(remove_punctuation).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten token lists and count token frequencies\n",
    "simple_token_flat = [token for tokens in df[\"cleaned_title_tokens_simple\"] for token in tokens]\n",
    "hf_token_flat = [token for tokens in df[\"cleaned_title_tokens_hf\"] for token in tokens]\n",
    "\n",
    "# Count frequencies\n",
    "simple_token_freq = Counter(simple_token_flat).most_common(50)\n",
    "hf_token_freq = Counter(hf_token_flat).most_common(50)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "freq_df = pd.DataFrame({\n",
    "    \"Simple Tokens\": [token for token, _ in simple_token_freq],\n",
    "    \"Simple Frequency\": [freq for _, freq in simple_token_freq],\n",
    "    \"HF Tokens\": [token for token, _ in hf_token_freq],\n",
    "    \"HF Frequency\": [freq for _, freq in hf_token_freq]\n",
    "})\n",
    "\n",
    "print(freq_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten token lists and count token frequencies\n",
    "simple_token_flat = [token for tokens in df[\"cleaned_abstract_tokens_simple\"] for token in tokens]\n",
    "hf_token_flat = [token for tokens in df[\"cleaned_abstract_tokens_hf\"] for token in tokens]\n",
    "\n",
    "# Count frequencies\n",
    "simple_token_freq = Counter(simple_token_flat).most_common(50)\n",
    "hf_token_freq = Counter(hf_token_flat).most_common(50)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "freq_df = pd.DataFrame({\n",
    "    \"Simple Tokens\": [token for token, _ in simple_token_freq],\n",
    "    \"Simple Frequency\": [freq for _, freq in simple_token_freq],\n",
    "    \"HF Tokens\": [token for token, _ in hf_token_freq],\n",
    "    \"HF Frequency\": [freq for _, freq in hf_token_freq]\n",
    "})\n",
    "\n",
    "print(freq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusions from 0)\n",
    "\n",
    "Based on our initial exploration, both simple tokenization and Hugging Face-based tokenization appear to be viable approaches for our research. \n",
    "\n",
    "However, cleaning required for these methods may prove to be overly time-intensive. \n",
    "\n",
    "As a result, we will prioritize alternative approaches for the time being. If these alternative methods fail to give satisfactory results, we will revisit and upgrade the tokenization-based strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Dictionary-Based Disease Detection\n",
    "\n",
    "In this approach, We manually create (or load) a disease dictionary (e.g., from ICD codes, known disease names). We then filter tokens to only keep tokens matching that dictionary. This is a straightforward method but can miss synonyms or multi-word diseases unless stored them as multiple entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 1) Dictionary-Based Approach\n",
    "###############################################################################\n",
    "\n",
    "# Example dictionary of diseases (this is just a tiny sample!) - could use some sort of base group of files or something like dict of ilnesses? icd-10-medical-diagnosis-codes maybe?\n",
    "disease_dict = {\n",
    "    # General and Common Diseases\n",
    "    \"cancer\", \"tumor\", \"diabetes\", \"hiv\", \"aids\", \"arthritis\", \"pneumonia\",\n",
    "    \"hypertension\", \"influenza\", \"malaria\", \"tuberculosis\", \"dementia\", \"asthma\", \n",
    "    \"depression\", \"anxiety\", \"stroke\", \"heart disease\", \"kidney disease\",\n",
    "    \n",
    "    # Infectious Diseases\n",
    "    \"hepatitis\", \"cholera\", \"dengue\", \"zika\", \"ebola\", \"typhoid\", \"plague\",\n",
    "    \"meningitis\", \"measles\", \"rubella\", \"chickenpox\", \"shingles\", \"covid\",\n",
    "    \"scarlet fever\", \"leprosy\", \"syphilis\", \"gonorrhea\", \"lyme disease\",\n",
    "    \n",
    "    # Neurological Disorders\n",
    "    \"parkinson's disease\", \"epilepsy\", \"multiple sclerosis\", \"migraine\", \n",
    "    \"alzheimer's disease\", \"amyotrophic lateral sclerosis\", \"huntington's disease\",\n",
    "    \"cerebral palsy\", \"autism\", \"adhd\", \"schizophrenia\", \"bipolar disorder\",\n",
    "    \n",
    "    # Respiratory Diseases\n",
    "    \"bronchitis\", \"emphysema\", \"chronic obstructive pulmonary disease (copd)\", \n",
    "    \"sleep apnea\", \"pulmonary fibrosis\", \"cystic fibrosis\",\n",
    "    \n",
    "    # Cardiovascular Diseases\n",
    "    \"high blood pressure\", \"arrhythmia\", \"coronary artery disease\", \n",
    "    \"heart failure\", \"heart attack\", \"aortic aneurysm\", \"angina\",\n",
    "    \n",
    "    # Digestive Disorders\n",
    "    \"irritable bowel syndrome (ibs)\", \"ulcerative colitis\", \"crohn's disease\",\n",
    "    \"gastritis\", \"peptic ulcer\", \"gastroesophageal reflux disease (gerd)\",\n",
    "    \"pancreatitis\", \"hepatitis a\", \"hepatitis b\", \"hepatitis c\",\n",
    "    \n",
    "    # Musculoskeletal Disorders\n",
    "    \"osteoporosis\", \"rheumatoid arthritis\", \"osteoarthritis\", \"gout\",\n",
    "    \"fibromyalgia\", \"scoliosis\", \"spinal cord injury\",\n",
    "    \n",
    "    # Endocrine Disorders\n",
    "    \"hyperthyroidism\", \"hypothyroidism\", \"cushing's syndrome\", \"addison's disease\",\n",
    "    \"polycystic ovary syndrome (pcos)\", \"metabolic syndrome\",\n",
    "    \n",
    "    # Skin Diseases\n",
    "    \"eczema\", \"psoriasis\", \"rosacea\", \"acne\", \"melanoma\", \"basal cell carcinoma\",\n",
    "    \"squamous cell carcinoma\",\n",
    "    \n",
    "    # Genetic Disorders\n",
    "    \"down syndrome\", \"turner syndrome\", \"klinefelter syndrome\", \n",
    "    \"sickle cell anemia\", \"cystic fibrosis\", \"marfan syndrome\",\n",
    "    \n",
    "    # Blood Disorders\n",
    "    \"anemia\", \"leukemia\", \"lymphoma\", \"hemophilia\", \"thalassemia\",\n",
    "    \"deep vein thrombosis\", \"pulmonary embolism\",\n",
    "    \n",
    "    # Eye Diseases\n",
    "    \"cataracts\", \"glaucoma\", \"macular degeneration\", \"diabetic retinopathy\",\n",
    "    \"conjunctivitis\", \"dry eye syndrome\",\n",
    "    \n",
    "    # Liver Diseases\n",
    "    \"liver cirrhosis\", \"fatty liver disease\", \"hepatitis\", \"liver cancer\",\n",
    "    \n",
    "    # Kidney and Urinary Diseases\n",
    "    \"kidney stones\", \"urinary tract infection (uti)\", \"chronic kidney disease\",\n",
    "    \"nephritis\", \"prostate cancer\", \"bladder cancer\",\n",
    "    \n",
    "    # Cancers\n",
    "    \"breast cancer\", \"lung cancer\", \"colon cancer\", \"skin cancer\",\n",
    "    \"pancreatic cancer\", \"prostate cancer\", \"ovarian cancer\", \n",
    "    \"brain cancer\", \"thyroid cancer\",\n",
    "    \n",
    "    # Reproductive Disorders\n",
    "    \"endometriosis\", \"ovarian cysts\", \"uterine fibroids\", \"erectile dysfunction\",\n",
    "    \"infertility\", \"pelvic inflammatory disease (pid)\",\n",
    "    \n",
    "    # Autoimmune Diseases\n",
    "    \"systemic lupus erythematosus\", \"hashimoto's disease\", \"sjogren's syndrome\",\n",
    "    \"celiac disease\", \"graves' disease\", \"type 1 diabetes\",\n",
    "    \n",
    "    # Others\n",
    "    \"sepsis\", \"allergies\", \"heat stroke\", \"hypothermia\", \"obesity\",\n",
    "    \"metabolic syndrome\", \"malnutrition\", \"alcoholism\", \"drug addiction\",\n",
    "    \"dyslexia\", \"anorexia\", \"bulimia\", \"hyperlipidemia\", \"bacterial vaginosis\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def keep_only_diseases(token_list):\n",
    "    \"\"\"\n",
    "    Return only those tokens present in the disease_dict.\n",
    "    We do a .lower() to unify. \n",
    "    If you have subwords in HF approach (like 'canc', '##er'),\n",
    "    you might want to check partial matches or reconstruct them.\n",
    "    \"\"\"\n",
    "    return [t for t in token_list if t.lower() in disease_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with cleaned columns:\n",
    "df[\"disease_title_tokens_simple\"] = df[\"cleaned_title_tokens_simple\"].apply(keep_only_diseases)\n",
    "df[\"disease_title_tokens_hf\"] = df[\"cleaned_title_tokens_hf\"].apply(keep_only_diseases)\n",
    "\n",
    "# Then, for frequency:\n",
    "from collections import Counter\n",
    "\n",
    "disease_flat_simple = [tok for tokens in df[\"disease_title_tokens_simple\"] for tok in tokens]\n",
    "simple_disease_freq = Counter(disease_flat_simple).most_common(20)\n",
    "\n",
    "disease_flat_hf = [tok for tokens in df[\"disease_title_tokens_hf\"] for tok in tokens]\n",
    "hf_disease_freq = Counter(disease_flat_hf).most_common(20)\n",
    "\n",
    "print(\"Top 20 diseases title (simple):\", simple_disease_freq)\n",
    "print(\"Top 20 diseases title (HF):\", hf_disease_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with cleaned columns:\n",
    "df[\"disease_abstract_tokens_simple\"] = df[\"cleaned_abstract_tokens_simple\"].apply(keep_only_diseases)\n",
    "df[\"disease_abstract_tokens_hf\"] = df[\"cleaned_abstract_tokens_hf\"].apply(keep_only_diseases)\n",
    "\n",
    "# Then, for frequency:\n",
    "from collections import Counter\n",
    "\n",
    "disease_flat_simple = [tok for tokens in df[\"disease_abstract_tokens_simple\"] for tok in tokens]\n",
    "simple_disease_freq = Counter(disease_flat_simple).most_common(20)\n",
    "\n",
    "disease_flat_hf = [tok for tokens in df[\"disease_abstract_tokens_hf\"] for tok in tokens]\n",
    "hf_disease_freq = Counter(disease_flat_hf).most_common(20)\n",
    "\n",
    "print(\"Top 20 diseases abstract (simple):\", simple_disease_freq)\n",
    "print(\"Top 20 diseases abstract (HF):\", hf_disease_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros**:\n",
    "\n",
    "Easy to implement.\n",
    "You quickly see if “cancer” or “diabetes” is the top disease token.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "Multi-word diseases like “heart failure” or “chronic obstructive pulmonary disease” won’t match unless we store them as separate tokens or reconstruct them.\n",
    "\n",
    "Subword issues: In Hugging Face tokens, “cancer” might appear as [canc, ##er]. That won’t match “cancer” in disease_dict unless we unify them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) `SciSpacy` (or spaCy) Biomedical NER Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, We use a trained biomedical NER model (like en_ner_bc5cdr_md) that can detect DISEASE entities in your text. This can handle multi-word diseases and synonyms automatically. You’ll need to:\n",
    "\n",
    "Install scispacy: pip install scispacy\n",
    "Install the specific model, e.g. en_ner_bc5cdr_md, via\n",
    "pip install \"your folder with .zip file\" - download at https://allenai.github.io/scispacy/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install scispacy\n",
    "#%pip install scispacy\n",
    "\n",
    "#en_ner_bc5cdr_md\tF1 84.28\tDISEASE, CHEMICAL\n",
    "\n",
    "# Check project directory\n",
    "# import os\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing certain model from archive file downloaded from scispacy website\n",
    "#%pip install ScispaCy/en_ner_bc5cdr_md-0.5.4.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Of Usage - scispacy website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scispacy\n",
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_ner_bc5cdr_md\")\n",
    "# text = \"\"\"\n",
    "# Myeloid derived suppressor cells (MDSC) are immature \n",
    "# myeloid cells with immunosuppressive activity. \n",
    "# They accumulate in tumor-bearing mice and humans \n",
    "# with different types of cancer, including hepatocellular \n",
    "# carcinoma (HCC).\n",
    "# \"\"\"\n",
    "# doc = nlp(text)\n",
    "\n",
    "# print(list(doc.sents))\n",
    "# # >>> [\"Myeloid derived suppressor cells (MDSC) are immature myeloid cells with immunosuppressive activity.\", \n",
    "# #      \"They accumulate in tumor-bearing mice and humans with different types of cancer, including hepatocellular carcinoma (HCC).\"]\n",
    "\n",
    "# # Examine the entities extracted by the mention detector.\n",
    "# # Note that they don't have types like in SpaCy, and they\n",
    "# # are more general (e.g including verbs) - these are any\n",
    "# # spans which might be an entity in UMLS, a large\n",
    "# # biomedical database.\n",
    "# print(doc.ents)\n",
    "# # >>> (Myeloid derived suppressor cells,\n",
    "# #      MDSC,\n",
    "# #      immature,\n",
    "# #      myeloid cells,\n",
    "# #      immunosuppressive activity,\n",
    "# #      accumulate,\n",
    "# #      tumor-bearing mice,\n",
    "# #      humans,\n",
    "# #      cancer,\n",
    "# #      hepatocellular carcinoma,\n",
    "# #      HCC)\n",
    "\n",
    "# # We can also visualise dependency parses\n",
    "# # (This renders automatically inside a jupyter notebook!):\n",
    "# from spacy import displacy\n",
    "# displacy.render(next(doc.sents), style='dep', jupyter=True)\n",
    "\n",
    "# # See below for the generated SVG.\n",
    "# # Zoom your browser in a bit!\n",
    "# # The graphic won't be used with our dataset as it kinda resource-intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning + Setting Up ENV with smaller dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Force the garbage collector to run\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%who\n",
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"abstract_tokens_simple\", \"abstract_tokens_hf\",\"title_tokens_simple\",\"title_tokens_hf\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    folder_path = \"Data/2.Processed/ModellingData\"\n",
    "    final_file = \"P1_all.parquet\"\n",
    "    batch_size = 100_000 \n",
    "    \n",
    "    result_path = save_and_merge_in_batches(\n",
    "        df=df,\n",
    "        batch_size=batch_size,\n",
    "        output_folder=folder_path,\n",
    "        final_filename=final_file,\n",
    "        temp_batch_prefix=\"temp_batch_\"\n",
    "    )\n",
    "\n",
    "    print(f\"All done. Merged file at: {result_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`UID` Distinct:\n",
    "1057871 (100%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title = df[['uid', 'title']].copy()\n",
    "df_abstract = df[['uid', 'abstract']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Folder and file paths\n",
    "folder_path = \"Data/2.Processed/ModellingData\"\n",
    "\n",
    "file_name_title = \"P2_title.parquet\"\n",
    "file_name_abstract = \"P2_abstract.parquet\"\n",
    "\n",
    "file_path_title = os.path.join(folder_path, file_name_title)\n",
    "file_path_abstract = os.path.join(folder_path, file_name_abstract)\n",
    "\n",
    "# 1. Save the DataFrame as a single Parquet file\n",
    "df_title.to_parquet(file_path_title, index=False, compression=\"snappy\")\n",
    "print(f\"DataFrame saved as a single Parquet file: {file_path_title}\")\n",
    "\n",
    "df_abstract.to_parquet(file_path_abstract, index=False, compression=\"snappy\")\n",
    "print(f\"DataFrame saved as a single Parquet file: {file_path_abstract}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reseting ev for cleaning memory\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Print CPU usage percentage\n",
    "print(f\"CPU usage: {psutil.cpu_percent()}%\")\n",
    "\n",
    "# Print memory usage\n",
    "memory_info = psutil.virtual_memory()\n",
    "print(f\"Memory usage: {memory_info.percent}%\")\n",
    "\n",
    "# Get the total number of CPUs\n",
    "print(f\"Number of CPUs available: {cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SciSpacy `Title`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing `Title` with SciSpacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"Data/2.Processed/ModellingData/P2_title.parquet\"\n",
    "    batch_size = 100_000  # Define your desired chunk size\n",
    "    \n",
    "    df_title = read_parquet_in_batches_with_progress(file_path, batch_size)\n",
    "    \n",
    "    print(f\"\\nFinal DataFrame with {len(df_title)} rows:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# COMPLETE CODE: CHUNK-BASED DISEASE EXTRACTION (BC5CDR) + TIME LEFT ESTIMATE\n",
    "###############################################################################\n",
    "\n",
    "import os\n",
    "import time\n",
    "import scispacy\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "###############################################################################\n",
    "# 1) LOAD BC5CDR MODEL, DISABLING COMPONENTS FOR SPEED\n",
    "###############################################################################\n",
    "try:\n",
    "    nlp_bc5cdr = spacy.load(\n",
    "        \"en_ner_bc5cdr_md\", \n",
    "        disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Could not load 'en_ner_bc5cdr_md'. Make sure you installed:\")\n",
    "    print(\"  pip install scispacy\")\n",
    "    print(\"  pip install en_ner_bc5cdr_md-0.5.4.tar.gz (or your local path)\")\n",
    "    raise e\n",
    "\n",
    "def extract_diseases_spacy(doc):\n",
    "    \"\"\"\n",
    "    Extract disease mentions from BC5CDR model. (ent.label_ in {CHEMICAL, DISEASE})\n",
    "    We only keep label == 'DISEASE'.\n",
    "    \"\"\"\n",
    "    diseases = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"DISEASE\":\n",
    "            diseases.append(ent.text)\n",
    "    return diseases\n",
    "\n",
    "###############################################################################\n",
    "# 2) CHUNK PROCESSING WITH RESUME & TIME REMAIN ESTIMATE\n",
    "###############################################################################\n",
    "def process_diseases_in_chunks_with_resume(\n",
    "    df,\n",
    "    text_col=\"title\",\n",
    "    chunk_size=10_000,\n",
    "    batch_size=32,\n",
    "    save_path=\"partial_bc5cdr.parquet\"\n",
    "):\n",
    "    \"\"\"\n",
    "    - df: main DataFrame\n",
    "    - text_col: column with text to process\n",
    "    - chunk_size: # of rows per chunk\n",
    "    - batch_size: # docs per nlp.pipe() batch\n",
    "    - save_path: Parquet file to store partial/final results\n",
    "\n",
    "    1) Resumes from an existing partial file if it exists.\n",
    "    2) Processes row by row in chunks, each chunk using spaCy's pipe for faster NER.\n",
    "    3) Shows a progress bar + estimates time left based on chunk durations.\n",
    "    4) Saves partial results after each chunk, then a final full save.\n",
    "    5) The disease mentions get stored in df[\"disease_entities_spacy\"].\n",
    "    \"\"\"\n",
    "    # Reset index so row order is stable\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Initialize the column if absent\n",
    "    if \"disease_entities_spacy\" not in df.columns:\n",
    "        df[\"disease_entities_spacy\"] = None\n",
    "\n",
    "    # Figure out how many rows are already done if partial file is found\n",
    "    start_idx = 0\n",
    "    if os.path.exists(save_path):\n",
    "        try:\n",
    "            partial_df = pd.read_parquet(save_path)\n",
    "            if \"disease_entities_spacy\" in partial_df.columns:\n",
    "                df[\"disease_entities_spacy\"] = partial_df[\"disease_entities_spacy\"]\n",
    "                done_mask = df[\"disease_entities_spacy\"].notna()\n",
    "                done_rows = done_mask.sum()\n",
    "                start_idx = done_rows\n",
    "                print(f\"Resuming from row {start_idx} based on partial file {save_path}.\")\n",
    "            else:\n",
    "                print(f\"WARNING: {save_path} lacks 'disease_entities_spacy'. Starting fresh.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading partial file {save_path}: {e}\")\n",
    "            print(\"Starting from scratch.\")\n",
    "    else:\n",
    "        print(\"No partial file found. Starting from scratch.\")\n",
    "\n",
    "    total_rows = len(df)\n",
    "    if start_idx >= total_rows:\n",
    "        print(f\"All {total_rows} rows processed. Nothing to do.\")\n",
    "        return df\n",
    "\n",
    "    # Calculate how many chunks remain\n",
    "    remaining = total_rows - start_idx\n",
    "    num_chunks = (remaining + chunk_size - 1) // chunk_size\n",
    "    print(f\"Starting chunked processing from row {start_idx}/{total_rows}, \"\n",
    "          f\"{remaining} rows left, {num_chunks} chunks.\\n\")\n",
    "\n",
    "    cur_row = start_idx\n",
    "    chunk_times = []  # keep track of each chunk's duration to estimate time left\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    with tqdm(total=num_chunks, desc=\"Processing Chunks\", unit=\"chunk\") as pbar:\n",
    "        for i in range(num_chunks):\n",
    "            chunk_start_time = time.time()\n",
    "\n",
    "            end_idx = min(cur_row + chunk_size, total_rows)\n",
    "            chunk = df.iloc[cur_row:end_idx].copy()\n",
    "            texts = chunk[text_col].fillna(\"\").tolist()\n",
    "\n",
    "            # We'll store the results\n",
    "            results = []\n",
    "\n",
    "            # Use spaCy pipe in batch\n",
    "            for doc in nlp_bc5cdr.pipe(texts, batch_size=batch_size):\n",
    "                diseases = extract_diseases_spacy(doc)\n",
    "                results.append(diseases)\n",
    "\n",
    "            # Store in chunk and main df\n",
    "            chunk[\"disease_entities_spacy\"] = results\n",
    "            df.iloc[cur_row:end_idx, df.columns.get_loc(\"disease_entities_spacy\")] = chunk[\"disease_entities_spacy\"]\n",
    "\n",
    "            # Partial save\n",
    "            df.iloc[:end_idx].to_parquet(save_path, index=False)\n",
    "\n",
    "            # Chunk timing\n",
    "            chunk_duration = time.time() - chunk_start_time\n",
    "            chunk_times.append(chunk_duration)\n",
    "            chunks_done = i + 1\n",
    "            chunks_left = num_chunks - chunks_done\n",
    "            # average chunk time so far\n",
    "            avg_chunk_time = sum(chunk_times) / chunks_done\n",
    "            est_time_left = avg_chunk_time * chunks_left\n",
    "\n",
    "            # Update progress bar description with estimated time left\n",
    "            pbar.set_postfix({\n",
    "                \"Last Chunk Time\": f\"{chunk_duration:.1f}s\",\n",
    "                \"Est. Time Left\": f\"{est_time_left/60:.1f} min\"\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "            cur_row = end_idx\n",
    "\n",
    "    # Final full save\n",
    "    df.to_parquet(save_path, index=False)\n",
    "    print(f\"All done! Full results saved to {save_path}.\\n\")\n",
    "    return df\n",
    "\n",
    "###############################################################################\n",
    "# EXAMPLE USAGE\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Adjust chunk_size, batch_size to fit your environment\n",
    "    df_title = process_diseases_in_chunks_with_resume(\n",
    "        df_title,\n",
    "        text_col=\"title\",\n",
    "        chunk_size=10_000,      \n",
    "        batch_size=64,       \n",
    "        save_path=\"Data/2.Processed/ModellingData/P3_bc5cdr_results_title.parquet\"\n",
    "    )\n",
    "\n",
    "    # Inspect final results\n",
    "df_title[[\"title\", \"disease_entities_spacy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of elements in each list inside 'disease_entities_spacy'\n",
    "df_title['entity_count'] = df_title['disease_entities_spacy'].apply(len)\n",
    "\n",
    "# 1. Value counts of the number of entities in lists\n",
    "entity_count_value_counts = df_title['entity_count'].value_counts()\n",
    "\n",
    "# 2. Binary categorization: 0 count or more than 0\n",
    "df_title['has_entities'] = df_title['entity_count'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Get value counts for the binary categorization\n",
    "binary_value_counts = df_title['has_entities'].value_counts()\n",
    "\n",
    "# Print both results\n",
    "print(\"Entity Count Value Counts:\")\n",
    "print(entity_count_value_counts)\n",
    "\n",
    "print(\"\\nBinary Categorization (0 vs. More Than 0):\")\n",
    "print(binary_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the rows where the 'entity_count' is 10\n",
    "rows_with_10_entities = df_title[df_title['entity_count'] == 10]\n",
    "\n",
    "rows_with_10_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first determine how many empty rows exist in the `abstract` variable. \n",
    "After assessing this, we will decide on the next steps based on the extent of overlap and the overall data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  SciSpacy `Abstract`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing `Abstract` with SciSpacy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"Data/2.Processed/ModellingData/P2_abstract.parquet\"\n",
    "    batch_size = 100_000  # Define your desired chunk size\n",
    "    \n",
    "    df_abstract = read_parquet_in_batches_with_progress(file_path, batch_size)\n",
    "    \n",
    "    print(f\"\\nFinal DataFrame with {len(df_abstract)} rows:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# COMPLETE CODE: CHUNK-BASED DISEASE EXTRACTION (BC5CDR) + TIME LEFT ESTIMATE\n",
    "###############################################################################\n",
    "\n",
    "import os\n",
    "import time\n",
    "import scispacy\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "###############################################################################\n",
    "# 1) LOAD BC5CDR MODEL, DISABLING COMPONENTS FOR SPEED\n",
    "###############################################################################\n",
    "try:\n",
    "    nlp_bc5cdr = spacy.load(\n",
    "        \"en_ner_bc5cdr_md\", \n",
    "        disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Could not load 'en_ner_bc5cdr_md'. Make sure you installed:\")\n",
    "    print(\"  pip install scispacy\")\n",
    "    print(\"  pip install en_ner_bc5cdr_md-0.5.4.tar.gz (or your local path)\")\n",
    "    raise e\n",
    "\n",
    "def extract_diseases_spacy(doc):\n",
    "    \"\"\"\n",
    "    Extract disease mentions from BC5CDR model. (ent.label_ in {CHEMICAL, DISEASE})\n",
    "    We only keep label == 'DISEASE'.\n",
    "    \"\"\"\n",
    "    diseases = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"DISEASE\":\n",
    "            diseases.append(ent.text)\n",
    "    return diseases\n",
    "\n",
    "###############################################################################\n",
    "# 2) CHUNK PROCESSING WITH RESUME & TIME REMAIN ESTIMATE\n",
    "###############################################################################\n",
    "def process_diseases_in_chunks_with_resume(\n",
    "    df,\n",
    "    text_col=\"title\",\n",
    "    chunk_size=10_000,\n",
    "    batch_size=32,\n",
    "    save_path=\"partial_bc5cdr.parquet\"\n",
    "):\n",
    "    \"\"\"\n",
    "    - df: main DataFrame\n",
    "    - text_col: column with text to process\n",
    "    - chunk_size: # of rows per chunk\n",
    "    - batch_size: # docs per nlp.pipe() batch\n",
    "    - save_path: Parquet file to store partial/final results\n",
    "\n",
    "    1) Resumes from an existing partial file if it exists.\n",
    "    2) Processes row by row in chunks, each chunk using spaCy's pipe for faster NER.\n",
    "    3) Shows a progress bar + estimates time left based on chunk durations.\n",
    "    4) Saves partial results after each chunk, then a final full save.\n",
    "    5) The disease mentions get stored in df[\"disease_entities_spacy\"].\n",
    "    \"\"\"\n",
    "    # Reset index so row order is stable\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Initialize the column if absent\n",
    "    if \"disease_entities_spacy\" not in df.columns:\n",
    "        df[\"disease_entities_spacy\"] = None\n",
    "\n",
    "    # Figure out how many rows are already done if partial file is found\n",
    "    start_idx = 0\n",
    "    if os.path.exists(save_path):\n",
    "        try:\n",
    "            partial_df = pd.read_parquet(save_path)\n",
    "            if \"disease_entities_spacy\" in partial_df.columns:\n",
    "                df[\"disease_entities_spacy\"] = partial_df[\"disease_entities_spacy\"]\n",
    "                done_mask = df[\"disease_entities_spacy\"].notna()\n",
    "                done_rows = done_mask.sum()\n",
    "                start_idx = done_rows\n",
    "                print(f\"Resuming from row {start_idx} based on partial file {save_path}.\")\n",
    "            else:\n",
    "                print(f\"WARNING: {save_path} lacks 'disease_entities_spacy'. Starting fresh.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading partial file {save_path}: {e}\")\n",
    "            print(\"Starting from scratch.\")\n",
    "    else:\n",
    "        print(\"No partial file found. Starting from scratch.\")\n",
    "\n",
    "    total_rows = len(df)\n",
    "    if start_idx >= total_rows:\n",
    "        print(f\"All {total_rows} rows processed. Nothing to do.\")\n",
    "        return df\n",
    "\n",
    "    # Calculate how many chunks remain\n",
    "    remaining = total_rows - start_idx\n",
    "    num_chunks = (remaining + chunk_size - 1) // chunk_size\n",
    "    print(f\"Starting chunked processing from row {start_idx}/{total_rows}, \"\n",
    "          f\"{remaining} rows left, {num_chunks} chunks.\\n\")\n",
    "\n",
    "    cur_row = start_idx\n",
    "    chunk_times = []  # keep track of each chunk's duration to estimate time left\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    with tqdm(total=num_chunks, desc=\"Processing Chunks\", unit=\"chunk\") as pbar:\n",
    "        for i in range(num_chunks):\n",
    "            chunk_start_time = time.time()\n",
    "\n",
    "            end_idx = min(cur_row + chunk_size, total_rows)\n",
    "            chunk = df.iloc[cur_row:end_idx].copy()\n",
    "            texts = chunk[text_col].fillna(\"\").tolist()\n",
    "\n",
    "            # We'll store the results\n",
    "            results = []\n",
    "\n",
    "            # Use spaCy pipe in batch\n",
    "            for doc in nlp_bc5cdr.pipe(texts, batch_size=batch_size):\n",
    "                diseases = extract_diseases_spacy(doc)\n",
    "                results.append(diseases)\n",
    "\n",
    "            # Store in chunk and main df\n",
    "            chunk[\"disease_entities_spacy\"] = results\n",
    "            df.iloc[cur_row:end_idx, df.columns.get_loc(\"disease_entities_spacy\")] = chunk[\"disease_entities_spacy\"]\n",
    "\n",
    "            # Partial save\n",
    "            df.iloc[:end_idx].to_parquet(save_path, index=False)\n",
    "\n",
    "            # Chunk timing\n",
    "            chunk_duration = time.time() - chunk_start_time\n",
    "            chunk_times.append(chunk_duration)\n",
    "            chunks_done = i + 1\n",
    "            chunks_left = num_chunks - chunks_done\n",
    "            # average chunk time so far\n",
    "            avg_chunk_time = sum(chunk_times) / chunks_done\n",
    "            est_time_left = avg_chunk_time * chunks_left\n",
    "\n",
    "            # Update progress bar description with estimated time left\n",
    "            pbar.set_postfix({\n",
    "                \"Last Chunk Time\": f\"{chunk_duration:.1f}s\",\n",
    "                \"Est. Time Left\": f\"{est_time_left/60:.1f} min\"\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "            cur_row = end_idx\n",
    "\n",
    "    # Final full save\n",
    "    df.to_parquet(save_path, index=False)\n",
    "    print(f\"All done! Full results saved to {save_path}.\\n\")\n",
    "    return df\n",
    "\n",
    "###############################################################################\n",
    "# EXAMPLE USAGE\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Adjust chunk_size, batch_size to fit your environment\n",
    "    df_abstract = process_diseases_in_chunks_with_resume(\n",
    "        df_abstract,\n",
    "        text_col=\"abstract\",\n",
    "        chunk_size=10_000,      \n",
    "        batch_size=64,       \n",
    "        save_path=\"Data/2.Processed/ModellingData/P3_bc5cdr_results_abstract.parquet\"\n",
    "    )\n",
    "\n",
    "    # Inspect final results\n",
    "    df_abstract[[\"abstract\", \"disease_entities_spacy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of elements in each list inside 'disease_entities_spacy'\n",
    "df_abstract['entity_count'] = df_abstract['disease_entities_spacy'].apply(len)\n",
    "\n",
    "# 1. Value counts of the number of entities in lists\n",
    "entity_count_value_counts = df_abstract['entity_count'].value_counts()\n",
    "\n",
    "# 2. Binary categorization: 0 count or more than 0\n",
    "df_abstract['has_entities'] = df_abstract['entity_count'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Get value counts for the binary categorization\n",
    "binary_value_counts = df_abstract['has_entities'].value_counts()\n",
    "\n",
    "# Print both results\n",
    "print(\"Entity Count Value Counts:\")\n",
    "print(entity_count_value_counts)\n",
    "\n",
    "print(\"\\nBinary Categorization (0 vs. More Than 0):\")\n",
    "print(binary_value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially identified 87,103 rows that might need to be removed. As part of the analysis, we also compared rows with empty entities in titles and abstracts to check how they intersect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames on the 'uid' column\n",
    "merged_df = pd.merge(\n",
    "    df_abstract[['uid', 'has_entities']], \n",
    "    df_title[['uid', 'has_entities']], \n",
    "    on='uid', \n",
    "    suffixes=('_abstract', '_title')\n",
    ")\n",
    "\n",
    "# Analyze rows based on their entity presence\n",
    "no_entities_abstract = merged_df['has_entities_abstract'] == 0\n",
    "no_entities_title = merged_df['has_entities_title'] == 0\n",
    "\n",
    "# Count rows with:\n",
    "# 1. No entities in both abstract and title\n",
    "no_entities_both = merged_df[no_entities_abstract & no_entities_title].shape[0]\n",
    "\n",
    "# 2. No entities in the abstract but entities in the title\n",
    "no_entities_abstract_only = merged_df[no_entities_abstract & ~no_entities_title].shape[0]\n",
    "\n",
    "# 3. No entities in the title but entities in the abstract\n",
    "no_entities_title_only = merged_df[~no_entities_abstract & no_entities_title].shape[0]\n",
    "\n",
    "# 4. Entities in both\n",
    "entities_in_both = merged_df[~no_entities_abstract & ~no_entities_title].shape[0]\n",
    "\n",
    "# Print the results\n",
    "print(\"Summary of Rows with Entities:\")\n",
    "print(f\"Rows with no entities in both abstract and title: {no_entities_both}\")\n",
    "print(f\"Rows with no entities in abstract but entities in title: {no_entities_abstract_only}\")\n",
    "print(f\"Rows with no entities in title but entities in abstract: {no_entities_title_only}\")\n",
    "print(f\"Rows with entities in both abstract and title: {entities_in_both}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The results are as follows:**\n",
    "\n",
    "1. Rows with no entities in both abstract and title: 80,868\n",
    "2. Rows with no entities in abstract but entities in title: 6,235\n",
    "3. Rows with no entities in title but entities in abstract: 323,102\n",
    "4. Rows with entities in both abstract and title: 647,666\n",
    "\n",
    "Based on this, we now focus on the **80,868** rows that lack entities in both abstract and title. For these rows, we will perform a search through key *MeSH keywords terms* to check if any tokens are present.\n",
    "\n",
    "**If no matching tokens are found:**\n",
    "\n",
    "*These rows may need to be removed entirely.*\n",
    "\n",
    "*Alternatively, we can explore inputting values from where we have done simple tokenization on another column (e.g., using tokenization techniques like happy hugging tokens) to generate adequate terms.*\n",
    "\n",
    "*Or just letting these articles be as they were due to our query and in future researches, it may be advised to check on articles (?, but then how, Someones could have even more entries to dataset than we have; 1 million)* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *WHAT'S NEXT?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Use UMLS or MeSH*:\n",
    "\n",
    "We could use resources like UMLS (Unified Medical Language System), MeSH (Medical Subject Headings), or disease-related databases that categorize diseases. These resources contain semantic relationships between terms, and use them to map diseases to broader categories automatically.\n",
    "\n",
    "UMLS contains concepts and relationships for diseases, including synonyms and broader categories like \"cancer,\" \"neurological disorders,\" etc.\n",
    "We could use a pre-built Python library like pydantic-uml or PyMedTermino to query UMLS and retrieve disease categories automatically.\n",
    "\n",
    "*Use a Synonym Mapping Dictionary*:\n",
    "\n",
    "We could download or build a synonym dictionary for diseases (for example, mapping all types of carcinoma to \"cancer\").\n",
    "There are various disease databases that already categorize diseases into high-level categories like \"cancer,\" \"infectious disease,\" \"neurological disorder,\" etc.\n",
    "\n",
    "*Entity Linking*:\n",
    "\n",
    "We could perform entity linking to map disease entities to broader categories based on a pre-trained model or a database of known relationships. Some tools (like scispaCy) have built-in support for linking recognized entities to broader concepts.\n",
    "Group Diseases into Categories Using Predefined Rules:\n",
    "\n",
    "After extracting diseases with spaCy and linking them to categories, We could group them under broader categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging data frames with SciSpacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 100_000  # Define your desired chunk size\n",
    "\n",
    "    # File paths\n",
    "    file_abstract = \"Data/2.Processed/ModellingData/P3_bc5cdr_results_abstract.parquet\"\n",
    "    file_title = \"Data/2.Processed/ModellingData/P3_bc5cdr_results_title.parquet\"\n",
    "    file_all = \"Data/2.Processed/ModellingData/P1_all.parquet\"\n",
    "\n",
    "    # Read the abstract and title datasets\n",
    "    df_abstract = read_parquet_in_batches_with_progress(file_abstract, batch_size)\n",
    "    df_title = read_parquet_in_batches_with_progress(file_title, batch_size)\n",
    "\n",
    "    # Rename columns\n",
    "    df_abstract.rename(columns={\"disease_entities_spacy\": \"disease_abstract_spacy\"}, inplace=True)\n",
    "    df_title.rename(columns={\"disease_entities_spacy\": \"disease_title_spacy\"}, inplace=True)\n",
    "\n",
    "    # Select only the necessary columns for merging\n",
    "    df_abstract = df_abstract[[\"uid\", \"disease_abstract_spacy\"]]\n",
    "    df_title = df_title[[\"uid\", \"disease_title_spacy\"]]\n",
    "\n",
    "    # Merge abstract and title datasets\n",
    "    df_combined = pd.merge(df_abstract, df_title, on=\"uid\", how=\"inner\")\n",
    "\n",
    "    # Read the main dataset\n",
    "    df_all = read_parquet_in_batches_with_progress(file_all, batch_size)\n",
    "\n",
    "    # Merge with the main dataset\n",
    "    df_final = pd.merge(df_all, df_combined, on=\"uid\", how=\"inner\")\n",
    "\n",
    "    print(f\"\\nFinal DataFrame with {len(df_final)} rows:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`saving dataframe` -> uncomment if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "\n",
    "# # Folder and file paths\n",
    "# folder_path = \"Data/2.Processed/ModellingData\"\n",
    "\n",
    "# file_name_final = \"P4_final_merged.parquet\"\n",
    "\n",
    "# file_path_final = os.path.join(folder_path, file_name_final)\n",
    "\n",
    "# # 1. Save the DataFrame as a single Parquet file\n",
    "# df_final.to_parquet(file_path_final, index=False, compression=\"snappy\")\n",
    "# print(f\"DataFrame saved as a single Parquet file: {file_path_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    folder_path = \"Data/2.Processed/ModellingData\"\n",
    "    final_file = \"P4_final_merged.parquet\"\n",
    "    batch_size = 100_000 \n",
    "    \n",
    "    result_path = save_and_merge_in_batches(\n",
    "        df=df_final,\n",
    "        batch_size=batch_size,\n",
    "        output_folder=folder_path,\n",
    "        final_filename=final_file,\n",
    "        temp_batch_prefix=\"temp_batch_\"\n",
    "    )\n",
    "\n",
    "    print(f\"All done. Merged file at: {result_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`resseting environment for realese of RAM and easing computer work`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"Data/2.Processed/ModellingData/P4_final_merged.parquet\"\n",
    "    batch_size = 100_000  # Define your desired chunk size\n",
    "    \n",
    "    df = read_parquet_in_batches_with_progress(file_path, batch_size)\n",
    "    \n",
    "    print(f\"\\nFinal DataFrame with {len(df)} rows:\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Further Analysis Of Abstract And Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def read_parquet_in_batches_with_progress(file_path, batch_size):\n",
    "    \"\"\"\n",
    "    Read a Parquet file in fixed-size row batches with a progress bar and per-chunk logging.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the Parquet file.\n",
    "        batch_size (int): Number of rows per batch.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame after processing all batches.\n",
    "    \"\"\"\n",
    "    # Open the Parquet file\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    \n",
    "    # Total number of rows in the file\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    \n",
    "    # Initialize a list to store DataFrame chunks\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    with tqdm(total=total_rows, desc=\"Processing Batches\", unit=\"rows\") as pbar:\n",
    "        # Enumerate batches for logging\n",
    "        for batch_number, batch in enumerate(parquet_file.iter_batches(batch_size=batch_size), start=1):\n",
    "            # Convert the batch to a Pandas DataFrame\n",
    "            df_batch = batch.to_pandas()\n",
    "            \n",
    "            # Simulate processing (add your custom logic here)\n",
    "            all_chunks.append(df_batch)\n",
    "            \n",
    "            # Update the progress bar\n",
    "            pbar.update(len(df_batch))\n",
    "            \n",
    "            # Print per-chunk information\n",
    "            print(f\"Processed Chunk {batch_number}: {len(df_batch)} rows\")\n",
    "    \n",
    "    # Combine all chunks into a single DataFrame\n",
    "    combined_df = pd.concat(all_chunks, ignore_index=True)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"Data/2.Processed/ModellingData/P4_final_merged.parquet\"\n",
    "    batch_size = 100_000  # Define your desired chunk size\n",
    "    \n",
    "    df = read_parquet_in_batches_with_progress(file_path, batch_size)\n",
    "    \n",
    "    print(f\"\\nFinal DataFrame with {len(df)} rows:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros**:\n",
    "\n",
    "Captures multi-word diseases, synonyms.\n",
    "Doesn’t require you to maintain a dictionary.\n",
    "The model can label “Parkinson’s disease,” “type 2 diabetes,” etc.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "Dependent on the model’s coverage and accuracy.\n",
    "Takes more time than a simple dictionary approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why [CLS] and ##er Appear (Subword Splits)\n",
    "Hugging Face’s DistilBERT tokenizer uses subword or BPE tokenization. It splits unknown words into smaller pieces. '##' means “this subword attaches to the prior subword.” If you keep them for advanced embedding tasks, that’s normal. For classical LDA/TF-IDF on plain words, they can be awkward. You can:\n",
    "\n",
    "Remove [CLS], [SEP], etc. (the special tokens).\n",
    "Potentially remove or unify subwords (canc + ##er → cancer).\n",
    "                             \n",
    "Doing Classical LDA or TF-IDF vs. Embedding Approaches\n",
    "If you do classical topic modeling (LDA, etc.):\n",
    "\n",
    "You typically want full words rather than subwords.\n",
    "You remove or merge subword fragments to form complete tokens.\n",
    "You remove punctuation, maybe remove stopwords, etc.\n",
    "You might store the final tokens as strings in a df[\"final_tokens\"] or something, then do TfidfVectorizer(...).fit_transform([\" \".join(tokens) for tokens in df[\"final_tokens\"]]).\n",
    "If you do advanced embedding-based classification:\n",
    "\n",
    "You keep the subword tokens as the model expects them.\n",
    "Or you feed raw text into AutoTokenizer with truncation=True, max_length=512 at inference time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) EMBEDDINGS - maybe to get into later, very heavy for computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# model_name = \"distilbert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# # Put the model in eval mode (we don't do further training here)\n",
    "# model.eval()\n",
    "\n",
    "# # GPU cuda usage:\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_distilbert_embedding(text, max_len=512):\n",
    "#     \"\"\"\n",
    "#     Convert 'text' into a DistilBERT embedding by:\n",
    "#       1) Tokenizing with subword tokens (including [CLS], [SEP], etc.).\n",
    "#       2) Running model to get last_hidden_state.\n",
    "#       3) Mean-pooling the token vectors to produce one 768D vector.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(text, str) or not text.strip():\n",
    "#         # Return a zero vector if empty\n",
    "#         return torch.zeros(model.config.hidden_size)\n",
    "\n",
    "#     # Tokenize & encode\n",
    "#     inputs = tokenizer(\n",
    "#         text,\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=max_len,\n",
    "#         truncation=True,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "#     # Move data to GPU if available\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "#     # Forward pass\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)\n",
    "#         # DistilBERT -> outputs.last_hidden_state is (batch_size, seq_len, hidden_dim)\n",
    "#         last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "#     # Mean pooling across seq_len dimension\n",
    "#     # shape: (batch_size, hidden_dim)\n",
    "#     embedding = last_hidden_state.mean(dim=1)[0].cpu()  # move back to CPU\n",
    "#     return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_embeddings_in_chunks(df, text_col, batch_size=1000, max_len=512):\n",
    "#     \"\"\"\n",
    "#     For each row in df, convert text_col to a DistilBERT embedding.\n",
    "#     We'll store the result in df[\"embedding\"] as a list of floats (768D).\n",
    "    \n",
    "#     If you have a huge DataFrame, chunking helps avoid GPU out-of-memory.\n",
    "#     \"\"\"\n",
    "#     df[\"embedding\"] = None  # Initialize empty column\n",
    "#     total_rows = len(df)\n",
    "#     num_batches = (total_rows + batch_size - 1) // batch_size\n",
    "\n",
    "#     start_idx = 0\n",
    "#     for i in range(num_batches):\n",
    "#         end_idx = min(start_idx + batch_size, total_rows)\n",
    "#         batch = df.iloc[start_idx:end_idx].copy()\n",
    "\n",
    "#         # Compute embeddings for each row\n",
    "#         embeddings_list = []\n",
    "#         for idx, row in batch.iterrows():\n",
    "#             text = row[text_col]\n",
    "#             emb = get_distilbert_embedding(text, max_len=max_len)\n",
    "#             # Convert to list of floats if you want to store in DataFrame easily\n",
    "#             embeddings_list.append(emb.tolist())\n",
    "\n",
    "#         # Assign them back to df\n",
    "#         df.loc[df.index[start_idx:end_idx], \"embedding\"] = embeddings_list\n",
    "\n",
    "#         start_idx = end_idx\n",
    "#         print(f\"Processed batch {i+1}/{num_batches}. Rows {start_idx} so far.\")\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Conclusion & Recommendations** ; title + abstract\n",
    "\n",
    "1. *Dictionary approach: Quick, but you must handle subwords or multi-word diseases carefully.*\n",
    "\n",
    "2. *NER approach (SciSpacy, en_ner_bc5cdr_md): Better for multi-word disease detection.*\n",
    "\n",
    "3. *Embedding approach: Keep subword tokens + special tokens for BERT-based classification or embedding extraction. Use the code above to produce a 768D vector per document.*\n",
    "\n",
    "Stopwords & punctuation: For embedding-based approaches (like DistilBERT), do not manually remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: `mesh_terms`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuation of exploring lack of tokens related to diseases for ~ 80k records to conclude wether we want to keep these these rows or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"mesh_list\"] = (\n",
    "    df[\"mesh_terms\"]\n",
    "    .fillna(\"\")  # replace NaN with empty string\n",
    "    .str.split(\";\")\n",
    "    .apply(lambda x: [m.strip() for m in x if m.strip()])  # strip spaces, remove empties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "mesh_counter = Counter()\n",
    "for mesh_terms in df[\"mesh_list\"]:\n",
    "    # mesh_terms is a list, e.g. [\"Adolescent\", \"Adult\", ...]\n",
    "    mesh_counter.update(mesh_terms)\n",
    "\n",
    "# Turn counter into a DataFrame sorted by frequency\n",
    "mesh_freq_df = pd.DataFrame(mesh_counter.most_common(), columns=[\"mesh_term\", \"count\"])\n",
    "mesh_freq_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the list so each row in df_exploded is one MeSH term\n",
    "df_exploded = df.explode(\"mesh_list\")\n",
    "# Then we can do a value_counts on the single item\n",
    "mesh_freq = df_exploded[\"mesh_list\"].value_counts(dropna=False)\n",
    "print(\"Number of unique MeSH terms:\", len(mesh_freq))\n",
    "print(\"Top 20 MeSH terms:\")\n",
    "print(mesh_freq.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mesh_list = df[['uid', 'mesh_list']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# Folder and file paths\n",
    "folder_path = \"Data/2.Processed/ModellingData\"\n",
    "\n",
    "file_name_mesh_list = \"P2_mesh_list.parquet\"\n",
    "\n",
    "file_path_mesh_list = os.path.join(folder_path, file_name_mesh_list)\n",
    "\n",
    "# 1. Save the DataFrame as a single Parquet file\n",
    "df_mesh_list.to_parquet(file_path_mesh_list, index=False, compression=\"snappy\")\n",
    "print(f\"DataFrame saved as a single Parquet file: {file_path_mesh_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# COMPLETE CODE: CHUNK-BASED DISEASE EXTRACTION (BC5CDR) + TIME LEFT ESTIMATE\n",
    "###############################################################################\n",
    "\n",
    "import os\n",
    "import time\n",
    "import scispacy\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "###############################################################################\n",
    "# 1) LOAD BC5CDR MODEL, DISABLING COMPONENTS FOR SPEED\n",
    "###############################################################################\n",
    "try:\n",
    "    nlp_bc5cdr = spacy.load(\n",
    "        \"en_ner_bc5cdr_md\", \n",
    "        disable=[\"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"Could not load 'en_ner_bc5cdr_md'. Make sure you installed:\")\n",
    "    print(\"  pip install scispacy\")\n",
    "    print(\"  pip install en_ner_bc5cdr_md-0.5.4.tar.gz (or your local path)\")\n",
    "    raise e\n",
    "\n",
    "def extract_diseases_spacy(doc):\n",
    "    \"\"\"\n",
    "    Extract disease mentions from BC5CDR model. (ent.label_ in {CHEMICAL, DISEASE})\n",
    "    We only keep label == 'DISEASE'.\n",
    "    \"\"\"\n",
    "    diseases = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"DISEASE\":\n",
    "            diseases.append(ent.text)\n",
    "    return diseases\n",
    "\n",
    "###############################################################################\n",
    "# 2) CHUNK PROCESSING WITH RESUME & TIME REMAIN ESTIMATE\n",
    "###############################################################################\n",
    "def process_diseases_in_chunks_with_resume(\n",
    "    df,\n",
    "    text_col=\"title\",\n",
    "    chunk_size=10_000,\n",
    "    batch_size=32,\n",
    "    save_path=\"partial_bc5cdr.parquet\"\n",
    "):\n",
    "    \"\"\"\n",
    "    - df: main DataFrame\n",
    "    - text_col: column with text to process\n",
    "    - chunk_size: # of rows per chunk\n",
    "    - batch_size: # docs per nlp.pipe() batch\n",
    "    - save_path: Parquet file to store partial/final results\n",
    "\n",
    "    1) Resumes from an existing partial file if it exists.\n",
    "    2) Processes row by row in chunks, each chunk using spaCy's pipe for faster NER.\n",
    "    3) Shows a progress bar + estimates time left based on chunk durations.\n",
    "    4) Saves partial results after each chunk, then a final full save.\n",
    "    5) The disease mentions get stored in df[\"disease_entities_spacy\"].\n",
    "    \"\"\"\n",
    "    # Reset index so row order is stable\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Initialize the column if absent\n",
    "    if \"disease_entities_spacy\" not in df.columns:\n",
    "        df[\"disease_entities_spacy\"] = None\n",
    "\n",
    "    # Figure out how many rows are already done if partial file is found\n",
    "    start_idx = 0\n",
    "    if os.path.exists(save_path):\n",
    "        try:\n",
    "            partial_df = pd.read_parquet(save_path)\n",
    "            if \"disease_entities_spacy\" in partial_df.columns:\n",
    "                df[\"disease_entities_spacy\"] = partial_df[\"disease_entities_spacy\"]\n",
    "                done_mask = df[\"disease_entities_spacy\"].notna()\n",
    "                done_rows = done_mask.sum()\n",
    "                start_idx = done_rows\n",
    "                print(f\"Resuming from row {start_idx} based on partial file {save_path}.\")\n",
    "            else:\n",
    "                print(f\"WARNING: {save_path} lacks 'disease_entities_spacy'. Starting fresh.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading partial file {save_path}: {e}\")\n",
    "            print(\"Starting from scratch.\")\n",
    "    else:\n",
    "        print(\"No partial file found. Starting from scratch.\")\n",
    "\n",
    "    total_rows = len(df)\n",
    "    if start_idx >= total_rows:\n",
    "        print(f\"All {total_rows} rows processed. Nothing to do.\")\n",
    "        return df\n",
    "\n",
    "    # Calculate how many chunks remain\n",
    "    remaining = total_rows - start_idx\n",
    "    num_chunks = (remaining + chunk_size - 1) // chunk_size\n",
    "    print(f\"Starting chunked processing from row {start_idx}/{total_rows}, \"\n",
    "          f\"{remaining} rows left, {num_chunks} chunks.\\n\")\n",
    "\n",
    "    cur_row = start_idx\n",
    "    chunk_times = []  # keep track of each chunk's duration to estimate time left\n",
    "\n",
    "    # Initialize the progress bar\n",
    "    with tqdm(total=num_chunks, desc=\"Processing Chunks\", unit=\"chunk\") as pbar:\n",
    "        for i in range(num_chunks):\n",
    "            chunk_start_time = time.time()\n",
    "\n",
    "            end_idx = min(cur_row + chunk_size, total_rows)\n",
    "            chunk = df.iloc[cur_row:end_idx].copy()\n",
    "            texts = chunk[text_col].fillna(\"\").tolist()\n",
    "\n",
    "            # We'll store the results\n",
    "            results = []\n",
    "\n",
    "            # Use spaCy pipe in batch\n",
    "            for doc in nlp_bc5cdr.pipe(texts, batch_size=batch_size):\n",
    "                diseases = extract_diseases_spacy(doc)\n",
    "                results.append(diseases)\n",
    "\n",
    "            # Store in chunk and main df\n",
    "            chunk[\"disease_entities_spacy\"] = results\n",
    "            df.iloc[cur_row:end_idx, df.columns.get_loc(\"disease_entities_spacy\")] = chunk[\"disease_entities_spacy\"]\n",
    "\n",
    "            # Partial save\n",
    "            df.iloc[:end_idx].to_parquet(save_path, index=False)\n",
    "\n",
    "            # Chunk timing\n",
    "            chunk_duration = time.time() - chunk_start_time\n",
    "            chunk_times.append(chunk_duration)\n",
    "            chunks_done = i + 1\n",
    "            chunks_left = num_chunks - chunks_done\n",
    "            # average chunk time so far\n",
    "            avg_chunk_time = sum(chunk_times) / chunks_done\n",
    "            est_time_left = avg_chunk_time * chunks_left\n",
    "\n",
    "            # Update progress bar description with estimated time left\n",
    "            pbar.set_postfix({\n",
    "                \"Last Chunk Time\": f\"{chunk_duration:.1f}s\",\n",
    "                \"Est. Time Left\": f\"{est_time_left/60:.1f} min\"\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "            cur_row = end_idx\n",
    "\n",
    "    # Final full save\n",
    "    df.to_parquet(save_path, index=False)\n",
    "    print(f\"All done! Full results saved to {save_path}.\\n\")\n",
    "    return df\n",
    "\n",
    "###############################################################################\n",
    "# EXAMPLE USAGE\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Convert list of MeSH terms to a single string per row\n",
    "    df_mesh_list[\"mesh_list_text\"] = df_mesh_list[\"mesh_list\"].apply(lambda x: \" \".join(x) if isinstance(x, list) else str(x))\n",
    "\n",
    "    # Adjust chunk_size, batch_size to fit your environment\n",
    "    df_mesh_list = process_diseases_in_chunks_with_resume(\n",
    "        df_mesh_list,\n",
    "        text_col=\"mesh_list_text\",\n",
    "        chunk_size=10_000,       \n",
    "        batch_size=64,       \n",
    "        save_path=\"Data/2.Processed/ModellingData/P3_bc5cdr_results_mesh_keywords.parquet\"\n",
    "    )\n",
    "\n",
    "    # Inspect final results\n",
    "    df_mesh_list[[\"mesh_list_text\", \"disease_entities_spacy\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mesh_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 100_000  # Define your desired chunk size\n",
    "\n",
    "    # File paths\n",
    "    file_mesh = \"Data/2.Processed/ModellingData/P3_bc5cdr_results_mesh_keywords.parquet\"\n",
    "    file_all = \"Data/2.Processed/ModellingData/P4_final_merged.parquet\"\n",
    "\n",
    "    # Read the abstract and title datasets\n",
    "    df_mesh_list = read_parquet_in_batches_with_progress(file_mesh, batch_size)\n",
    "    df = read_parquet_in_batches_with_progress(file_all, batch_size)\n",
    "\n",
    "    # Rename columns\n",
    "    df_mesh_list.rename(columns={\"disease_entities_spacy\": \"disease_mesh_terms_spacy\"}, inplace=True)\n",
    "\n",
    "    # Select only the necessary columns for merging\n",
    "    df_mesh_list = df_mesh_list[[\"uid\", \"disease_mesh_terms_spacy\"]]\n",
    "\n",
    "    # Read the main dataset\n",
    "    df_all = read_parquet_in_batches_with_progress(file_all, batch_size)\n",
    "\n",
    "    # Merge with the main dataset\n",
    "    df_final = pd.merge(df_all, df_mesh_list, on=\"uid\", how=\"inner\")\n",
    "\n",
    "    print(f\"\\nFinal DataFrame with {len(df_final)} rows:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    folder_path = \"Data/2.Processed/ModellingData\"\n",
    "    final_file = \"P5_final_new.parquet\"\n",
    "    batch_size = 100_000  # e.g. if you want ~10 batches for our current dataset\n",
    "\n",
    "    result_path = save_and_merge_in_batches(\n",
    "        df=df_final,\n",
    "        batch_size=batch_size,\n",
    "        output_folder=folder_path,\n",
    "        final_filename=final_file,\n",
    "        temp_batch_prefix=\"temp_batch_\"\n",
    "    )\n",
    "\n",
    "    print(f\"All done. Merged file at: {result_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Reading dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finalized dataset (as for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    file_path = \"Data/2.Processed/ModellingData/P5_final_new.parquet\"\n",
    "    batch_size = 100_000  # Define your desired chunk size\n",
    "    \n",
    "    df = read_parquet_in_batches_with_progress(file_path, batch_size)\n",
    "    \n",
    "    print(f\"\\nFinal DataFrame with {len(df)} rows:\")\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the absence of tokens in each variable\n",
    "no_tokens_abstract = df['disease_abstract_spacy'].apply(len) == 0\n",
    "no_tokens_title = df['disease_title_spacy'].apply(len) == 0\n",
    "no_tokens_mesh_terms = df['disease_mesh_terms_spacy'].apply(len) == 0\n",
    "\n",
    "# Calculate insights\n",
    "# 1. No tokens in all three variables\n",
    "no_tokens_in_all = (no_tokens_abstract & no_tokens_title & no_tokens_mesh_terms).sum()\n",
    "\n",
    "# 2. No tokens in abstract only\n",
    "no_tokens_in_abstract_only = (no_tokens_abstract & ~no_tokens_title & ~no_tokens_mesh_terms).sum()\n",
    "\n",
    "# 3. No tokens in title only\n",
    "no_tokens_in_title_only = (~no_tokens_abstract & no_tokens_title & ~no_tokens_mesh_terms).sum()\n",
    "\n",
    "# 4. No tokens in mesh terms only\n",
    "no_tokens_in_mesh_terms_only = (~no_tokens_abstract & ~no_tokens_title & no_tokens_mesh_terms).sum()\n",
    "\n",
    "# 5. Tokens missing in at least one variable\n",
    "no_tokens_in_at_least_one = (no_tokens_abstract | no_tokens_title | no_tokens_mesh_terms).sum()\n",
    "\n",
    "# 6. Tokens missing in all variables\n",
    "no_tokens_in_all = (no_tokens_abstract & no_tokens_title & no_tokens_mesh_terms).sum()\n",
    "\n",
    "# Print insights\n",
    "print(\"Summary of Insights:\")\n",
    "print(f\"Rows with no tokens in all three variables: {no_tokens_in_all}\")\n",
    "print(f\"Rows with no tokens in abstract only: {no_tokens_in_abstract_only}\")\n",
    "print(f\"Rows with no tokens in title only: {no_tokens_in_title_only}\")\n",
    "print(f\"Rows with no tokens in mesh terms only: {no_tokens_in_mesh_terms_only}\")\n",
    "print(f\"Rows with no tokens in at least one variable: {no_tokens_in_at_least_one}\")\n",
    "print(f\"Rows with no tokens in all variables: {no_tokens_in_all}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still over 60000 records without clasified medical tokens\n",
    "Rows with no tokens in all variables: 60247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the presence or absence of tokens in each variable\n",
    "no_tokens_abstract = df['disease_abstract_spacy'].apply(len) == 0\n",
    "no_tokens_title = df['disease_title_spacy'].apply(len) == 0\n",
    "has_tokens_mesh_terms = df['disease_mesh_terms_spacy'].apply(len) > 0\n",
    "\n",
    "# Filter rows: Tokens in mesh terms but no tokens in abstract and title\n",
    "rows_with_tokens_in_mesh_only = df[no_tokens_abstract & no_tokens_title & has_tokens_mesh_terms]\n",
    "\n",
    "# Display the filtered rows\n",
    "print(f\"Number of rows with tokens in mesh terms but no tokens in abstract and title: {len(rows_with_tokens_in_mesh_only)}\")\n",
    "rows_with_tokens_in_mesh_only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the absence of tokens in each column\n",
    "no_tokens_abstract = df['disease_abstract_spacy'].apply(len) == 0\n",
    "no_tokens_title = df['disease_title_spacy'].apply(len) == 0\n",
    "no_tokens_mesh_terms = df['disease_mesh_terms_spacy'].apply(len) == 0\n",
    "\n",
    "# Filter rows: No tokens in all three columns\n",
    "rows_with_no_tokens_in_all = df[no_tokens_abstract & no_tokens_title & no_tokens_mesh_terms]\n",
    "\n",
    "# Display the filtered rows\n",
    "print(f\"Number of rows with no tokens in all three columns: {len(rows_with_no_tokens_in_all)}\")\n",
    "rows_with_no_tokens_in_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all columns to check for emptiness\n",
    "columns_to_check = [\n",
    "    'disease_abstract_spacy', \n",
    "    'disease_title_spacy', \n",
    "    'disease_mesh_terms_spacy', \n",
    "    'disease_title_tokens_simple', \n",
    "    'disease_title_tokens_hf', \n",
    "    'disease_abstract_tokens_simple', \n",
    "    'disease_abstract_tokens_hf'\n",
    "]\n",
    "\n",
    "# Check for emptiness in each column\n",
    "conditions = [df[col].apply(len) == 0 for col in columns_to_check]\n",
    "\n",
    "# Combine conditions: Check if all specified columns are empty for each row\n",
    "all_empty_condition = conditions[0]\n",
    "for condition in conditions[1:]:\n",
    "    all_empty_condition &= condition\n",
    "\n",
    "# Filter rows where all columns are empty\n",
    "rows_with_all_columns_empty = df[all_empty_condition]\n",
    "\n",
    "# Display the filtered rows\n",
    "print(f\"Number of rows with all specified columns empty: {len(rows_with_all_columns_empty)}\")\n",
    "rows_with_all_columns_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the original and new columns\n",
    "original_columns = ['disease_abstract_spacy', 'disease_title_spacy', 'disease_mesh_terms_spacy']\n",
    "new_columns = ['disease_title_tokens_simple', 'disease_title_tokens_hf', 'disease_abstract_tokens_simple', 'disease_abstract_tokens_hf']\n",
    "\n",
    "# Check for emptiness in the original columns\n",
    "original_columns_empty = [df[col].apply(len) == 0 for col in original_columns]\n",
    "\n",
    "# Check for non-emptiness in the new columns\n",
    "new_columns_not_empty = [df[col].apply(len) > 0 for col in new_columns]\n",
    "\n",
    "# Combine conditions: Original columns are empty, and new columns are not empty\n",
    "original_empty_condition = original_columns_empty[0]\n",
    "for condition in original_columns_empty[1:]:\n",
    "    original_empty_condition &= condition\n",
    "\n",
    "new_not_empty_condition = new_columns_not_empty[0]\n",
    "for condition in new_columns_not_empty[1:]:\n",
    "    new_not_empty_condition |= condition  # At least one of the new columns must not be empty\n",
    "\n",
    "# Filter rows where original columns are empty but new columns are not empty\n",
    "rows_with_new_columns_not_empty = df[original_empty_condition & new_not_empty_condition]\n",
    "\n",
    "# Display the filtered rows\n",
    "print(f\"Number of rows where original columns are empty but new columns are not: {len(rows_with_new_columns_not_empty)}\")\n",
    "rows_with_new_columns_not_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target UID values\n",
    "target_uids = [37843779]\n",
    "\n",
    "# Filter the DataFrame for rows with the target UID values\n",
    "filtered_rows_by_uid = df[df['uid'] == \"37843779\"]\n",
    "\n",
    "# Display the filtered rows\n",
    "print(f\"Rows with target UIDs {target_uids}:\")\n",
    "filtered_rows_by_uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Final Decision: Data Retention vs. Row Removal** `title` + `abstract` + `mesh terms`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "During the data processing pipeline, we faced a choice regarding whether to remove rows that lacked tokens related to illnesses. (there is still possibility to try and check these articles for any possible tokens there)\n",
    "\n",
    "The goal was to ensure high-quality data for further analysis and modelling while retaining as much relevant information as possible.\n",
    "\n",
    "**Choices Considered**:\n",
    "\n",
    "Title only: Retain rows with illness-related tokens in the title.\n",
    "*Result*: ~1,000,000 → 480,000 rows removed (~520,000 retained).\n",
    "\n",
    "- Abstract + Title: Retain rows with tokens in either title or abstract.\n",
    "*Result*: ~1,000,000 → 84,000 rows removed (~916,000 retained).\n",
    "\n",
    "- Abstract + Title + MeSH terms: Include tokens found in MeSH terms.\n",
    "*Result*: ~1,000,000 → 60,000rows removed (~940,000 retained).\n",
    "\n",
    "- Abstract + Title + MeSH + Hybrid Filtering: Use advanced filtering (e.g., hybrid matching and simpler keyword matching).\n",
    "*Result*: ~1,000,000 → 54,000 rows removed (~946,000 retained).\n",
    "\n",
    "- Retain All Rows: Do not remove rows based on token presence.\n",
    "\n",
    "**Final Decision**:\n",
    "We chose Option 5 to retain all rows. This decision was based on the reasoning that data lacking illness-related tokens is not inherently irrelevant and there may be possibility to check these inputs (articles) and see what's the problem with them. \n",
    "\n",
    "By keeping all rows we additionaly:\n",
    "\n",
    "- We maintain the integrity and diversity of the dataset.\n",
    "\n",
    "- Downstream analyses remain flexible, as token filtering can be performed at a later stage if necessary.\n",
    "\n",
    "- It enables broader exploration and modeling possibilities without prematurely discarding data.\n",
    "\n",
    "- By taking this approach, the dataset remains inclusive, ensuring no potential insights are lost. This aligns with our goal of providing a comprehensive resource for PubMed data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Processing - disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of variables we will do brief EDA for now, and eventually come back to them when we go into bibliometrics or other relevant analysis, network creation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: `journal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_counts = df[\"journal\"].value_counts(dropna=False)\n",
    "print(\"Top 20 journals:\")\n",
    "print(journal_counts.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "journal_counts = df[\"journal\"].value_counts().head(20)\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "bars = plt.bar(range(len(journal_counts)), journal_counts.values, color='steelblue')\n",
    "\n",
    "# X-ticks\n",
    "plt.xticks(range(len(journal_counts)), journal_counts.index, rotation=90, ha='right')\n",
    "plt.title(\"Top 20 Journals\")\n",
    "plt.xlabel(\"Journal Name\")\n",
    "plt.ylabel(\"Count of Articles\")\n",
    "\n",
    "# Label each bar with its count\n",
    "for i, rect in enumerate(bars):\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2, height,\n",
    "             f\"{int(height)}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: `authors`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the pipeline, we added a new variable to the dataset for intermediate analysis. However, we decided not to save this updated dataset with the new variable at this stage. \n",
    "\n",
    "If this variable is required in further processing steps, the dataset will be updated and saved accordingly in subsequent parts of the pipeline.\n",
    "\n",
    "This is just insight into making networks, overall they will be done later in the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"authors_list\"] = df[\"authors\"].fillna(\"\").str.split(\";\").apply(\n",
    "    lambda x: [a.strip() for a in x if a.strip()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "df[\"authors_list\"] = (\n",
    "    df[\"authors\"]\n",
    "    .fillna(\"\")\n",
    "    .str.split(\";\")\n",
    "    .apply(lambda x: [a.strip() for a in x if a.strip()])\n",
    ")\n",
    "\n",
    "# Now each row has a Python list of authors\n",
    "author_counter = Counter()\n",
    "\n",
    "for authors in df[\"authors_list\"]:\n",
    "    author_counter.update(authors)\n",
    "\n",
    "# Inspect top 20 authors\n",
    "for author, freq in author_counter.most_common(20):\n",
    "    print(author, freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Network early analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_netx = df[[\"uid\",\"authors\",\"authors_list\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Function to process the authors in the DataFrame\n",
    "def process_authors(df_authors):\n",
    "    for authors in tqdm(df_authors, desc=\"Processing Authors\"):\n",
    "        for pair in combinations(authors, 2):  # Create pairs of co-authors\n",
    "            if G.has_edge(pair[0], pair[1]):\n",
    "                G[pair[0]][pair[1]][\"weight\"] += 1\n",
    "            else:\n",
    "                G.add_edge(pair[0], pair[1], weight=1)\n",
    "\n",
    "# Ensure 'authors_list' is in the correct format\n",
    "df_netx[\"authors_list\"] = df_netx[\"authors_list\"].apply(\n",
    "    lambda x: eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "\n",
    "# Process the authors from the DataFrame\n",
    "print(\"Building the collaboration network...\")\n",
    "process_authors(df_netx[\"authors_list\"])\n",
    "\n",
    "print(\"\\nGraph construction completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the top 100 authors by degree\n",
    "print(\"\\nExtracting the top 50 authors by degree...\")\n",
    "top_authors = sorted(G.degree, key=lambda x: x[1], reverse=True)[:50]\n",
    "\n",
    "# Create a subgraph with the top 100 authors\n",
    "top_nodes = [node for node, _ in top_authors]\n",
    "subgraph = G.subgraph(top_nodes)\n",
    "\n",
    "# Display statistics for the subgraph\n",
    "print(f\"\\nTop 100 Subgraph Statistics:\")\n",
    "print(f\"Total Nodes: {subgraph.number_of_nodes()}\")\n",
    "print(f\"Total Edges: {subgraph.number_of_edges()}\")\n",
    "\n",
    "# Save the subgraph to a GEXF file for visualization\n",
    "nx.write_gexf(subgraph, \"Data/2.Processed/EDA_AnalysisData/top_50_author_network.gexf\")\n",
    "print(\"\\nSubgraph saved to 'top_50_author_network.gexf'.\")\n",
    "\n",
    "# Optional: Print the top 10 authors and their degrees\n",
    "print(\"\\nTop 10 Authors by Degree:\")\n",
    "for author, degree in top_authors[:10]:\n",
    "    print(f\"{author}: {degree} connections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "# Customize node sizes based on degree\n",
    "node_sizes = [subgraph.degree[node] * 20 for node in subgraph.nodes]  # Scale size\n",
    "node_colors = range(len(subgraph.nodes))  # Gradient color mapping\n",
    "\n",
    "# Customize edge widths based on weight\n",
    "edge_widths = [subgraph[u][v]['weight'] for u, v in subgraph.edges]\n",
    "\n",
    "# Draw graph with spring layout\n",
    "plt.figure(figsize=(14, 14))\n",
    "pos = nx.spring_layout(subgraph, seed=42)  # Seed ensures consistent layout\n",
    "\n",
    "nx.draw_networkx_nodes(\n",
    "    subgraph,\n",
    "    pos,\n",
    "    node_size=node_sizes,\n",
    "    node_color=node_colors,\n",
    "    cmap=plt.cm.viridis,  # Color map\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "nx.draw_networkx_edges(\n",
    "    subgraph,\n",
    "    pos,\n",
    "    width=[w / 10 for w in edge_widths],  # Scale down edge weights\n",
    "    alpha=0.5,\n",
    "    edge_color=\"gray\"\n",
    ")\n",
    "\n",
    "# Add labels for the top 10 authors\n",
    "top_labels = {node: node for node, degree in sorted(subgraph.degree, key=lambda x: x[1], reverse=True)[:10]}\n",
    "nx.draw_networkx_labels(\n",
    "    subgraph, pos, labels=top_labels, font_size=10, font_color=\"darkred\"\n",
    ")\n",
    "\n",
    "plt.title(\"Top 50 Author Collaboration Network (Spring Layout)\", fontsize=18)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize node sizes and colors\n",
    "node_sizes = [subgraph.degree[node] * 20 for node in subgraph.nodes]\n",
    "node_colors = range(len(subgraph.nodes))\n",
    "\n",
    "# Draw graph with circular layout\n",
    "plt.figure(figsize=(14, 14))\n",
    "pos = nx.circular_layout(subgraph)  # Circular layout for visualization\n",
    "\n",
    "nx.draw_networkx_nodes(\n",
    "    subgraph,\n",
    "    pos,\n",
    "    node_size=node_sizes,\n",
    "    node_color=node_colors,\n",
    "    cmap=plt.cm.coolwarm,  # Color map\n",
    "    alpha=0.8\n",
    ")\n",
    "\n",
    "nx.draw_networkx_edges(\n",
    "    subgraph,\n",
    "    pos,\n",
    "    width=[w / 10 for w in edge_widths],  # Scale down edge weights\n",
    "    alpha=0.5,\n",
    "    edge_color=\"gray\"\n",
    ")\n",
    "\n",
    "# Add labels for the top 10 authors\n",
    "nx.draw_networkx_labels(\n",
    "    subgraph, pos, labels=top_labels, font_size=10, font_color=\"blue\"\n",
    ")\n",
    "\n",
    "plt.title(\"Top 50 Author Collaboration Network (Circular Layout)\", fontsize=18)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top nodes and edges\n",
    "top_nodes = sorted(subgraph.degree, key=lambda x: x[1], reverse=True)[:10]\n",
    "top_edges = sorted(subgraph.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)[:20]\n",
    "\n",
    "# Extract top nodes and edges for highlighting\n",
    "highlighted_nodes = [node for node, _ in top_nodes]\n",
    "highlighted_edges = [(u, v) for u, v, _ in top_edges]\n",
    "\n",
    "# Highlighted graph\n",
    "plt.figure(figsize=(14, 14))\n",
    "pos = nx.spring_layout(subgraph, seed=42)\n",
    "\n",
    "# Regular nodes and edges\n",
    "nx.draw_networkx_nodes(\n",
    "    subgraph,\n",
    "    pos,\n",
    "    node_size=node_sizes,\n",
    "    node_color=\"lightgray\",\n",
    "    alpha=0.6\n",
    ")\n",
    "nx.draw_networkx_edges(\n",
    "    subgraph,\n",
    "    pos,\n",
    "    width=0.5,\n",
    "    alpha=0.3,\n",
    "    edge_color=\"lightgray\"\n",
    ")\n",
    "\n",
    "# Highlighted nodes and edges\n",
    "nx.draw_networkx_nodes(\n",
    "    subgraph,\n",
    "    pos,\n",
    "    nodelist=highlighted_nodes,\n",
    "    node_size=300,\n",
    "    node_color=\"red\"\n",
    ")\n",
    "nx.draw_networkx_edges(\n",
    "    subgraph,\n",
    "    pos,\n",
    "    edgelist=highlighted_edges,\n",
    "    width=2,\n",
    "    alpha=0.7,\n",
    "    edge_color=\"red\"\n",
    ")\n",
    "\n",
    "# Add labels for top nodes\n",
    "highlighted_labels = {node: node for node in highlighted_nodes}\n",
    "nx.draw_networkx_labels(\n",
    "    subgraph, pos, labels=highlighted_labels, font_size=12, font_color=\"darkred\"\n",
    ")\n",
    "\n",
    "plt.title(\"Highlighted Key Authors and Collaborations\", fontsize=18)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame for Counter-based rankings\n",
    "counter_ranking = pd.DataFrame(author_counter.most_common(), columns=[\"Author\", \"Frequency\"])\n",
    "counter_ranking[\"Counter Rank\"] = counter_ranking.index + 1\n",
    "\n",
    "# Create a DataFrame for Graph-based rankings\n",
    "graph_ranking = pd.DataFrame(G.degree, columns=[\"Author\", \"Degree\"])\n",
    "graph_ranking[\"Graph Rank\"] = graph_ranking[\"Degree\"].rank(ascending=False).astype(int)\n",
    "\n",
    "# Merge the two rankings\n",
    "ranking_comparison = pd.merge(counter_ranking, graph_ranking, on=\"Author\", how=\"inner\")\n",
    "\n",
    "# Display top authors with both rankings\n",
    "print(ranking_comparison.head(20))\n",
    "\n",
    "# Check correlation\n",
    "correlation = ranking_comparison[\"Frequency\"].corr(ranking_comparison[\"Degree\"])\n",
    "print(f\"Correlation between Counter and Graph rankings: {correlation:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the subgraph created for top 100 authors\n",
    "G = subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate unique and shared connections\n",
    "unique_connections = {}\n",
    "shared_connections = {}\n",
    "\n",
    "for node in G.nodes:\n",
    "    neighbors = set(G.neighbors(node))\n",
    "    unique_connections[node] = len(neighbors)\n",
    "    shared_connections[node] = sum(1 for n in neighbors if len(set(G.neighbors(n)) & neighbors) > 1)\n",
    "\n",
    "# Combine into a DataFrame\n",
    "connection_stats = pd.DataFrame.from_dict(unique_connections, orient=\"index\", columns=[\"Unique Connections\"])\n",
    "connection_stats[\"Shared Connections\"] = connection_stats.index.map(shared_connections)\n",
    "\n",
    "# Display top authors by unique and shared connections\n",
    "print(connection_stats.sort_values(\"Unique Connections\", ascending=False).head(10))\n",
    "print(connection_stats.sort_values(\"Shared Connections\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collaborator_ranks = []\n",
    "\n",
    "for node, degree in top_authors:\n",
    "    for neighbor in G.neighbors(node):\n",
    "        neighbor_degree = G.degree(neighbor)\n",
    "        collaborator_ranks.append({\"Author\": node, \"Author Degree\": degree, \"Neighbor\": neighbor, \"Neighbor Degree\": neighbor_degree})\n",
    "\n",
    "collaborator_ranks_df = pd.DataFrame(collaborator_ranks)\n",
    "\n",
    "# Example: Check if high-ranked authors collaborate with similarly ranked ones\n",
    "high_ranked_collaborators = collaborator_ranks_df[collaborator_ranks_df[\"Author Degree\"] > 5000]\n",
    "print(high_ranked_collaborators.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering coefficient\n",
    "clustering = nx.clustering(G)\n",
    "clustering_df = pd.DataFrame.from_dict(clustering, orient=\"index\", columns=[\"Clustering Coefficient\"])\n",
    "\n",
    "# Betweenness centrality\n",
    "betweenness = nx.betweenness_centrality(G)\n",
    "betweenness_df = pd.DataFrame.from_dict(betweenness, orient=\"index\", columns=[\"Betweenness Centrality\"])\n",
    "\n",
    "# Merge metrics\n",
    "author_metrics = pd.concat([clustering_df, betweenness_df], axis=1)\n",
    "\n",
    "# Display top authors by centrality\n",
    "print(author_metrics.sort_values(\"Betweenness Centrality\", ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: `affiliations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_counter = df[\"affiliations\"].fillna(\"\").value_counts()\n",
    "print(\"Top 20 affiliations:\")\n",
    "aff_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_freq = df[\"affiliations\"].value_counts(dropna=False)\n",
    "print(\"Number of unique affiliations:\", len(aff_freq))\n",
    "print(\"Top 20 affiliation strings:\")\n",
    "print(aff_freq.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: `mesh_terms` continuation, some additional insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"mesh_list\"] = df[\"mesh_terms\"].fillna(\"\").str.split(\";\").apply(\n",
    "#     lambda x: [m.strip() for m in x if m.strip()]\n",
    "# )\n",
    "df[\"mesh_list\"] = (\n",
    "    df[\"mesh_terms\"]\n",
    "    .fillna(\"\")  # replace NaN with empty string\n",
    "    .str.split(\";\")\n",
    "    .apply(lambda x: [m.strip() for m in x if m.strip()])  # strip spaces, remove empties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "mesh_counter = Counter()\n",
    "for mesh_terms in df[\"mesh_list\"]:\n",
    "    # mesh_terms is a list, e.g. [\"Adolescent\", \"Adult\", ...]\n",
    "    mesh_counter.update(mesh_terms)\n",
    "\n",
    "# Turn counter into a DataFrame sorted by frequency\n",
    "mesh_freq_df = pd.DataFrame(mesh_counter.most_common(), columns=[\"mesh_term\", \"count\"])\n",
    "mesh_freq_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the list so each row in df_exploded is one MeSH term\n",
    "df_exploded = df.explode(\"mesh_list\")\n",
    "# Then we can do a value_counts on the single item\n",
    "mesh_freq = df_exploded[\"mesh_list\"].value_counts(dropna=False)\n",
    "print(\"Number of unique MeSH terms:\", len(mesh_freq))\n",
    "print(\"Top 20 MeSH terms:\")\n",
    "print(mesh_freq.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mesh term will be explored in next file that involve analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: `keywords`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keywords will be will be explored in next file that involve analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable: `coi_statement`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is too much NA for us to process it meaningfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINALIZING DATASET SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h:\\\\000_Projects\\\\01_GitHub\\\\05_PythonProjects\\\\PubMedResearch\\\\Notebooks'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   9%|▉         | 100000/1057871 [01:07<10:49, 1474.29rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 1: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  19%|█▉        | 200000/1057871 [01:14<04:30, 3166.17rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 2: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  28%|██▊       | 300000/1057871 [01:20<02:32, 4966.46rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 3: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  38%|███▊      | 400000/1057871 [01:25<01:32, 7097.79rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 4: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  47%|████▋     | 500000/1057871 [01:30<01:01, 9118.62rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 5: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  57%|█████▋    | 600000/1057871 [01:35<00:40, 11373.10rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 6: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  66%|██████▌   | 700000/1057871 [01:39<00:26, 13581.42rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 7: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  76%|███████▌  | 800000/1057871 [01:44<00:16, 15602.29rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 8: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  85%|████████▌ | 900000/1057871 [01:48<00:09, 17086.30rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 9: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:  95%|█████████▍| 1000000/1057871 [01:53<00:03, 18317.15rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 10: 100000 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 1057871/1057871 [01:56<00:00, 9098.94rows/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Chunk 11: 57871 rows\n",
      "\n",
      "Final DataFrame with 1057871 rows:\n",
      "        uid                                              title  \\\n",
      "0  10186596  The potential impact of health care reform on ...   \n",
      "1  10186588  New Jersey health promotion and disease preven...   \n",
      "2  10186587  Who will provide preventive services? The chan...   \n",
      "3  10163501  Cytoreduction of small intestine metastases us...   \n",
      "4  10157383  Racial differences in access to kidney transpl...   \n",
      "\n",
      "                                             journal  \\\n",
      "0  Journal of public health management and practi...   \n",
      "1  Journal of public health management and practi...   \n",
      "2  Journal of public health management and practi...   \n",
      "3                     Journal of gynecologic surgery   \n",
      "4                       Health care financing review   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  General: This article observes that, despite t...   \n",
      "1  General: Health promotion is a major component...   \n",
      "2  General: Health care reform in the United Stat...   \n",
      "3  General: The Cavitron Ultrasonic Surgical Aspi...   \n",
      "4  General: Previous work has documented large di...   \n",
      "\n",
      "                             authors  \\\n",
      "0              Auerbach J; McGuire J   \n",
      "1                         Louria D B   \n",
      "2  Pearson T A; Spencer M; Jenkins P   \n",
      "3                        Adelson M D   \n",
      "4                         Eggers P W   \n",
      "\n",
      "                                        affiliations  \\\n",
      "0  HIV/AIDS Bureau, Massachusetts Department of P...   \n",
      "1  Department of Preventive Medicine and Communit...   \n",
      "2  Mary Imogene Bassett Research Institute, Coope...   \n",
      "3  Department of Obstetrics and Gynecology, Crous...   \n",
      "4  Office of Research and Demonstrations, Health ...   \n",
      "\n",
      "                                          mesh_terms  \\\n",
      "0  Financing, Government; HIV Infections; Health ...   \n",
      "1  Female; Health Education; Health Promotion; Hu...   \n",
      "2  Delivery of Health Care; Female; Health Care R...   \n",
      "3  Adenocarcinoma; Fallopian Tube Neoplasms; Fema...   \n",
      "4  Adolescent; Adult; Black or African American; ...   \n",
      "\n",
      "                                            keywords coi_statement  \\\n",
      "0                                                              N/A   \n",
      "1                                                              N/A   \n",
      "2                                                              N/A   \n",
      "3                                                              N/A   \n",
      "4  Empirical Approach; End Stage Renal Disease Pr...           N/A   \n",
      "\n",
      "  parsed_date  ...                            cleaned_title_tokens_hf  \\\n",
      "0  1995-01-01  ...  [[CLS], potential, impact, health, care, refor...   \n",
      "1  1995-01-01  ...  [[CLS], new, jersey, health, promotion, diseas...   \n",
      "2  1995-01-01  ...  [[CLS], provide, prevent, ##ive, services, ?, ...   \n",
      "3  1995-01-01  ...  [[CLS], cy, ##tore, ##duction, small, int, ##e...   \n",
      "4  1995-01-01  ...  [[CLS], racial, differences, access, kidney, t...   \n",
      "\n",
      "                      cleaned_abstract_tokens_simple  \\\n",
      "0  [general, article, observes, despite, clear, p...   \n",
      "1  [general, health, promotion, major, component,...   \n",
      "2  [general, health, care, reform, united, states...   \n",
      "3  [general, cavitron, ultrasonic, surgical, aspi...   \n",
      "4  [general, previous, work, documented, large, d...   \n",
      "\n",
      "                          cleaned_abstract_tokens_hf  \\\n",
      "0  [[CLS], general, article, observes, despite, c...   \n",
      "1  [[CLS], general, health, promotion, major, com...   \n",
      "2  [[CLS], general, health, care, reform, united,...   \n",
      "3  [[CLS], general, ca, ##vi, ##tron, ultra, ##so...   \n",
      "4  [[CLS], general, previous, work, documented, l...   \n",
      "\n",
      "  disease_title_tokens_simple disease_title_tokens_hf  \\\n",
      "0                       [hiv]                   [hiv]   \n",
      "1                          []                      []   \n",
      "2                          []                      []   \n",
      "3                          []                      []   \n",
      "4                          []                      []   \n",
      "\n",
      "  disease_abstract_tokens_simple disease_abstract_tokens_hf  \\\n",
      "0                    [hiv, aids]                [hiv, aids]   \n",
      "1                             []                         []   \n",
      "2                             []                         []   \n",
      "3                        [tumor]                    [tumor]   \n",
      "4                             []                         []   \n",
      "\n",
      "                              disease_abstract_spacy disease_title_spacy  \\\n",
      "0  [human immunodeficiency virus (HIV) disease, a...                  []   \n",
      "1                                                 []                  []   \n",
      "2                                                 []                  []   \n",
      "3  [carcinoma of the ovary, and one each had, tub...                  []   \n",
      "4  [renal failure, renal failure, end stage renal...                  []   \n",
      "\n",
      "                         disease_mesh_terms_spacy  \n",
      "0                                [HIV Infections]  \n",
      "1                                              []  \n",
      "2                                              []  \n",
      "3  [Adenocarcinoma, Neoplasms, Ovarian Neoplasms]  \n",
      "4                       [American Kidney Failure]  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the base path relative to the current file\n",
    "    base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Move one directory up\n",
    "    \n",
    "    # Construct the file path\n",
    "    file_path = os.path.join(base_path, \"Data\", \"2.Processed\", \"ModellingData\", \"P5_final_new.parquet\")\n",
    "    \n",
    "    batch_size = 100_000  # Define your desired chunk size\n",
    "    \n",
    "    # Read the file in batches\n",
    "    df = read_parquet_in_batches_with_progress(file_path, batch_size)\n",
    "    \n",
    "    print(f\"\\nFinal DataFrame with {len(df)} rows:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting DataFrame of 1057871 rows into 11 batches (size=100000).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:   9%|▉         | 1/11 [00:10<01:42, 10.28s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 1 rows [0:100000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_1.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  18%|█▊        | 2/11 [00:19<01:28,  9.84s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 2 rows [100000:200000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_2.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  27%|██▋       | 3/11 [00:29<01:18,  9.86s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 3 rows [200000:300000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_3.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  36%|███▋      | 4/11 [00:39<01:09,  9.93s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 4 rows [300000:400000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_4.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  45%|████▌     | 5/11 [00:49<01:00, 10.01s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 5 rows [400000:500000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_5.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  55%|█████▍    | 6/11 [01:00<00:50, 10.08s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 6 rows [500000:600000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_6.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  64%|██████▎   | 7/11 [01:11<00:41, 10.42s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 7 rows [600000:700000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_7.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  73%|███████▎  | 8/11 [01:22<00:32, 10.84s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 8 rows [700000:800000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_8.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  82%|████████▏ | 9/11 [01:35<00:22, 11.22s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 9 rows [800000:900000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_9.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  91%|█████████ | 10/11 [01:47<00:11, 11.56s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 10 rows [900000:1000000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_10.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches: 100%|██████████| 11/11 [01:54<00:00, 10.40s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 11 rows [1000000:1057871] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_11.parquet\n",
      "\n",
      "Merging 11 batch files into h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\P6_merged_tokens.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging Batches: 100%|██████████| 11/11 [01:44<00:00,  9.48s/file]\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     # Define the base path relative to the current file\n",
    "#     base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Move one directory up\n",
    "\n",
    "#     # Construct the file path\n",
    "#     file_path = os.path.join(base_path, \"Data\", \"2.Processed\", \"ModellingData\", \"P5_final_new.parquet\")\n",
    "\n",
    "#     # Define the output folder and final file\n",
    "#     output_folder = os.path.join(base_path, \"Data\", \"2.Processed\", \"ModellingData\")\n",
    "#     final_file = \"P6_merged_tokens.parquet\"\n",
    "\n",
    "#     batch_size = 100_000  # Define your desired chunk size\n",
    "    \n",
    "#     # Save and merge in batches\n",
    "#     result_path = save_and_merge_in_batches(\n",
    "#         df=df,\n",
    "#         batch_size=batch_size,\n",
    "#         output_folder=output_folder,\n",
    "#         final_filename=final_file,\n",
    "#         temp_batch_prefix=\"temp_batch_\"\n",
    "#     )\n",
    "\n",
    "#     print(f\"All done. Merged file at: {result_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>keywords</th>\n",
       "      <th>coi_statement</th>\n",
       "      <th>parsed_date</th>\n",
       "      <th>...</th>\n",
       "      <th>cleaned_title_tokens_hf</th>\n",
       "      <th>cleaned_abstract_tokens_simple</th>\n",
       "      <th>cleaned_abstract_tokens_hf</th>\n",
       "      <th>disease_title_tokens_simple</th>\n",
       "      <th>disease_title_tokens_hf</th>\n",
       "      <th>disease_abstract_tokens_simple</th>\n",
       "      <th>disease_abstract_tokens_hf</th>\n",
       "      <th>disease_abstract_spacy</th>\n",
       "      <th>disease_title_spacy</th>\n",
       "      <th>disease_mesh_terms_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10186596</td>\n",
       "      <td>The potential impact of health care reform on ...</td>\n",
       "      <td>Journal of public health management and practi...</td>\n",
       "      <td>General: This article observes that, despite t...</td>\n",
       "      <td>Auerbach J; McGuire J</td>\n",
       "      <td>HIV/AIDS Bureau, Massachusetts Department of P...</td>\n",
       "      <td>Financing, Government; HIV Infections; Health ...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[[CLS], potential, impact, health, care, refor...</td>\n",
       "      <td>[general, article, observes, despite, clear, p...</td>\n",
       "      <td>[[CLS], general, article, observes, despite, c...</td>\n",
       "      <td>[hiv]</td>\n",
       "      <td>[hiv]</td>\n",
       "      <td>[hiv, aids]</td>\n",
       "      <td>[hiv, aids]</td>\n",
       "      <td>[human immunodeficiency virus (HIV) disease, a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[HIV Infections]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10186588</td>\n",
       "      <td>New Jersey health promotion and disease preven...</td>\n",
       "      <td>Journal of public health management and practi...</td>\n",
       "      <td>General: Health promotion is a major component...</td>\n",
       "      <td>Louria D B</td>\n",
       "      <td>Department of Preventive Medicine and Communit...</td>\n",
       "      <td>Female; Health Education; Health Promotion; Hu...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[[CLS], new, jersey, health, promotion, diseas...</td>\n",
       "      <td>[general, health, promotion, major, component,...</td>\n",
       "      <td>[[CLS], general, health, promotion, major, com...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10186587</td>\n",
       "      <td>Who will provide preventive services? The chan...</td>\n",
       "      <td>Journal of public health management and practi...</td>\n",
       "      <td>General: Health care reform in the United Stat...</td>\n",
       "      <td>Pearson T A; Spencer M; Jenkins P</td>\n",
       "      <td>Mary Imogene Bassett Research Institute, Coope...</td>\n",
       "      <td>Delivery of Health Care; Female; Health Care R...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[[CLS], provide, prevent, ##ive, services, ?, ...</td>\n",
       "      <td>[general, health, care, reform, united, states...</td>\n",
       "      <td>[[CLS], general, health, care, reform, united,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10163501</td>\n",
       "      <td>Cytoreduction of small intestine metastases us...</td>\n",
       "      <td>Journal of gynecologic surgery</td>\n",
       "      <td>General: The Cavitron Ultrasonic Surgical Aspi...</td>\n",
       "      <td>Adelson M D</td>\n",
       "      <td>Department of Obstetrics and Gynecology, Crous...</td>\n",
       "      <td>Adenocarcinoma; Fallopian Tube Neoplasms; Fema...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[[CLS], cy, ##tore, ##duction, small, int, ##e...</td>\n",
       "      <td>[general, cavitron, ultrasonic, surgical, aspi...</td>\n",
       "      <td>[[CLS], general, ca, ##vi, ##tron, ultra, ##so...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tumor]</td>\n",
       "      <td>[tumor]</td>\n",
       "      <td>[carcinoma of the ovary, and one each had, tub...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Adenocarcinoma, Neoplasms, Ovarian Neoplasms]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10157383</td>\n",
       "      <td>Racial differences in access to kidney transpl...</td>\n",
       "      <td>Health care financing review</td>\n",
       "      <td>General: Previous work has documented large di...</td>\n",
       "      <td>Eggers P W</td>\n",
       "      <td>Office of Research and Demonstrations, Health ...</td>\n",
       "      <td>Adolescent; Adult; Black or African American; ...</td>\n",
       "      <td>Empirical Approach; End Stage Renal Disease Pr...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[[CLS], racial, differences, access, kidney, t...</td>\n",
       "      <td>[general, previous, work, documented, large, d...</td>\n",
       "      <td>[[CLS], general, previous, work, documented, l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[renal failure, renal failure, end stage renal...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[American Kidney Failure]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid                                              title  \\\n",
       "0  10186596  The potential impact of health care reform on ...   \n",
       "1  10186588  New Jersey health promotion and disease preven...   \n",
       "2  10186587  Who will provide preventive services? The chan...   \n",
       "3  10163501  Cytoreduction of small intestine metastases us...   \n",
       "4  10157383  Racial differences in access to kidney transpl...   \n",
       "\n",
       "                                             journal  \\\n",
       "0  Journal of public health management and practi...   \n",
       "1  Journal of public health management and practi...   \n",
       "2  Journal of public health management and practi...   \n",
       "3                     Journal of gynecologic surgery   \n",
       "4                       Health care financing review   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  General: This article observes that, despite t...   \n",
       "1  General: Health promotion is a major component...   \n",
       "2  General: Health care reform in the United Stat...   \n",
       "3  General: The Cavitron Ultrasonic Surgical Aspi...   \n",
       "4  General: Previous work has documented large di...   \n",
       "\n",
       "                             authors  \\\n",
       "0              Auerbach J; McGuire J   \n",
       "1                         Louria D B   \n",
       "2  Pearson T A; Spencer M; Jenkins P   \n",
       "3                        Adelson M D   \n",
       "4                         Eggers P W   \n",
       "\n",
       "                                        affiliations  \\\n",
       "0  HIV/AIDS Bureau, Massachusetts Department of P...   \n",
       "1  Department of Preventive Medicine and Communit...   \n",
       "2  Mary Imogene Bassett Research Institute, Coope...   \n",
       "3  Department of Obstetrics and Gynecology, Crous...   \n",
       "4  Office of Research and Demonstrations, Health ...   \n",
       "\n",
       "                                          mesh_terms  \\\n",
       "0  Financing, Government; HIV Infections; Health ...   \n",
       "1  Female; Health Education; Health Promotion; Hu...   \n",
       "2  Delivery of Health Care; Female; Health Care R...   \n",
       "3  Adenocarcinoma; Fallopian Tube Neoplasms; Fema...   \n",
       "4  Adolescent; Adult; Black or African American; ...   \n",
       "\n",
       "                                            keywords coi_statement  \\\n",
       "0                                                              N/A   \n",
       "1                                                              N/A   \n",
       "2                                                              N/A   \n",
       "3                                                              N/A   \n",
       "4  Empirical Approach; End Stage Renal Disease Pr...           N/A   \n",
       "\n",
       "  parsed_date  ...                            cleaned_title_tokens_hf  \\\n",
       "0  1995-01-01  ...  [[CLS], potential, impact, health, care, refor...   \n",
       "1  1995-01-01  ...  [[CLS], new, jersey, health, promotion, diseas...   \n",
       "2  1995-01-01  ...  [[CLS], provide, prevent, ##ive, services, ?, ...   \n",
       "3  1995-01-01  ...  [[CLS], cy, ##tore, ##duction, small, int, ##e...   \n",
       "4  1995-01-01  ...  [[CLS], racial, differences, access, kidney, t...   \n",
       "\n",
       "                      cleaned_abstract_tokens_simple  \\\n",
       "0  [general, article, observes, despite, clear, p...   \n",
       "1  [general, health, promotion, major, component,...   \n",
       "2  [general, health, care, reform, united, states...   \n",
       "3  [general, cavitron, ultrasonic, surgical, aspi...   \n",
       "4  [general, previous, work, documented, large, d...   \n",
       "\n",
       "                          cleaned_abstract_tokens_hf  \\\n",
       "0  [[CLS], general, article, observes, despite, c...   \n",
       "1  [[CLS], general, health, promotion, major, com...   \n",
       "2  [[CLS], general, health, care, reform, united,...   \n",
       "3  [[CLS], general, ca, ##vi, ##tron, ultra, ##so...   \n",
       "4  [[CLS], general, previous, work, documented, l...   \n",
       "\n",
       "  disease_title_tokens_simple disease_title_tokens_hf  \\\n",
       "0                       [hiv]                   [hiv]   \n",
       "1                          []                      []   \n",
       "2                          []                      []   \n",
       "3                          []                      []   \n",
       "4                          []                      []   \n",
       "\n",
       "  disease_abstract_tokens_simple disease_abstract_tokens_hf  \\\n",
       "0                    [hiv, aids]                [hiv, aids]   \n",
       "1                             []                         []   \n",
       "2                             []                         []   \n",
       "3                        [tumor]                    [tumor]   \n",
       "4                             []                         []   \n",
       "\n",
       "                              disease_abstract_spacy disease_title_spacy  \\\n",
       "0  [human immunodeficiency virus (HIV) disease, a...                  []   \n",
       "1                                                 []                  []   \n",
       "2                                                 []                  []   \n",
       "3  [carcinoma of the ovary, and one each had, tub...                  []   \n",
       "4  [renal failure, renal failure, end stage renal...                  []   \n",
       "\n",
       "                         disease_mesh_terms_spacy  \n",
       "0                                [HIV Infections]  \n",
       "1                                              []  \n",
       "2                                              []  \n",
       "3  [Adenocarcinoma, Neoplasms, Ovarian Neoplasms]  \n",
       "4                       [American Kidney Failure]  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uid                                       object\n",
       "title                                     object\n",
       "journal                                   object\n",
       "abstract                                  object\n",
       "authors                                   object\n",
       "affiliations                              object\n",
       "mesh_terms                                object\n",
       "keywords                                  object\n",
       "coi_statement                             object\n",
       "parsed_date                       datetime64[ns]\n",
       "cleaned_title_tokens_simple               object\n",
       "cleaned_title_tokens_hf                   object\n",
       "cleaned_abstract_tokens_simple            object\n",
       "cleaned_abstract_tokens_hf                object\n",
       "disease_title_tokens_simple               object\n",
       "disease_title_tokens_hf                   object\n",
       "disease_abstract_tokens_simple            object\n",
       "disease_abstract_tokens_hf                object\n",
       "disease_abstract_spacy                    object\n",
       "disease_title_spacy                       object\n",
       "disease_mesh_terms_spacy                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing rows for each column (sorted by missing count):\n",
      "disease_title_tokens_hf: 854118 missing rows\n",
      "disease_title_tokens_simple: 796808 missing rows\n",
      "disease_abstract_tokens_hf: 643789 missing rows\n",
      "disease_abstract_tokens_simple: 548913 missing rows\n",
      "disease_title_spacy: 403970 missing rows\n",
      "disease_mesh_terms_spacy: 299410 missing rows\n",
      "disease_abstract_spacy: 87103 missing rows\n",
      "Included disease_title_tokens_hf: 854118 rows still missing\n",
      "Included disease_title_tokens_simple: 793403 rows still missing\n",
      "Included disease_abstract_tokens_hf: 590192 rows still missing\n",
      "Included disease_abstract_tokens_simple: 537052 rows still missing\n",
      "Included disease_title_spacy: 251758 rows still missing\n",
      "Included disease_mesh_terms_spacy: 145753 rows still missing\n",
      "Included disease_abstract_spacy: 54819 rows still missing\n",
      "\n",
      "Final results after merging columns:\n",
      "                 Included Columns  Missing Rows\n",
      "0         disease_title_tokens_hf        854118\n",
      "1     disease_title_tokens_simple        793403\n",
      "2      disease_abstract_tokens_hf        590192\n",
      "3  disease_abstract_tokens_simple        537052\n",
      "4             disease_title_spacy        251758\n",
      "5        disease_mesh_terms_spacy        145753\n",
      "6          disease_abstract_spacy         54819\n",
      "\n",
      "Inspecting rows still missing after the final iteration...\n",
      "             uid                                              title  \\\n",
      "131072  11099584  Providing pediatric subspecialty care: A workf...   \n",
      "1       10186588  New Jersey health promotion and disease preven...   \n",
      "2       10186587  Who will provide preventive services? The chan...   \n",
      "262147  15722370  Impact of misclassification of in vitro fertil...   \n",
      "917506  34794827  Physiologic flow-conditioning limits vascular ...   \n",
      "\n",
      "                                                 abstract  \\\n",
      "131072  OBJECTIVE: To provide a snapshot of pediatric ...   \n",
      "1       General: Health promotion is a major component...   \n",
      "2       General: Health care reform in the United Stat...   \n",
      "262147  OBJECTIVE: To determine whether failure to ade...   \n",
      "917506  General: Hemodynamics play a central role in t...   \n",
      "\n",
      "                                               mesh_terms  \n",
      "131072  Adolescent; Adult; Aged; Cardiology; Child; Cr...  \n",
      "1       Female; Health Education; Health Promotion; Hu...  \n",
      "2       Delivery of Health Care; Female; Health Care R...  \n",
      "262147  Data Collection; Dietary Supplements; Female; ...  \n",
      "917506  Animals; Capillaries; Endothelial Cells; Hemod...  \n",
      "Total missing rows after all columns included: 54819\n"
     ]
    }
   ],
   "source": [
    "token_columns = [\n",
    "        \"disease_title_tokens_simple\",\n",
    "        \"disease_title_tokens_hf\",\n",
    "        \"disease_abstract_tokens_simple\",\n",
    "        \"disease_abstract_tokens_hf\",\n",
    "        \"disease_title_spacy\",\n",
    "        \"disease_mesh_terms_spacy\",\n",
    "        \"disease_abstract_spacy\",\n",
    "    ]\n",
    "\n",
    "# Step 1: Identify missing rows for each column\n",
    "def count_missing_rows(column):\n",
    "    def is_missing(value):\n",
    "        # Handle lists\n",
    "        if isinstance(value, list):\n",
    "            return len(value) == 0  # Empty list\n",
    "        # Handle numpy arrays\n",
    "        if hasattr(value, \"shape\") and len(value.shape) > 0:\n",
    "            return value.size == 0  # Empty array\n",
    "        # Handle None or empty scalar values\n",
    "        return value is None or pd.isna(value)\n",
    "    return df[column].apply(is_missing).sum()\n",
    "\n",
    "missing_counts = {col: count_missing_rows(col) for col in token_columns}\n",
    "sorted_columns = sorted(missing_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Missing rows for each column (sorted by missing count):\")\n",
    "for col, count in sorted_columns:\n",
    "    print(f\"{col}: {count} missing rows\")\n",
    "\n",
    "# Step 2: Iteratively combine columns and calculate remaining missing rows\n",
    "cumulative_set = set()  # Tracks rows that are NOT missing across combined columns\n",
    "results = []  # Stores results for each iteration\n",
    "final_missing_indices = set(df.index)  # To track rows still missing after final iteration\n",
    "\n",
    "for col, _ in sorted_columns:\n",
    "    # Get non-missing rows for the current column\n",
    "    def not_missing(value):\n",
    "        if isinstance(value, list):\n",
    "            return len(value) > 0  # Non-empty list\n",
    "        if hasattr(value, \"shape\") and len(value.shape) > 0:\n",
    "            return value.size > 0  # Non-empty array\n",
    "        return value is not None and not pd.isna(value)\n",
    "    current_set = set(df.index[df[col].apply(not_missing)])\n",
    "    cumulative_set |= current_set  # Union with cumulative non-missing rows\n",
    "    final_missing_indices -= current_set  # Remove these rows from the missing set\n",
    "    missing_rows_after_merge = len(df) - len(cumulative_set)  # Remaining missing rows\n",
    "    results.append({\"Included Columns\": col, \"Missing Rows\": missing_rows_after_merge})\n",
    "    print(f\"Included {col}: {missing_rows_after_merge} rows still missing\")\n",
    "\n",
    "# Step 3: Output results as a DataFrame for inspection\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nFinal results after merging columns:\")\n",
    "print(results_df)\n",
    "\n",
    "# Step 4: Get insights into the abstracts or rows still missing after the final iteration\n",
    "print(\"\\nInspecting rows still missing after the final iteration...\")\n",
    "\n",
    "# Convert the set of indices to a list\n",
    "missing_indices_list = list(final_missing_indices)\n",
    "\n",
    "# Extract the rows using the corrected list\n",
    "missing_df = df.loc[missing_indices_list, [\"uid\", \"title\", \"abstract\", \"mesh_terms\"]]\n",
    "\n",
    "# Display a few examples of missing rows\n",
    "print(missing_df.head())\n",
    "print(f\"Total missing rows after all columns included: {len(missing_df)}\")\n",
    "\n",
    "# Save missing rows to a file if needed for further inspection\n",
    "# missing_df.to_csv(\"missing_rows_final.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>mesh_terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>131072</th>\n",
       "      <td>11099584</td>\n",
       "      <td>Providing pediatric subspecialty care: A workf...</td>\n",
       "      <td>OBJECTIVE: To provide a snapshot of pediatric ...</td>\n",
       "      <td>Adolescent; Adult; Aged; Cardiology; Child; Cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10186588</td>\n",
       "      <td>New Jersey health promotion and disease preven...</td>\n",
       "      <td>General: Health promotion is a major component...</td>\n",
       "      <td>Female; Health Education; Health Promotion; Hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10186587</td>\n",
       "      <td>Who will provide preventive services? The chan...</td>\n",
       "      <td>General: Health care reform in the United Stat...</td>\n",
       "      <td>Delivery of Health Care; Female; Health Care R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262147</th>\n",
       "      <td>15722370</td>\n",
       "      <td>Impact of misclassification of in vitro fertil...</td>\n",
       "      <td>OBJECTIVE: To determine whether failure to ade...</td>\n",
       "      <td>Data Collection; Dietary Supplements; Female; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917506</th>\n",
       "      <td>34794827</td>\n",
       "      <td>Physiologic flow-conditioning limits vascular ...</td>\n",
       "      <td>General: Hemodynamics play a central role in t...</td>\n",
       "      <td>Animals; Capillaries; Endothelial Cells; Hemod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786420</th>\n",
       "      <td>30631154</td>\n",
       "      <td>LncRNAs-directed PTEN enzymatic switch governs...</td>\n",
       "      <td>General: Despite the structural conservation o...</td>\n",
       "      <td>Animals; Cell Line; Epithelial-Mesenchymal Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>917494</th>\n",
       "      <td>34796962</td>\n",
       "      <td>Autoantibodies to red blood cell surface Glyco...</td>\n",
       "      <td>BACKGROUND: Both M and N alleles encode antige...</td>\n",
       "      <td>Autoantibodies; Blood Group Antigens; Erythroc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262138</th>\n",
       "      <td>15723817</td>\n",
       "      <td>The JCAHO patient safety event taxonomy: a sta...</td>\n",
       "      <td>BACKGROUND: The current US national discussion...</td>\n",
       "      <td>Causality; Classification; Communication; Heal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524285</th>\n",
       "      <td>21216555</td>\n",
       "      <td>Health care utilization before and after an ou...</td>\n",
       "      <td>BACKGROUND: Older adults in the United States ...</td>\n",
       "      <td>Aged; Chi-Square Distribution; Delivery of Hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048575</th>\n",
       "      <td>39082394</td>\n",
       "      <td>Cross-disciplinary mathematical modelling to b...</td>\n",
       "      <td>General: Clinical pharmacology is often the ne...</td>\n",
       "      <td>Humans; Pharmacology, Clinical; Delivery of He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54819 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              uid                                              title  \\\n",
       "131072   11099584  Providing pediatric subspecialty care: A workf...   \n",
       "1        10186588  New Jersey health promotion and disease preven...   \n",
       "2        10186587  Who will provide preventive services? The chan...   \n",
       "262147   15722370  Impact of misclassification of in vitro fertil...   \n",
       "917506   34794827  Physiologic flow-conditioning limits vascular ...   \n",
       "...           ...                                                ...   \n",
       "786420   30631154  LncRNAs-directed PTEN enzymatic switch governs...   \n",
       "917494   34796962  Autoantibodies to red blood cell surface Glyco...   \n",
       "262138   15723817  The JCAHO patient safety event taxonomy: a sta...   \n",
       "524285   21216555  Health care utilization before and after an ou...   \n",
       "1048575  39082394  Cross-disciplinary mathematical modelling to b...   \n",
       "\n",
       "                                                  abstract  \\\n",
       "131072   OBJECTIVE: To provide a snapshot of pediatric ...   \n",
       "1        General: Health promotion is a major component...   \n",
       "2        General: Health care reform in the United Stat...   \n",
       "262147   OBJECTIVE: To determine whether failure to ade...   \n",
       "917506   General: Hemodynamics play a central role in t...   \n",
       "...                                                    ...   \n",
       "786420   General: Despite the structural conservation o...   \n",
       "917494   BACKGROUND: Both M and N alleles encode antige...   \n",
       "262138   BACKGROUND: The current US national discussion...   \n",
       "524285   BACKGROUND: Older adults in the United States ...   \n",
       "1048575  General: Clinical pharmacology is often the ne...   \n",
       "\n",
       "                                                mesh_terms  \n",
       "131072   Adolescent; Adult; Aged; Cardiology; Child; Cr...  \n",
       "1        Female; Health Education; Health Promotion; Hu...  \n",
       "2        Delivery of Health Care; Female; Health Care R...  \n",
       "262147   Data Collection; Dietary Supplements; Female; ...  \n",
       "917506   Animals; Capillaries; Endothelial Cells; Hemod...  \n",
       "...                                                    ...  \n",
       "786420   Animals; Cell Line; Epithelial-Mesenchymal Tra...  \n",
       "917494   Autoantibodies; Blood Group Antigens; Erythroc...  \n",
       "262138   Causality; Classification; Communication; Heal...  \n",
       "524285   Aged; Chi-Square Distribution; Delivery of Hea...  \n",
       "1048575  Humans; Pharmacology, Clinical; Delivery of He...  \n",
       "\n",
       "[54819 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uid                                       object\n",
       "title                                     object\n",
       "journal                                   object\n",
       "abstract                                  object\n",
       "authors                                   object\n",
       "affiliations                              object\n",
       "mesh_terms                                object\n",
       "keywords                                  object\n",
       "coi_statement                             object\n",
       "parsed_date                       datetime64[ns]\n",
       "cleaned_title_tokens_simple               object\n",
       "cleaned_title_tokens_hf                   object\n",
       "cleaned_abstract_tokens_simple            object\n",
       "cleaned_abstract_tokens_hf                object\n",
       "disease_title_tokens_simple               object\n",
       "disease_title_tokens_hf                   object\n",
       "disease_abstract_tokens_simple            object\n",
       "disease_abstract_tokens_hf                object\n",
       "disease_abstract_spacy                    object\n",
       "disease_title_spacy                       object\n",
       "disease_mesh_terms_spacy                  object\n",
       "merged_tokens                             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you already have your DataFrame loaded as df\n",
    "# df = pd.read_parquet(\"your_file.parquet\", engine=\"pyarrow\")  # Example load\n",
    "\n",
    "token_columns = [\n",
    "    \"disease_title_tokens_simple\",\n",
    "    \"disease_title_tokens_hf\",\n",
    "    \"disease_abstract_tokens_simple\",\n",
    "    \"disease_abstract_tokens_hf\",\n",
    "    \"disease_title_spacy\",\n",
    "    \"disease_mesh_terms_spacy\",\n",
    "    \"disease_abstract_spacy\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Create a new column with the merged tokens\n",
    "# df['merged_tokens'] = df.apply(\n",
    "#     lambda row: [\n",
    "#         token \n",
    "#         for col in token_columns\n",
    "#         # row[col] is a NumPy array; convert to list before iterating\n",
    "#         for token in row[col].tolist()\n",
    "#     ],\n",
    "#     axis=1\n",
    "# )\n",
    "\n",
    "# # (Optional) Inspect the first few rows\n",
    "# print(df[['merged_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       merged_tokens\n",
      "0  [hiv, hiv, hiv, aids, hiv, aids, HIV Infection...\n",
      "1                                                 []\n",
      "2                                                 []\n",
      "3  [tumor, tumor, Adenocarcinoma, Neoplasms, Ovar...\n",
      "4  [American Kidney Failure, renal failure, renal...\n"
     ]
    }
   ],
   "source": [
    "df['merged_tokens'] = [\n",
    "    [token for col in token_columns for token in row[col].tolist()]\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# (Optional) Inspect the first few rows\n",
    "print(df[['merged_tokens']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>keywords</th>\n",
       "      <th>coi_statement</th>\n",
       "      <th>parsed_date</th>\n",
       "      <th>...</th>\n",
       "      <th>cleaned_abstract_tokens_simple</th>\n",
       "      <th>cleaned_abstract_tokens_hf</th>\n",
       "      <th>disease_title_tokens_simple</th>\n",
       "      <th>disease_title_tokens_hf</th>\n",
       "      <th>disease_abstract_tokens_simple</th>\n",
       "      <th>disease_abstract_tokens_hf</th>\n",
       "      <th>disease_abstract_spacy</th>\n",
       "      <th>disease_title_spacy</th>\n",
       "      <th>disease_mesh_terms_spacy</th>\n",
       "      <th>merged_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10186596</td>\n",
       "      <td>The potential impact of health care reform on ...</td>\n",
       "      <td>Journal of public health management and practi...</td>\n",
       "      <td>General: This article observes that, despite t...</td>\n",
       "      <td>Auerbach J; McGuire J</td>\n",
       "      <td>HIV/AIDS Bureau, Massachusetts Department of P...</td>\n",
       "      <td>Financing, Government; HIV Infections; Health ...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[general, article, observes, despite, clear, p...</td>\n",
       "      <td>[[CLS], general, article, observes, despite, c...</td>\n",
       "      <td>[hiv]</td>\n",
       "      <td>[hiv]</td>\n",
       "      <td>[hiv, aids]</td>\n",
       "      <td>[hiv, aids]</td>\n",
       "      <td>[human immunodeficiency virus (HIV) disease, a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[HIV Infections]</td>\n",
       "      <td>[hiv, hiv, hiv, aids, hiv, aids, HIV Infection...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10186588</td>\n",
       "      <td>New Jersey health promotion and disease preven...</td>\n",
       "      <td>Journal of public health management and practi...</td>\n",
       "      <td>General: Health promotion is a major component...</td>\n",
       "      <td>Louria D B</td>\n",
       "      <td>Department of Preventive Medicine and Communit...</td>\n",
       "      <td>Female; Health Education; Health Promotion; Hu...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[general, health, promotion, major, component,...</td>\n",
       "      <td>[[CLS], general, health, promotion, major, com...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10186587</td>\n",
       "      <td>Who will provide preventive services? The chan...</td>\n",
       "      <td>Journal of public health management and practi...</td>\n",
       "      <td>General: Health care reform in the United Stat...</td>\n",
       "      <td>Pearson T A; Spencer M; Jenkins P</td>\n",
       "      <td>Mary Imogene Bassett Research Institute, Coope...</td>\n",
       "      <td>Delivery of Health Care; Female; Health Care R...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[general, health, care, reform, united, states...</td>\n",
       "      <td>[[CLS], general, health, care, reform, united,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10163501</td>\n",
       "      <td>Cytoreduction of small intestine metastases us...</td>\n",
       "      <td>Journal of gynecologic surgery</td>\n",
       "      <td>General: The Cavitron Ultrasonic Surgical Aspi...</td>\n",
       "      <td>Adelson M D</td>\n",
       "      <td>Department of Obstetrics and Gynecology, Crous...</td>\n",
       "      <td>Adenocarcinoma; Fallopian Tube Neoplasms; Fema...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[general, cavitron, ultrasonic, surgical, aspi...</td>\n",
       "      <td>[[CLS], general, ca, ##vi, ##tron, ultra, ##so...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tumor]</td>\n",
       "      <td>[tumor]</td>\n",
       "      <td>[carcinoma of the ovary, and one each had, tub...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Adenocarcinoma, Neoplasms, Ovarian Neoplasms]</td>\n",
       "      <td>[tumor, tumor, Adenocarcinoma, Neoplasms, Ovar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10157383</td>\n",
       "      <td>Racial differences in access to kidney transpl...</td>\n",
       "      <td>Health care financing review</td>\n",
       "      <td>General: Previous work has documented large di...</td>\n",
       "      <td>Eggers P W</td>\n",
       "      <td>Office of Research and Demonstrations, Health ...</td>\n",
       "      <td>Adolescent; Adult; Black or African American; ...</td>\n",
       "      <td>Empirical Approach; End Stage Renal Disease Pr...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1995-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>[general, previous, work, documented, large, d...</td>\n",
       "      <td>[[CLS], general, previous, work, documented, l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[renal failure, renal failure, end stage renal...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[American Kidney Failure]</td>\n",
       "      <td>[American Kidney Failure, renal failure, renal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid                                              title  \\\n",
       "0  10186596  The potential impact of health care reform on ...   \n",
       "1  10186588  New Jersey health promotion and disease preven...   \n",
       "2  10186587  Who will provide preventive services? The chan...   \n",
       "3  10163501  Cytoreduction of small intestine metastases us...   \n",
       "4  10157383  Racial differences in access to kidney transpl...   \n",
       "\n",
       "                                             journal  \\\n",
       "0  Journal of public health management and practi...   \n",
       "1  Journal of public health management and practi...   \n",
       "2  Journal of public health management and practi...   \n",
       "3                     Journal of gynecologic surgery   \n",
       "4                       Health care financing review   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  General: This article observes that, despite t...   \n",
       "1  General: Health promotion is a major component...   \n",
       "2  General: Health care reform in the United Stat...   \n",
       "3  General: The Cavitron Ultrasonic Surgical Aspi...   \n",
       "4  General: Previous work has documented large di...   \n",
       "\n",
       "                             authors  \\\n",
       "0              Auerbach J; McGuire J   \n",
       "1                         Louria D B   \n",
       "2  Pearson T A; Spencer M; Jenkins P   \n",
       "3                        Adelson M D   \n",
       "4                         Eggers P W   \n",
       "\n",
       "                                        affiliations  \\\n",
       "0  HIV/AIDS Bureau, Massachusetts Department of P...   \n",
       "1  Department of Preventive Medicine and Communit...   \n",
       "2  Mary Imogene Bassett Research Institute, Coope...   \n",
       "3  Department of Obstetrics and Gynecology, Crous...   \n",
       "4  Office of Research and Demonstrations, Health ...   \n",
       "\n",
       "                                          mesh_terms  \\\n",
       "0  Financing, Government; HIV Infections; Health ...   \n",
       "1  Female; Health Education; Health Promotion; Hu...   \n",
       "2  Delivery of Health Care; Female; Health Care R...   \n",
       "3  Adenocarcinoma; Fallopian Tube Neoplasms; Fema...   \n",
       "4  Adolescent; Adult; Black or African American; ...   \n",
       "\n",
       "                                            keywords coi_statement  \\\n",
       "0                                                              N/A   \n",
       "1                                                              N/A   \n",
       "2                                                              N/A   \n",
       "3                                                              N/A   \n",
       "4  Empirical Approach; End Stage Renal Disease Pr...           N/A   \n",
       "\n",
       "  parsed_date  ...                     cleaned_abstract_tokens_simple  \\\n",
       "0  1995-01-01  ...  [general, article, observes, despite, clear, p...   \n",
       "1  1995-01-01  ...  [general, health, promotion, major, component,...   \n",
       "2  1995-01-01  ...  [general, health, care, reform, united, states...   \n",
       "3  1995-01-01  ...  [general, cavitron, ultrasonic, surgical, aspi...   \n",
       "4  1995-01-01  ...  [general, previous, work, documented, large, d...   \n",
       "\n",
       "                          cleaned_abstract_tokens_hf  \\\n",
       "0  [[CLS], general, article, observes, despite, c...   \n",
       "1  [[CLS], general, health, promotion, major, com...   \n",
       "2  [[CLS], general, health, care, reform, united,...   \n",
       "3  [[CLS], general, ca, ##vi, ##tron, ultra, ##so...   \n",
       "4  [[CLS], general, previous, work, documented, l...   \n",
       "\n",
       "  disease_title_tokens_simple disease_title_tokens_hf  \\\n",
       "0                       [hiv]                   [hiv]   \n",
       "1                          []                      []   \n",
       "2                          []                      []   \n",
       "3                          []                      []   \n",
       "4                          []                      []   \n",
       "\n",
       "  disease_abstract_tokens_simple disease_abstract_tokens_hf  \\\n",
       "0                    [hiv, aids]                [hiv, aids]   \n",
       "1                             []                         []   \n",
       "2                             []                         []   \n",
       "3                        [tumor]                    [tumor]   \n",
       "4                             []                         []   \n",
       "\n",
       "                              disease_abstract_spacy disease_title_spacy  \\\n",
       "0  [human immunodeficiency virus (HIV) disease, a...                  []   \n",
       "1                                                 []                  []   \n",
       "2                                                 []                  []   \n",
       "3  [carcinoma of the ovary, and one each had, tub...                  []   \n",
       "4  [renal failure, renal failure, end stage renal...                  []   \n",
       "\n",
       "                         disease_mesh_terms_spacy  \\\n",
       "0                                [HIV Infections]   \n",
       "1                                              []   \n",
       "2                                              []   \n",
       "3  [Adenocarcinoma, Neoplasms, Ovarian Neoplasms]   \n",
       "4                       [American Kidney Failure]   \n",
       "\n",
       "                                       merged_tokens  \n",
       "0  [hiv, hiv, hiv, aids, hiv, aids, HIV Infection...  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3  [tumor, tumor, Adenocarcinoma, Neoplasms, Ovar...  \n",
       "4  [American Kidney Failure, renal failure, renal...  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54819"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"is_missing_merged_tokens\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "uid                                       object\n",
       "title                                     object\n",
       "journal                                   object\n",
       "abstract                                  object\n",
       "authors                                   object\n",
       "affiliations                              object\n",
       "mesh_terms                                object\n",
       "keywords                                  object\n",
       "coi_statement                             object\n",
       "parsed_date                       datetime64[ns]\n",
       "cleaned_title_tokens_simple               object\n",
       "cleaned_title_tokens_hf                   object\n",
       "cleaned_abstract_tokens_simple            object\n",
       "cleaned_abstract_tokens_hf                object\n",
       "disease_title_tokens_simple               object\n",
       "disease_title_tokens_hf                   object\n",
       "disease_abstract_tokens_simple            object\n",
       "disease_abstract_tokens_hf                object\n",
       "disease_abstract_spacy                    object\n",
       "disease_title_spacy                       object\n",
       "disease_mesh_terms_spacy                  object\n",
       "merged_tokens                             object\n",
       "is_missing_merged_tokens                    bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: uid\n",
      "uid\n",
      "<class 'str'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: title\n",
      "title\n",
      "<class 'str'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: journal\n",
      "journal\n",
      "<class 'str'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: abstract\n",
      "abstract\n",
      "<class 'str'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: authors\n",
      "authors\n",
      "<class 'str'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: affiliations\n",
      "affiliations\n",
      "<class 'str'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: mesh_terms\n",
      "mesh_terms\n",
      "<class 'str'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: keywords\n",
      "keywords\n",
      "<class 'str'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: coi_statement\n",
      "coi_statement\n",
      "<class 'str'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: cleaned_title_tokens_simple\n",
      "cleaned_title_tokens_simple\n",
      "<class 'numpy.ndarray'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: cleaned_title_tokens_hf\n",
      "cleaned_title_tokens_hf\n",
      "<class 'numpy.ndarray'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: cleaned_abstract_tokens_simple\n",
      "cleaned_abstract_tokens_simple\n",
      "<class 'numpy.ndarray'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: cleaned_abstract_tokens_hf\n",
      "cleaned_abstract_tokens_hf\n",
      "<class 'numpy.ndarray'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: disease_title_tokens_simple\n",
      "disease_title_tokens_simple\n",
      "<class 'numpy.ndarray'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: disease_title_tokens_hf\n",
      "disease_title_tokens_hf\n",
      "<class 'numpy.ndarray'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: disease_abstract_tokens_simple\n",
      "disease_abstract_tokens_simple\n",
      "<class 'numpy.ndarray'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: disease_abstract_tokens_hf\n",
      "disease_abstract_tokens_hf\n",
      "<class 'numpy.ndarray'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: disease_abstract_spacy\n",
      "disease_abstract_spacy\n",
      "<class 'numpy.ndarray'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: disease_title_spacy\n",
      "disease_title_spacy\n",
      "<class 'numpy.ndarray'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: disease_mesh_terms_spacy\n",
      "disease_mesh_terms_spacy\n",
      "<class 'numpy.ndarray'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "Column: merged_tokens\n",
      "merged_tokens\n",
      "<class 'list'>    1057871\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Function to check types in all columns\n",
    "def inspect_column_types(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":  # Focus on object columns\n",
    "            type_counts = df[col].apply(type).value_counts()\n",
    "            print(f\"Column: {col}\")\n",
    "            print(type_counts)\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "# Run the inspection\n",
    "inspect_column_types(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting DataFrame of 1057871 rows into 11 batches (size=100000).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:   9%|▉         | 1/11 [00:10<01:42, 10.28s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 1 rows [0:100000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_1.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  18%|█▊        | 2/11 [00:20<01:30, 10.01s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 2 rows [100000:200000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_2.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  27%|██▋       | 3/11 [00:30<01:20, 10.12s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 3 rows [200000:300000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_3.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  36%|███▋      | 4/11 [00:40<01:10, 10.11s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 4 rows [300000:400000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_4.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  45%|████▌     | 5/11 [00:50<01:00, 10.16s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 5 rows [400000:500000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_5.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  55%|█████▍    | 6/11 [01:01<00:51, 10.28s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 6 rows [500000:600000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_6.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  64%|██████▎   | 7/11 [01:12<00:42, 10.68s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 7 rows [600000:700000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_7.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  73%|███████▎  | 8/11 [01:24<00:33, 11.02s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 8 rows [700000:800000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_8.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  82%|████████▏ | 9/11 [01:36<00:22, 11.33s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 9 rows [800000:900000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_9.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches:  91%|█████████ | 10/11 [01:49<00:11, 11.74s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 10 rows [900000:1000000] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_10.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Batches: 100%|██████████| 11/11 [01:57<00:00, 10.64s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Batch 11 rows [1000000:1057871] saved to h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\temp_batches\\temp_batch_11.parquet\n",
      "\n",
      "Merging 11 batch files into h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\P6_merged_tokens.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging Batches: 100%|██████████| 11/11 [01:19<00:00,  7.21s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged DataFrame saved as: h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\P6_merged_tokens.parquet\n",
      "\n",
      "Temporary batch files removed. All done!\n",
      "All done. Merged file at: h:\\000_Projects\\01_GitHub\\05_PythonProjects\\PubMedResearch\\Data\\2.Processed\\ModellingData\\P6_merged_tokens.parquet\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Define the base path relative to the current file\n",
    "    base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Move one directory up\n",
    "\n",
    "    # Construct the file path\n",
    "    file_path = os.path.join(base_path, \"Data\", \"2.Processed\", \"ModellingData\", \"P5_final_new.parquet\")\n",
    "\n",
    "    # Define the output folder and final file\n",
    "    output_folder = os.path.join(base_path, \"Data\", \"2.Processed\", \"ModellingData\")\n",
    "    final_file = \"P6_merged_tokens.parquet\"\n",
    "\n",
    "    batch_size = 100_000  # Define your desired chunk size\n",
    "    \n",
    "    # Save and merge in batches\n",
    "    result_path = save_and_merge_in_batches(\n",
    "        df=df,\n",
    "        batch_size=batch_size,\n",
    "        output_folder=output_folder,\n",
    "        final_filename=final_file,\n",
    "        temp_batch_prefix=\"temp_batch_\"\n",
    "    )\n",
    "\n",
    "    print(f\"All done. Merged file at: {result_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
