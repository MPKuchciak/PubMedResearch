{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10443237,"sourceType":"datasetVersion","datasetId":6366131}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:17:00.800200Z","iopub.execute_input":"2025-01-11T13:17:00.800495Z","iopub.status.idle":"2025-01-11T13:17:00.812932Z","shell.execute_reply.started":"2025-01-11T13:17:00.800471Z","shell.execute_reply":"2025-01-11T13:17:00.811967Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/pubmedabstracts/PubMedAbstracts_final.parquet\n/kaggle/input/pubmedabstracts/P5_final_new.parquet\n/kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_6.parquet\n/kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_4.parquet\n/kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_7.parquet\n/kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_2.parquet\n/kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_1.parquet\n/kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_5.parquet\n/kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_8.parquet\n/kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_3.parquet\n/kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_10.parquet\n/kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_11.parquet\n/kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_9.parquet\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Creation of folder labeled_chunks for labeled data\nimport os\n\noutput_dir = \"/kaggle/working/labeled_chunks\"\n\n# Create the directory if it does not exist\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n    print(f\"Directory created: {output_dir}\")\nelse:\n    print(f\"Directory already exists: {output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:17:01.105462Z","iopub.execute_input":"2025-01-11T13:17:01.105787Z","iopub.status.idle":"2025-01-11T13:17:01.110700Z","shell.execute_reply.started":"2025-01-11T13:17:01.105760Z","shell.execute_reply":"2025-01-11T13:17:01.109856Z"}},"outputs":[{"name":"stdout","text":"Directory already exists: /kaggle/working/labeled_chunks\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# 1. Import Libraries\nimport os\nimport pandas as pd\nfrom transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:17:01.455608Z","iopub.execute_input":"2025-01-11T13:17:01.455885Z","iopub.status.idle":"2025-01-11T13:17:01.459822Z","shell.execute_reply.started":"2025-01-11T13:17:01.455864Z","shell.execute_reply":"2025-01-11T13:17:01.459070Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"# Input and output directories\ninput_dir = \"/kaggle/input/pubmedabstracts/Chunks\"  # Folder containing input chunks\noutput_dir = \"/kaggle/working/labeled_chunks\"  # Folder for labeled output\n\n# Create the output directory if it does not exist\nos.makedirs(output_dir, exist_ok=True)\nprint(f\"Output directory: {output_dir}\")\n\n# File pattern for identifying chunk files\nfile_pattern = \"sent_chunks_chunk\"\n\n# Pre-trained model for sentiment analysis\nbiomedbert_model = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:17:01.973499Z","iopub.execute_input":"2025-01-11T13:17:01.973834Z","iopub.status.idle":"2025-01-11T13:17:01.979103Z","shell.execute_reply.started":"2025-01-11T13:17:01.973811Z","shell.execute_reply":"2025-01-11T13:17:01.978399Z"}},"outputs":[{"name":"stdout","text":"Output directory: /kaggle/working/labeled_chunks\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Path to the chunks folder\nchunks_dir = \"/kaggle/input/pubmedabstracts/Chunks\"\n\n# List all Parquet files\nchunk_files = sorted([os.path.join(chunks_dir, f) for f in os.listdir(chunks_dir) if f.endswith(\".parquet\")])\n\n# Load one chunk to preview\nfor chunk_file in chunk_files:\n    print(f\"Previewing file: {chunk_file}\")\n    df_chunk = pd.read_parquet(chunk_file)\n    print(df_chunk.head())  # Display first few rows\n    break  # Only preview the first file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:17:03.527470Z","iopub.execute_input":"2025-01-11T13:17:03.527790Z","iopub.status.idle":"2025-01-11T13:17:05.070571Z","shell.execute_reply.started":"2025-01-11T13:17:03.527767Z","shell.execute_reply":"2025-01-11T13:17:05.069634Z"}},"outputs":[{"name":"stdout","text":"Previewing file: /kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_1.parquet\n        uid                                           abstract\n0  10186596  General: This article observes that, despite t...\n1  10186588  General: Health promotion is a major component...\n2  10186587  General: Health care reform in the United Stat...\n3  10163501  General: The Cavitron Ultrasonic Surgical Aspi...\n4  10157383  General: Previous work has documented large di...\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# 2. Helper Function for Long Texts\ndef predict_long_text_biomed(\n    text: str,\n    pipe,\n    max_length=512,\n    stride=256\n):\n    \"\"\"\n    Handles long texts using a sliding window.\n    \"\"\"\n    tokenizer = pipe.tokenizer\n    input_ids = tokenizer(text, add_special_tokens=True)[\"input_ids\"]\n    total_tokens = len(input_ids)\n\n    if total_tokens <= max_length:\n        return pipe(text)[0]\n\n    subchunk_sentiments = []\n    start = 0\n    while start < total_tokens:\n        end = start + max_length\n        sub_ids = input_ids[start:end]\n        sub_text = tokenizer.decode(sub_ids, skip_special_tokens=True)\n\n        sub_res = pipe(sub_text)\n        subchunk_sentiments.append(sub_res[0])\n\n        if end >= total_tokens:\n            break\n        start += max_length - stride\n\n    label_counts = {}\n    for chunk_res in subchunk_sentiments:\n        lbl = chunk_res[\"label\"]\n        label_counts[lbl] = label_counts.get(lbl, 0) + 1\n\n    overall_label = max(label_counts, key=label_counts.get)\n    overall_score = label_counts[overall_label] / len(subchunk_sentiments)\n    return {\"label\": overall_label, \"score\": overall_score}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:17:05.071597Z","iopub.execute_input":"2025-01-11T13:17:05.071906Z","iopub.status.idle":"2025-01-11T13:17:05.077233Z","shell.execute_reply.started":"2025-01-11T13:17:05.071882Z","shell.execute_reply":"2025-01-11T13:17:05.076610Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# 3. Single Chunk Processing\ndef process_chunk(\n    chunk_file: str,\n    output_folder: str,\n    biomedbert_model: str,\n    max_length: int = 512,\n    stride: int = 256,\n    batch_size: int = 500,  # Process in batches\n    text_column: str = \"abstract\",\n    uid_column: str = \"uid\"\n):\n    \"\"\"\n    Processes a single chunk, predicts sentiment in batches, and saves the output.\n    \"\"\"\n    output_file = os.path.join(output_folder, f\"labeled_{os.path.basename(chunk_file)}\")\n    if os.path.exists(output_file):\n        print(f\"Chunk {chunk_file} already processed. Skipping.\")\n        return\n\n    # Load chunk\n    df_chunk = pd.read_parquet(chunk_file)\n    print(f\"Processing {len(df_chunk)} rows from {chunk_file}...\")\n\n    # Initialize pipeline\n    tokenizer = AutoTokenizer.from_pretrained(biomedbert_model)\n    model = AutoModelForSequenceClassification.from_pretrained(biomedbert_model)\n    pipe = pipeline(\n        \"sentiment-analysis\",\n        model=model,\n        tokenizer=tokenizer,\n        clean_up_tokenization_spaces=True,  # warning avoidance #newly added line\n        truncation=True,\n        max_length=max_length,\n        device=0  # Use GPU\n    )\n\n    results = []\n    for start in tqdm(range(0, len(df_chunk), batch_size), desc=\"Batch Processing\"):\n        # Process batch\n        sub_batch = df_chunk.iloc[start:start + batch_size]\n        for _, row in sub_batch.iterrows():\n            prediction = predict_long_text_biomed(\n                text=row[text_column],\n                pipe=pipe,\n                max_length=max_length,\n                stride=stride\n            )\n            results.append({\n                \"UID\": row[uid_column],\n                \"label\": prediction[\"label\"],\n                \"score\": prediction[\"score\"]\n            })\n\n    # Save results\n    output_df = pd.DataFrame(results)\n    output_df.to_parquet(output_file, index=False)\n    print(f\"Saved labeled chunk to {output_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:17:05.078701Z","iopub.execute_input":"2025-01-11T13:17:05.078916Z","iopub.status.idle":"2025-01-11T13:17:05.095131Z","shell.execute_reply.started":"2025-01-11T13:17:05.078897Z","shell.execute_reply":"2025-01-11T13:17:05.094375Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# 4. Sequential Processing for All Chunks\ndef biomedbert_sequential_processing(\n    input_folder: str,\n    output_folder: str,\n    file_pattern: str,\n    biomedbert_model: str,\n    max_length: int = 512,\n    stride: int = 256,\n    text_column: str = \"abstract\",\n    uid_column: str = \"uid\",\n    specific_chunk: str = None\n):\n    \"\"\"\n    Processes all chunks sequentially or a specific chunk.\n    \"\"\"\n    os.makedirs(output_folder, exist_ok=True)\n\n    chunk_files = [\n        os.path.join(input_folder, f)\n        for f in os.listdir(input_folder)\n        if file_pattern in f\n    ]\n    chunk_files.sort()\n\n    if specific_chunk:\n        process_chunk(\n            chunk_file=specific_chunk,\n            output_folder=output_folder,\n            biomedbert_model=biomedbert_model,\n            max_length=max_length,\n            stride=stride,\n            text_column=text_column,\n            uid_column=uid_column\n        )\n    else:\n        for chunk_file in tqdm(chunk_files, desc=\"Processing Chunks\"):\n            process_chunk(\n                chunk_file=chunk_file,\n                output_folder=output_folder,\n                biomedbert_model=biomedbert_model,\n                max_length=max_length,\n                stride=stride,\n                text_column=text_column,\n                uid_column=uid_column\n            )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:17:05.096098Z","iopub.execute_input":"2025-01-11T13:17:05.096392Z","iopub.status.idle":"2025-01-11T13:17:05.111919Z","shell.execute_reply.started":"2025-01-11T13:17:05.096363Z","shell.execute_reply":"2025-01-11T13:17:05.111229Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    input_dir = \"/kaggle/input/pubmedabstracts/Chunks\"\n    output_dir = \"/kaggle/working/labeled_chunks\"\n    file_pattern = \"sent_chunks_chunk\"\n    biomedbert_model = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract\"\n\n    # Specify the chunk you want to process\n    specific_chunk = None #\"/kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_7.parquet\" #None #\"/kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_1.parquet\"\n\n    # Call the function\n    biomedbert_sequential_processing(\n        input_folder=input_dir,\n        output_folder=output_dir,\n        file_pattern=file_pattern,\n        biomedbert_model=biomedbert_model,\n        max_length=512,\n        stride=256,\n        text_column=\"abstract\",\n        uid_column=\"uid\",\n        specific_chunk=specific_chunk\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:17:05.112756Z","iopub.execute_input":"2025-01-11T13:17:05.113033Z","iopub.status.idle":"2025-01-11T13:53:42.715830Z","shell.execute_reply.started":"2025-01-11T13:17:05.113004Z","shell.execute_reply":"2025-01-11T13:53:42.714889Z"}},"outputs":[{"name":"stderr","text":"Processing Chunks:   0%|          | 0/11 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Chunk /kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_1.parquet already processed. Skipping.\nChunk /kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_10.parquet already processed. Skipping.\nChunk /kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_11.parquet already processed. Skipping.\nChunk /kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_2.parquet already processed. Skipping.\nChunk /kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_3.parquet already processed. Skipping.\nChunk /kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_4.parquet already processed. Skipping.\nChunk /kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_5.parquet already processed. Skipping.\nChunk /kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_6.parquet already processed. Skipping.\nChunk /kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_7.parquet already processed. Skipping.\nProcessing 100000 rows from /kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_8.parquet...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nBatch Processing:   0%|          | 0/200 [00:00<?, ?it/s]\u001b[A\nBatch Processing:   0%|          | 1/200 [00:10<35:10, 10.61s/it]\u001b[A\nBatch Processing:   1%|          | 2/200 [00:22<37:05, 11.24s/it]\u001b[A\nBatch Processing:   2%|▏         | 3/200 [00:33<36:37, 11.15s/it]\u001b[A\nBatch Processing:   2%|▏         | 4/200 [00:43<35:47, 10.95s/it]\u001b[A\nBatch Processing:   2%|▎         | 5/200 [00:53<34:20, 10.57s/it]\u001b[A\nBatch Processing:   3%|▎         | 6/200 [01:04<34:30, 10.67s/it]\u001b[A\nBatch Processing:   4%|▎         | 7/200 [01:15<34:28, 10.72s/it]\u001b[A\nBatch Processing:   4%|▍         | 8/200 [01:26<34:22, 10.74s/it]\u001b[A\nBatch Processing:   4%|▍         | 9/200 [01:38<35:50, 11.26s/it]\u001b[A\nBatch Processing:   5%|▌         | 10/200 [01:48<34:14, 10.81s/it]\u001b[A\nBatch Processing:   6%|▌         | 11/200 [02:00<34:52, 11.07s/it]\u001b[A\nBatch Processing:   6%|▌         | 12/200 [02:10<34:07, 10.89s/it]\u001b[A\nBatch Processing:   6%|▋         | 13/200 [02:21<33:54, 10.88s/it]\u001b[A\nBatch Processing:   7%|▋         | 14/200 [02:32<33:48, 10.91s/it]\u001b[A\nBatch Processing:   8%|▊         | 15/200 [02:43<33:23, 10.83s/it]\u001b[A\nBatch Processing:   8%|▊         | 16/200 [02:53<32:41, 10.66s/it]\u001b[A\nBatch Processing:   8%|▊         | 17/200 [03:04<32:53, 10.78s/it]\u001b[A\nBatch Processing:   9%|▉         | 18/200 [03:15<32:33, 10.73s/it]\u001b[A\nBatch Processing:  10%|▉         | 19/200 [03:26<33:06, 10.97s/it]\u001b[A\nBatch Processing:  10%|█         | 20/200 [03:37<33:08, 11.05s/it]\u001b[A\nBatch Processing:  10%|█         | 21/200 [03:49<33:37, 11.27s/it]\u001b[A\nBatch Processing:  11%|█         | 22/200 [03:59<32:35, 10.99s/it]\u001b[A\nBatch Processing:  12%|█▏        | 23/200 [04:10<32:19, 10.96s/it]\u001b[A\nBatch Processing:  12%|█▏        | 24/200 [04:22<32:18, 11.02s/it]\u001b[A\nBatch Processing:  12%|█▎        | 25/200 [04:33<32:14, 11.05s/it]\u001b[A\nBatch Processing:  13%|█▎        | 26/200 [04:44<32:13, 11.11s/it]\u001b[A\nBatch Processing:  14%|█▎        | 27/200 [04:54<31:20, 10.87s/it]\u001b[A\nBatch Processing:  14%|█▍        | 28/200 [05:05<30:51, 10.77s/it]\u001b[A\nBatch Processing:  14%|█▍        | 29/200 [05:16<31:04, 10.90s/it]\u001b[A\nBatch Processing:  15%|█▌        | 30/200 [05:27<31:07, 10.98s/it]\u001b[A\nBatch Processing:  16%|█▌        | 31/200 [05:38<30:53, 10.97s/it]\u001b[A\nBatch Processing:  16%|█▌        | 32/200 [05:49<30:36, 10.93s/it]\u001b[A\nBatch Processing:  16%|█▋        | 33/200 [06:00<30:47, 11.07s/it]\u001b[A\nBatch Processing:  17%|█▋        | 34/200 [06:10<29:37, 10.71s/it]\u001b[A\nBatch Processing:  18%|█▊        | 35/200 [06:21<29:46, 10.83s/it]\u001b[A\nBatch Processing:  18%|█▊        | 36/200 [06:32<29:47, 10.90s/it]\u001b[A\nBatch Processing:  18%|█▊        | 37/200 [06:43<29:24, 10.82s/it]\u001b[A\nBatch Processing:  19%|█▉        | 38/200 [06:53<28:49, 10.68s/it]\u001b[A\nBatch Processing:  20%|█▉        | 39/200 [07:03<28:04, 10.47s/it]\u001b[A\nBatch Processing:  20%|██        | 40/200 [07:14<27:57, 10.49s/it]\u001b[A\nBatch Processing:  20%|██        | 41/200 [07:25<28:29, 10.75s/it]\u001b[A\nBatch Processing:  21%|██        | 42/200 [07:37<28:46, 10.93s/it]\u001b[A\nBatch Processing:  22%|██▏       | 43/200 [07:47<28:21, 10.84s/it]\u001b[A\nBatch Processing:  22%|██▏       | 44/200 [07:58<28:09, 10.83s/it]\u001b[A\nBatch Processing:  22%|██▎       | 45/200 [08:09<28:23, 10.99s/it]\u001b[A\nBatch Processing:  23%|██▎       | 46/200 [08:20<27:51, 10.85s/it]\u001b[A\nBatch Processing:  24%|██▎       | 47/200 [08:31<28:07, 11.03s/it]\u001b[A\nBatch Processing:  24%|██▍       | 48/200 [08:42<28:00, 11.06s/it]\u001b[A\nBatch Processing:  24%|██▍       | 49/200 [08:53<27:29, 10.92s/it]\u001b[A\nBatch Processing:  25%|██▌       | 50/200 [09:03<26:41, 10.67s/it]\u001b[A\nBatch Processing:  26%|██▌       | 51/200 [09:14<26:19, 10.60s/it]\u001b[A\nBatch Processing:  26%|██▌       | 52/200 [09:23<25:25, 10.31s/it]\u001b[A\nBatch Processing:  26%|██▋       | 53/200 [09:34<25:54, 10.57s/it]\u001b[A\nBatch Processing:  27%|██▋       | 54/200 [09:45<26:01, 10.70s/it]\u001b[A\nBatch Processing:  28%|██▊       | 55/200 [09:57<26:23, 10.92s/it]\u001b[A\nBatch Processing:  28%|██▊       | 56/200 [10:07<25:46, 10.74s/it]\u001b[A\nBatch Processing:  28%|██▊       | 57/200 [10:18<25:40, 10.78s/it]\u001b[A\nBatch Processing:  29%|██▉       | 58/200 [10:28<24:59, 10.56s/it]\u001b[A\nBatch Processing:  30%|██▉       | 59/200 [10:40<25:36, 10.90s/it]\u001b[A\nBatch Processing:  30%|███       | 60/200 [10:51<25:51, 11.08s/it]\u001b[A\nBatch Processing:  30%|███       | 61/200 [11:02<25:46, 11.12s/it]\u001b[A\nBatch Processing:  31%|███       | 62/200 [11:14<26:00, 11.31s/it]\u001b[A\nBatch Processing:  32%|███▏      | 63/200 [11:24<25:04, 10.98s/it]\u001b[A\nBatch Processing:  32%|███▏      | 64/200 [11:34<24:02, 10.61s/it]\u001b[A\nBatch Processing:  32%|███▎      | 65/200 [11:46<24:39, 10.96s/it]\u001b[A\nBatch Processing:  33%|███▎      | 66/200 [11:57<24:47, 11.10s/it]\u001b[A\nBatch Processing:  34%|███▎      | 67/200 [12:08<24:29, 11.05s/it]\u001b[A\nBatch Processing:  34%|███▍      | 68/200 [12:18<23:43, 10.79s/it]\u001b[A\nBatch Processing:  34%|███▍      | 69/200 [12:28<22:49, 10.46s/it]\u001b[A\nBatch Processing:  35%|███▌      | 70/200 [12:38<21:57, 10.13s/it]\u001b[A\nBatch Processing:  36%|███▌      | 71/200 [12:48<21:44, 10.11s/it]\u001b[A\nBatch Processing:  36%|███▌      | 72/200 [12:58<21:51, 10.24s/it]\u001b[A\nBatch Processing:  36%|███▋      | 73/200 [13:09<21:53, 10.34s/it]\u001b[A\nBatch Processing:  37%|███▋      | 74/200 [13:19<21:55, 10.44s/it]\u001b[A\nBatch Processing:  38%|███▊      | 75/200 [13:30<21:51, 10.50s/it]\u001b[A\nBatch Processing:  38%|███▊      | 76/200 [13:41<21:58, 10.63s/it]\u001b[A\nBatch Processing:  38%|███▊      | 77/200 [13:52<21:55, 10.70s/it]\u001b[A\nBatch Processing:  39%|███▉      | 78/200 [14:02<21:26, 10.55s/it]\u001b[A\nBatch Processing:  40%|███▉      | 79/200 [14:12<21:12, 10.51s/it]\u001b[A\nBatch Processing:  40%|████      | 80/200 [14:24<21:43, 10.86s/it]\u001b[A\nBatch Processing:  40%|████      | 81/200 [14:35<21:25, 10.81s/it]\u001b[A\nBatch Processing:  41%|████      | 82/200 [14:45<20:52, 10.62s/it]\u001b[A\nBatch Processing:  42%|████▏     | 83/200 [14:56<21:05, 10.82s/it]\u001b[A\nBatch Processing:  42%|████▏     | 84/200 [15:07<20:57, 10.84s/it]\u001b[A\nBatch Processing:  42%|████▎     | 85/200 [15:18<20:43, 10.82s/it]\u001b[A\nBatch Processing:  43%|████▎     | 86/200 [15:29<20:41, 10.89s/it]\u001b[A\nBatch Processing:  44%|████▎     | 87/200 [15:39<20:02, 10.64s/it]\u001b[A\nBatch Processing:  44%|████▍     | 88/200 [15:50<19:48, 10.61s/it]\u001b[A\nBatch Processing:  44%|████▍     | 89/200 [16:01<19:59, 10.80s/it]\u001b[A\nBatch Processing:  45%|████▌     | 90/200 [16:12<19:51, 10.83s/it]\u001b[A\nBatch Processing:  46%|████▌     | 91/200 [16:22<19:31, 10.75s/it]\u001b[A\nBatch Processing:  46%|████▌     | 92/200 [16:33<19:28, 10.82s/it]\u001b[A\nBatch Processing:  46%|████▋     | 93/200 [16:44<19:06, 10.72s/it]\u001b[A\nBatch Processing:  47%|████▋     | 94/200 [16:55<19:10, 10.86s/it]\u001b[A\nBatch Processing:  48%|████▊     | 95/200 [17:06<19:20, 11.05s/it]\u001b[A\nBatch Processing:  48%|████▊     | 96/200 [17:18<19:24, 11.19s/it]\u001b[A\nBatch Processing:  48%|████▊     | 97/200 [17:30<19:26, 11.32s/it]\u001b[A\nBatch Processing:  49%|████▉     | 98/200 [17:40<18:55, 11.13s/it]\u001b[A\nBatch Processing:  50%|████▉     | 99/200 [17:51<18:29, 10.99s/it]\u001b[A\nBatch Processing:  50%|█████     | 100/200 [18:02<18:19, 11.00s/it]\u001b[A\nBatch Processing:  50%|█████     | 101/200 [18:13<18:22, 11.13s/it]\u001b[A\nBatch Processing:  51%|█████     | 102/200 [18:24<18:04, 11.07s/it]\u001b[A\nBatch Processing:  52%|█████▏    | 103/200 [18:35<17:50, 11.04s/it]\u001b[A\nBatch Processing:  52%|█████▏    | 104/200 [18:47<18:07, 11.32s/it]\u001b[A\nBatch Processing:  52%|█████▎    | 105/200 [18:57<17:11, 10.86s/it]\u001b[A\nBatch Processing:  53%|█████▎    | 106/200 [19:08<17:06, 10.92s/it]\u001b[A\nBatch Processing:  54%|█████▎    | 107/200 [19:19<17:07, 11.05s/it]\u001b[A\nBatch Processing:  54%|█████▍    | 108/200 [19:30<16:50, 10.99s/it]\u001b[A\nBatch Processing:  55%|█████▍    | 109/200 [19:41<16:21, 10.78s/it]\u001b[A\nBatch Processing:  55%|█████▌    | 110/200 [19:51<16:09, 10.77s/it]\u001b[A\nBatch Processing:  56%|█████▌    | 111/200 [20:01<15:40, 10.57s/it]\u001b[A\nBatch Processing:  56%|█████▌    | 112/200 [20:13<15:49, 10.79s/it]\u001b[A\nBatch Processing:  56%|█████▋    | 113/200 [20:24<15:54, 10.97s/it]\u001b[A\nBatch Processing:  57%|█████▋    | 114/200 [20:35<15:51, 11.06s/it]\u001b[A\nBatch Processing:  57%|█████▊    | 115/200 [20:46<15:23, 10.86s/it]\u001b[A\nBatch Processing:  58%|█████▊    | 116/200 [20:57<15:14, 10.88s/it]\u001b[A\nBatch Processing:  58%|█████▊    | 117/200 [21:07<14:45, 10.67s/it]\u001b[A\nBatch Processing:  59%|█████▉    | 118/200 [21:19<15:02, 11.01s/it]\u001b[A\nBatch Processing:  60%|█████▉    | 119/200 [21:30<15:07, 11.21s/it]\u001b[A\nBatch Processing:  60%|██████    | 120/200 [21:41<14:52, 11.15s/it]\u001b[A\nBatch Processing:  60%|██████    | 121/200 [21:52<14:31, 11.04s/it]\u001b[A\nBatch Processing:  61%|██████    | 122/200 [22:03<14:18, 11.00s/it]\u001b[A\nBatch Processing:  62%|██████▏   | 123/200 [22:14<14:04, 10.97s/it]\u001b[A\nBatch Processing:  62%|██████▏   | 124/200 [22:24<13:34, 10.72s/it]\u001b[A\nBatch Processing:  62%|██████▎   | 125/200 [22:35<13:23, 10.72s/it]\u001b[A\nBatch Processing:  63%|██████▎   | 126/200 [22:46<13:25, 10.88s/it]\u001b[A\nBatch Processing:  64%|██████▎   | 127/200 [22:57<13:14, 10.88s/it]\u001b[A\nBatch Processing:  64%|██████▍   | 128/200 [23:08<13:15, 11.04s/it]\u001b[A\nBatch Processing:  64%|██████▍   | 129/200 [23:19<13:02, 11.02s/it]\u001b[A\nBatch Processing:  65%|██████▌   | 130/200 [23:30<12:51, 11.02s/it]\u001b[A\nBatch Processing:  66%|██████▌   | 131/200 [23:41<12:39, 11.01s/it]\u001b[A\nBatch Processing:  66%|██████▌   | 132/200 [23:53<12:35, 11.10s/it]\u001b[A\nBatch Processing:  66%|██████▋   | 133/200 [24:04<12:25, 11.13s/it]\u001b[A\nBatch Processing:  67%|██████▋   | 134/200 [24:15<12:19, 11.21s/it]\u001b[A\nBatch Processing:  68%|██████▊   | 135/200 [24:27<12:14, 11.30s/it]\u001b[A\nBatch Processing:  68%|██████▊   | 136/200 [24:37<11:49, 11.09s/it]\u001b[A\nBatch Processing:  68%|██████▊   | 137/200 [24:48<11:35, 11.04s/it]\u001b[A\nBatch Processing:  69%|██████▉   | 138/200 [25:00<11:34, 11.20s/it]\u001b[A\nBatch Processing:  70%|██████▉   | 139/200 [25:11<11:25, 11.23s/it]\u001b[A\nBatch Processing:  70%|███████   | 140/200 [25:23<11:15, 11.26s/it]\u001b[A\nBatch Processing:  70%|███████   | 141/200 [25:35<11:27, 11.65s/it]\u001b[A\nBatch Processing:  71%|███████   | 142/200 [25:45<10:47, 11.17s/it]\u001b[A\nBatch Processing:  72%|███████▏  | 143/200 [25:56<10:38, 11.20s/it]\u001b[A\nBatch Processing:  72%|███████▏  | 144/200 [26:08<10:32, 11.30s/it]\u001b[A\nBatch Processing:  72%|███████▎  | 145/200 [26:19<10:17, 11.23s/it]\u001b[A\nBatch Processing:  73%|███████▎  | 146/200 [26:30<10:00, 11.11s/it]\u001b[A\nBatch Processing:  74%|███████▎  | 147/200 [26:40<09:28, 10.72s/it]\u001b[A\nBatch Processing:  74%|███████▍  | 148/200 [26:50<09:06, 10.52s/it]\u001b[A\nBatch Processing:  74%|███████▍  | 149/200 [27:00<08:53, 10.46s/it]\u001b[A\nBatch Processing:  75%|███████▌  | 150/200 [27:11<08:54, 10.69s/it]\u001b[A\nBatch Processing:  76%|███████▌  | 151/200 [27:22<08:51, 10.85s/it]\u001b[A\nBatch Processing:  76%|███████▌  | 152/200 [27:34<08:44, 10.93s/it]\u001b[A\nBatch Processing:  76%|███████▋  | 153/200 [27:44<08:19, 10.63s/it]\u001b[A\nBatch Processing:  77%|███████▋  | 154/200 [27:54<08:10, 10.67s/it]\u001b[A\nBatch Processing:  78%|███████▊  | 155/200 [28:06<08:14, 11.00s/it]\u001b[A\nBatch Processing:  78%|███████▊  | 156/200 [28:17<07:58, 10.88s/it]\u001b[A\nBatch Processing:  78%|███████▊  | 157/200 [28:28<07:50, 10.95s/it]\u001b[A\nBatch Processing:  79%|███████▉  | 158/200 [28:41<08:05, 11.56s/it]\u001b[A\nBatch Processing:  80%|███████▉  | 159/200 [28:52<07:46, 11.38s/it]\u001b[A\nBatch Processing:  80%|████████  | 160/200 [29:02<07:20, 11.01s/it]\u001b[A\nBatch Processing:  80%|████████  | 161/200 [29:13<07:14, 11.13s/it]\u001b[A\nBatch Processing:  81%|████████  | 162/200 [29:24<07:01, 11.09s/it]\u001b[A\nBatch Processing:  82%|████████▏ | 163/200 [29:36<06:52, 11.16s/it]\u001b[A\nBatch Processing:  82%|████████▏ | 164/200 [29:47<06:43, 11.21s/it]\u001b[A\nBatch Processing:  82%|████████▎ | 165/200 [29:58<06:29, 11.13s/it]\u001b[A\nBatch Processing:  83%|████████▎ | 166/200 [30:09<06:15, 11.03s/it]\u001b[A\nBatch Processing:  84%|████████▎ | 167/200 [30:20<06:06, 11.09s/it]\u001b[A\nBatch Processing:  84%|████████▍ | 168/200 [30:32<06:00, 11.27s/it]\u001b[A\nBatch Processing:  84%|████████▍ | 169/200 [30:43<05:46, 11.18s/it]\u001b[A\nBatch Processing:  85%|████████▌ | 170/200 [30:53<05:31, 11.07s/it]\u001b[A\nBatch Processing:  86%|████████▌ | 171/200 [31:05<05:26, 11.26s/it]\u001b[A\nBatch Processing:  86%|████████▌ | 172/200 [31:16<05:13, 11.19s/it]\u001b[A\nBatch Processing:  86%|████████▋ | 173/200 [31:28<05:05, 11.30s/it]\u001b[A\nBatch Processing:  87%|████████▋ | 174/200 [31:40<04:58, 11.50s/it]\u001b[A\nBatch Processing:  88%|████████▊ | 175/200 [31:51<04:47, 11.49s/it]\u001b[A\nBatch Processing:  88%|████████▊ | 176/200 [32:02<04:34, 11.43s/it]\u001b[A\nBatch Processing:  88%|████████▊ | 177/200 [32:14<04:22, 11.41s/it]\u001b[A\nBatch Processing:  89%|████████▉ | 178/200 [32:24<04:06, 11.19s/it]\u001b[A\nBatch Processing:  90%|████████▉ | 179/200 [32:36<03:57, 11.33s/it]\u001b[A\nBatch Processing:  90%|█████████ | 180/200 [32:47<03:47, 11.36s/it]\u001b[A\nBatch Processing:  90%|█████████ | 181/200 [32:59<03:38, 11.52s/it]\u001b[A\nBatch Processing:  91%|█████████ | 182/200 [33:10<03:23, 11.31s/it]\u001b[A\nBatch Processing:  92%|█████████▏| 183/200 [33:22<03:13, 11.37s/it]\u001b[A\nBatch Processing:  92%|█████████▏| 184/200 [33:33<02:59, 11.25s/it]\u001b[A\nBatch Processing:  92%|█████████▎| 185/200 [33:44<02:48, 11.26s/it]\u001b[A\nBatch Processing:  93%|█████████▎| 186/200 [33:56<02:39, 11.40s/it]\u001b[A\nBatch Processing:  94%|█████████▎| 187/200 [34:07<02:29, 11.48s/it]\u001b[A\nBatch Processing:  94%|█████████▍| 188/200 [34:18<02:16, 11.36s/it]\u001b[A\nBatch Processing:  94%|█████████▍| 189/200 [34:30<02:04, 11.31s/it]\u001b[A\nBatch Processing:  95%|█████████▌| 190/200 [34:42<01:55, 11.54s/it]\u001b[A\nBatch Processing:  96%|█████████▌| 191/200 [34:52<01:41, 11.27s/it]\u001b[A\nBatch Processing:  96%|█████████▌| 192/200 [35:04<01:30, 11.33s/it]\u001b[A\nBatch Processing:  96%|█████████▋| 193/200 [35:16<01:20, 11.55s/it]\u001b[A\nBatch Processing:  97%|█████████▋| 194/200 [35:27<01:07, 11.31s/it]\u001b[A\nBatch Processing:  98%|█████████▊| 195/200 [35:38<00:55, 11.18s/it]\u001b[A\nBatch Processing:  98%|█████████▊| 196/200 [35:49<00:45, 11.29s/it]\u001b[A\nBatch Processing:  98%|█████████▊| 197/200 [36:01<00:34, 11.37s/it]\u001b[A\nBatch Processing:  99%|█████████▉| 198/200 [36:12<00:22, 11.44s/it]\u001b[A\nBatch Processing: 100%|█████████▉| 199/200 [36:24<00:11, 11.60s/it]\u001b[A\nBatch Processing: 100%|██████████| 200/200 [36:35<00:00, 10.98s/it]\u001b[A\nProcessing Chunks: 100%|██████████| 11/11 [36:37<00:00, 199.78s/it]","output_type":"stream"},{"name":"stdout","text":"Saved labeled chunk to /kaggle/working/labeled_chunks/labeled_sent_chunks_chunk_8.parquet\nChunk /kaggle/input/pubmedabstracts/Chunks/sent_chunks_chunk_9.parquet already processed. Skipping.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import shutil\n\n# Zip the labeled_chunks folder\nshutil.make_archive(\"/kaggle/working/labeled_chunks\", 'zip', \"/kaggle/working/labeled_chunks\")\n\nprint(\"Folder zipped successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T15:09:30.916175Z","iopub.execute_input":"2025-01-11T15:09:30.916478Z","iopub.status.idle":"2025-01-11T15:09:32.031486Z","shell.execute_reply.started":"2025-01-11T15:09:30.916447Z","shell.execute_reply":"2025-01-11T15:09:32.030449Z"}},"outputs":[{"name":"stdout","text":"Folder zipped successfully!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\n\n# List files in the labeled_chunks folder\noutput_dir = \"/kaggle/working/labeled_chunks\"\nfiles = os.listdir(output_dir)\nprint(\"Files in labeled_chunks:\", files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T10:17:01.735168Z","iopub.execute_input":"2025-01-11T10:17:01.735468Z","iopub.status.idle":"2025-01-11T10:17:01.742013Z","shell.execute_reply.started":"2025-01-11T10:17:01.735437Z","shell.execute_reply":"2025-01-11T10:17:01.741059Z"}},"outputs":[{"name":"stdout","text":"Files in labeled_chunks: ['labeled_sent_chunks_chunk_3.parquet', 'labeled_sent_chunks_chunk_1.parquet', 'labeled_sent_chunks_chunk_9.parquet', 'labeled_sent_chunks_chunk_10.parquet', 'labeled_sent_chunks_chunk_7.parquet', 'labeled_sent_chunks_chunk_4.parquet', 'labeled_sent_chunks_chunk_11.parquet', 'labeled_sent_chunks_chunk_5.parquet', 'labeled_sent_chunks_chunk_2.parquet', 'labeled_sent_chunks_chunk_6.parquet']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# import os\n\n# # Path to the file you want to delete\n# file_to_delete = #\"/kaggle/working/labeled_chunks.zip\" #\"/kaggle/working/merged_labeled_chunks.parquet\"#\"/kaggle/working/labeled_chunks/chunk_8.parquet\"\n\n# # Check if the file exists and delete it\n# if os.path.exists(file_to_delete):\n#     os.remove(file_to_delete)\n#     print(f\"Deleted: {file_to_delete}\")\n# else:\n#     print(f\"File not found: {file_to_delete}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:16:19.793872Z","iopub.execute_input":"2025-01-11T13:16:19.794148Z","iopub.status.idle":"2025-01-11T13:16:19.799055Z","shell.execute_reply.started":"2025-01-11T13:16:19.794127Z","shell.execute_reply":"2025-01-11T13:16:19.798350Z"}},"outputs":[{"name":"stdout","text":"Deleted: /kaggle/working/labeled_chunks.zip\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# import pandas as pd\n# import os\n\n# # Paths to the files and folders\n# p5_file = \"/kaggle/input/pubmedabstracts/P5_final_new.parquet\"\n# labeled_dir = \"/kaggle/working/labeled_chunks\"\n# output_file = \"/kaggle/working/chunk_8.parquet\"\n\n# # Step 1: Load P5 file and add an auto-incremented index\n# p5_df = pd.read_parquet(p5_file)\n# p5_df = p5_df.reset_index(drop=True).reset_index()\n# p5_df.rename(columns={\"index\": \"auto_index\"}, inplace=True)\n# print(f\"Loaded P5 Parquet file with {len(p5_df)} rows and added auto-incremented index.\")\n\n# # Step 2: Load and combine all labeled chunks\n# labeled_dfs = []\n# for file_name in os.listdir(labeled_dir):\n#     if file_name.endswith(\".parquet\"):\n#         file_path = os.path.join(labeled_dir, file_name)\n#         print(f\"Reading labeled file: {file_path}\")\n#         labeled_dfs.append(pd.read_parquet(file_path))\n\n# labeled_df = pd.concat(labeled_dfs, ignore_index=True)\n# print(f\"Combined labeled DataFrame has {len(labeled_df)} rows.\")\n\n# # Step 3: Identify rows in P5 that are not in the labeled data\n# # Ensure the labeled data has 'UID' column for matching\n# if \"UID\" not in labeled_df.columns:\n#     raise ValueError(\"Labeled DataFrame must have a 'UID' column for matching.\")\n\n# labeled_uids = set(labeled_df[\"UID\"])\n# missing_rows = p5_df[~p5_df[\"uid\"].isin(labeled_uids)]\n# print(f\"Found {len(missing_rows)} missing rows in P5 file.\")\n\n# # Step 4: Create a new DataFrame with 'uid' and 'abstract' for the missing rows\n# remaining_rows = missing_rows[[\"uid\", \"abstract\"]]\n# print(f\"Remaining rows DataFrame created with {len(remaining_rows)} rows.\")\n\n# # Step 5: Save the remaining rows as chunk_8.parquet\n# remaining_rows.to_parquet(output_file, index=False)\n# print(f\"Remaining rows saved as {output_file}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:00:41.874666Z","iopub.execute_input":"2025-01-11T13:00:41.874944Z","iopub.status.idle":"2025-01-11T13:01:39.153106Z","shell.execute_reply.started":"2025-01-11T13:00:41.874924Z","shell.execute_reply":"2025-01-11T13:01:39.152414Z"}},"outputs":[{"name":"stdout","text":"Loaded P5 Parquet file with 1057871 rows and added auto-incremented index.\nReading labeled file: /kaggle/working/labeled_chunks/labeled_sent_chunks_chunk_3.parquet\nReading labeled file: /kaggle/working/labeled_chunks/labeled_sent_chunks_chunk_1.parquet\nReading labeled file: /kaggle/working/labeled_chunks/labeled_sent_chunks_chunk_9.parquet\nReading labeled file: /kaggle/working/labeled_chunks/labeled_sent_chunks_chunk_10.parquet\nReading labeled file: /kaggle/working/labeled_chunks/labeled_sent_chunks_chunk_7.parquet\nReading labeled file: /kaggle/working/labeled_chunks/labeled_sent_chunks_chunk_4.parquet\nReading labeled file: /kaggle/working/labeled_chunks/labeled_sent_chunks_chunk_11.parquet\nReading labeled file: /kaggle/working/labeled_chunks/labeled_sent_chunks_chunk_5.parquet\nReading labeled file: /kaggle/working/labeled_chunks/labeled_sent_chunks_chunk_2.parquet\nReading labeled file: /kaggle/working/labeled_chunks/labeled_sent_chunks_chunk_6.parquet\nCombined labeled DataFrame has 957871 rows.\nFound 100000 missing rows in P5 file.\nRemaining rows DataFrame created with 100000 rows.\nRemaining rows saved as /kaggle/working/chunk_8.parquet.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# missing_rows[[\"auto_index\", \"uid\", \"abstract\"]].head()a","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:01:39.154146Z","iopub.execute_input":"2025-01-11T13:01:39.154363Z","iopub.status.idle":"2025-01-11T13:01:39.167389Z","shell.execute_reply.started":"2025-01-11T13:01:39.154344Z","shell.execute_reply":"2025-01-11T13:01:39.166734Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"        auto_index       uid  \\\n700000      700000  27377686   \n700001      700001  27377678   \n700002      700002  27377577   \n700003      700003  27377169   \n700004      700004  27376900   \n\n                                                 abstract  \n700000  AIMS: Type 2 diabetes has grown to epidemic pr...  \n700001  SCOPE: Trimethylamine-N-oxide (TMAO), a metabo...  \n700002  General: A pilot study was conducted to assess...  \n700003  OBJECTIVES/HYPOTHESIS: Low-grade myofibroblast...  \n700004  General: Little is known about the relationshi...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auto_index</th>\n      <th>uid</th>\n      <th>abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>700000</th>\n      <td>700000</td>\n      <td>27377686</td>\n      <td>AIMS: Type 2 diabetes has grown to epidemic pr...</td>\n    </tr>\n    <tr>\n      <th>700001</th>\n      <td>700001</td>\n      <td>27377678</td>\n      <td>SCOPE: Trimethylamine-N-oxide (TMAO), a metabo...</td>\n    </tr>\n    <tr>\n      <th>700002</th>\n      <td>700002</td>\n      <td>27377577</td>\n      <td>General: A pilot study was conducted to assess...</td>\n    </tr>\n    <tr>\n      <th>700003</th>\n      <td>700003</td>\n      <td>27377169</td>\n      <td>OBJECTIVES/HYPOTHESIS: Low-grade myofibroblast...</td>\n    </tr>\n    <tr>\n      <th>700004</th>\n      <td>700004</td>\n      <td>27376900</td>\n      <td>General: Little is known about the relationshi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# missing_rows[[\"auto_index\", \"uid\", \"abstract\"]].tail()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:01:39.168826Z","iopub.execute_input":"2025-01-11T13:01:39.169037Z","iopub.status.idle":"2025-01-11T13:01:39.186317Z","shell.execute_reply.started":"2025-01-11T13:01:39.169018Z","shell.execute_reply":"2025-01-11T13:01:39.185709Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"        auto_index       uid  \\\n799995      799995  31548641   \n799996      799996  31548545   \n799997      799997  31547868   \n799998      799998  31547842   \n799999      799999  31547819   \n\n                                                 abstract  \n799995  General: Genome-wide association studies (GWAS...  \n799996  General: Breast cancer is the most frequent ma...  \n799997  General: The field of regenerative medicine pr...  \n799998  OBJECTIVE: Histopathological studies suggest t...  \n799999  BACKGROUND: Medulloblastoma (MB), the most com...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auto_index</th>\n      <th>uid</th>\n      <th>abstract</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>799995</th>\n      <td>799995</td>\n      <td>31548641</td>\n      <td>General: Genome-wide association studies (GWAS...</td>\n    </tr>\n    <tr>\n      <th>799996</th>\n      <td>799996</td>\n      <td>31548545</td>\n      <td>General: Breast cancer is the most frequent ma...</td>\n    </tr>\n    <tr>\n      <th>799997</th>\n      <td>799997</td>\n      <td>31547868</td>\n      <td>General: The field of regenerative medicine pr...</td>\n    </tr>\n    <tr>\n      <th>799998</th>\n      <td>799998</td>\n      <td>31547842</td>\n      <td>OBJECTIVE: Histopathological studies suggest t...</td>\n    </tr>\n    <tr>\n      <th>799999</th>\n      <td>799999</td>\n      <td>31547819</td>\n      <td>BACKGROUND: Medulloblastoma (MB), the most com...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# # Display the length of the DataFrame containing missing rows\n# missing_rows_subset = missing_rows[[\"auto_index\", \"uid\", \"abstract\"]]\n# print(f\"Length of missing rows (with auto_index, uid, and abstract): {len(missing_rows_subset)}\")\n\n# # Optional: Preview the DataFrame to ensure correctness\n# print(\"Preview of missing rows (with auto_index, uid, abstract):\")\n# print(missing_rows_subset.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-11T13:01:39.187164Z","iopub.execute_input":"2025-01-11T13:01:39.187465Z","iopub.status.idle":"2025-01-11T13:01:39.204742Z","shell.execute_reply.started":"2025-01-11T13:01:39.187435Z","shell.execute_reply":"2025-01-11T13:01:39.203990Z"}},"outputs":[{"name":"stdout","text":"Length of missing rows (with auto_index, uid, and abstract): 100000\nPreview of missing rows (with auto_index, uid, abstract):\n        auto_index       uid  \\\n700000      700000  27377686   \n700001      700001  27377678   \n700002      700002  27377577   \n700003      700003  27377169   \n700004      700004  27376900   \n\n                                                 abstract  \n700000  AIMS: Type 2 diabetes has grown to epidemic pr...  \n700001  SCOPE: Trimethylamine-N-oxide (TMAO), a metabo...  \n700002  General: A pilot study was conducted to assess...  \n700003  OBJECTIVES/HYPOTHESIS: Low-grade myofibroblast...  \n700004  General: Little is known about the relationshi...  \n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}