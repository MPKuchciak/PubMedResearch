{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 : CONVERTING DATA INTO PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 370 files to process from 1994 to 2024.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting JSON to Parquet: 100%|██████████| 370/370 [10:07<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting all records into one DataFrame and saving to Parquet...\n",
      "Saved to data/parquet_output\\all_results.parquet.\n",
      "Done converting JSON to Parquet.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm  # For nicer progress bars (optional)\n",
    "# pip install tqdm if you don't have it already\n",
    "\n",
    "def flatten_article(article, file_date_str):\n",
    "    \"\"\"\n",
    "    Flatten a single article's nested fields (abstract_sections, authors, mesh_terms, etc.)\n",
    "    into a dictionary of simple columns.\n",
    "    \"\"\"\n",
    "    # 1) Abstract\n",
    "    abstract_sections = article.get(\"abstract_sections\", [])\n",
    "    # Join each abstract section into a single multi-paragraph string\n",
    "    # including the label, if desired\n",
    "    if abstract_sections:\n",
    "        abstract = \"\\n\\n\".join([\n",
    "            f\"{section.get('label', '')}: {section.get('text', '').strip()}\"\n",
    "            for section in abstract_sections\n",
    "            if section.get(\"text\")\n",
    "        ]).strip()\n",
    "    else:\n",
    "        abstract = None\n",
    "\n",
    "    # 2) Authors\n",
    "    authors_list = article.get(\"authors\", [])\n",
    "    authors = \"; \".join(\n",
    "        [a.get(\"name\", \"\") for a in authors_list if a.get(\"name\")]\n",
    "    )\n",
    "    # Flatten affiliations as well\n",
    "    affiliations = \"; \".join(\n",
    "        [a.get(\"affiliation\", \"\") for a in authors_list if a.get(\"affiliation\")]\n",
    "    )\n",
    "\n",
    "    # 3) Mesh terms\n",
    "    mesh_list = article.get(\"mesh_terms\", [])\n",
    "    mesh_terms = \"; \".join(\n",
    "        [m.get(\"descriptor\", \"\") for m in mesh_list if m.get(\"descriptor\")]\n",
    "    )\n",
    "\n",
    "    # 4) Keywords\n",
    "    keywords_list = article.get(\"keywords\", [])\n",
    "    # Convert any None to an empty string\n",
    "    keywords_list = [k if k is not None else \"\" for k in keywords_list]\n",
    "    keywords_str = \"; \".join(keywords_list)\n",
    "\n",
    "\n",
    "    # 5) Build the flattened record\n",
    "    record = {\n",
    "        \"uid\": article.get(\"uid\"),\n",
    "        \"title\": article.get(\"title\"),\n",
    "        \"journal\": article.get(\"journal\"),\n",
    "        \"pubdate\": article.get(\"pubdate\"),    # e.g. \"1996-Aug-01\"\n",
    "        \"abstract\": abstract,\n",
    "        \"authors\": authors,\n",
    "        \"affiliations\": affiliations,\n",
    "        \"mesh_terms\": mesh_terms,\n",
    "        \"keywords\": keywords_str,\n",
    "        \"coi_statement\": article.get(\"coi_statement\"),\n",
    "        # Add the date derived from filename/folder\n",
    "        \"date\": file_date_str\n",
    "    }\n",
    "    return record\n",
    "\n",
    "def get_year_month_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extract year and month from filename like 'results_1996_10.json'\n",
    "    Returns (year, month) as strings.\n",
    "    \"\"\"\n",
    "    # You could also parse with a regex or with split\n",
    "    match = re.search(r\"results_(\\d{4})_(\\d{2})\\.json\", filename)\n",
    "    if match:\n",
    "        year = match.group(1)\n",
    "        month = match.group(2)\n",
    "        return year, month\n",
    "    return None, None\n",
    "\n",
    "def convert_jsons_to_parquet(\n",
    "    input_folder: str,\n",
    "    output_folder: str,\n",
    "    start_year: int = 1994,\n",
    "    end_year: int = 2024,\n",
    "    one_big_file: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Read all results_YYYY_MM.json files from input_folder, flatten them,\n",
    "    and save to Parquet. If one_big_file=True, everything goes into one file,\n",
    "    otherwise it saves one Parquet per month.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    - input_folder  : str  => Path to your data/results folder\n",
    "    - output_folder : str  => Where to write the Parquet(s)\n",
    "    - start_year, end_year : int => Range of years to look for\n",
    "    - one_big_file : bool  => True = single Parquet file, \n",
    "                              False = one Parquet per year-month\n",
    "    \"\"\"\n",
    "    # Make sure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Gather all possible file paths\n",
    "    file_paths = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            filename = f\"results_{year}_{month:02d}.json\"\n",
    "            full_path = os.path.join(input_folder, filename)\n",
    "            if os.path.exists(full_path):\n",
    "                file_paths.append(full_path)\n",
    "\n",
    "    if not file_paths:\n",
    "        print(\"No files found in the specified range. Aborting.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(file_paths)} files to process from {start_year} to {end_year}.\")\n",
    "\n",
    "    # We will accumulate data in a DataFrame if one_big_file is True,\n",
    "    # otherwise, we process each month separately.\n",
    "    all_records = []  # Only used if one_big_file is True\n",
    "\n",
    "    # tqdm is optional for progress bars. If you prefer basic prints, remove it.\n",
    "    for file_path in tqdm(file_paths, desc=\"Converting JSON to Parquet\"):\n",
    "        filename = os.path.basename(file_path)\n",
    "        year_str, month_str = get_year_month_from_filename(filename)\n",
    "\n",
    "        # e.g. \"1996-10-01\"\n",
    "        date_str = f\"{year_str}-{month_str}-01\" if year_str and month_str else None\n",
    "\n",
    "        # Load the JSON\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)  # data is a list of articles\n",
    "\n",
    "        # Flatten each article\n",
    "        month_records = []\n",
    "        for article in data:\n",
    "            record = flatten_article(article, date_str)\n",
    "            month_records.append(record)\n",
    "\n",
    "        # If writing monthly Parquet\n",
    "        if not one_big_file:\n",
    "            # Convert month_records -> DataFrame -> save to Parquet\n",
    "            df = pd.DataFrame(month_records)\n",
    "\n",
    "            # e.g. data/parquet/results_1996_10.parquet\n",
    "            out_file = os.path.join(\n",
    "                output_folder, f\"results_{year_str}_{month_str}.parquet\"\n",
    "            )\n",
    "            df.to_parquet(out_file, index=False)\n",
    "        else:\n",
    "            # Accumulate in memory\n",
    "            all_records.extend(month_records)\n",
    "\n",
    "    if one_big_file:\n",
    "        print(f\"\\nConverting all records into one DataFrame and saving to Parquet...\")\n",
    "        df_all = pd.DataFrame(all_records)\n",
    "        out_file = os.path.join(output_folder, \"all_results.parquet\")\n",
    "        df_all.to_parquet(out_file, index=False)\n",
    "        print(f\"Saved to {out_file}.\")\n",
    "\n",
    "    print(\"Done converting JSON to Parquet.\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    results_folder = \"data/results\"        # Folder containing results_YYYY_MM.json\n",
    "    output_folder = \"data/parquet_output\"  # Where to save Parquet files\n",
    "    convert_jsons_to_parquet(\n",
    "        input_folder=results_folder,\n",
    "        output_folder=output_folder,\n",
    "        start_year=1994,\n",
    "        end_year=2024,\n",
    "        one_big_file=True  # or True for a single file\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>keywords</th>\n",
       "      <th>coi_statement</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10150804</td>\n",
       "      <td>Evaluation of survival in medically treated pa...</td>\n",
       "      <td>Journal of insurance medicine (New York, N.Y.)</td>\n",
       "      <td>None-01-01</td>\n",
       "      <td>General: Articles published in medical journal...</td>\n",
       "      <td>Iacovino J R</td>\n",
       "      <td>New York Life Insurance Company, New York, USA.</td>\n",
       "      <td>Adolescent; Adult; Age Distribution; Chelation...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1994-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9061841</td>\n",
       "      <td>Cohort versus cross-sectional design in large ...</td>\n",
       "      <td>Statistics in medicine</td>\n",
       "      <td>1994-Jan-15</td>\n",
       "      <td>General: In planning large longitudinal field ...</td>\n",
       "      <td>Feldman H A; McKinlay S M</td>\n",
       "      <td>New England Research Institute, Inc., Watertow...</td>\n",
       "      <td>Analysis of Variance; Cluster Analysis; Cohort...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1994-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9061840</td>\n",
       "      <td>Network analytic methods for epidemiological r...</td>\n",
       "      <td>Statistics in medicine</td>\n",
       "      <td>1994-Jan-15</td>\n",
       "      <td>General: The authors measure the efficacy of t...</td>\n",
       "      <td>Altmann M; Wee B C; Willard K; Peterson D; Gat...</td>\n",
       "      <td>Division of Health Computer Sciences, Universi...</td>\n",
       "      <td>Analysis of Variance; Communicable Disease Con...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1994-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9061838</td>\n",
       "      <td>Estimating age, period and cohort effects usin...</td>\n",
       "      <td>Statistics in medicine</td>\n",
       "      <td>1994-Jan-15</td>\n",
       "      <td>General: To understand cancer aetiology better...</td>\n",
       "      <td>Holford T R; Zhang Z; McKay L A</td>\n",
       "      <td>Department of Epidemiology and Public Health, ...</td>\n",
       "      <td>Age Factors; Cell Transformation, Neoplastic; ...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1994-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16353609</td>\n",
       "      <td>A perspective on the hormonal abnormalities of...</td>\n",
       "      <td>Obesity research</td>\n",
       "      <td>1994-Jan-01</td>\n",
       "      <td>General: Studies in our laboratory and elsewhe...</td>\n",
       "      <td>Zumoff B; Strain G W</td>\n",
       "      <td>Division of Endocrinology and Metabolism, Depa...</td>\n",
       "      <td>Adolescent; Adult; Estradiol; Estrone; Female;...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1994-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid                                              title  \\\n",
       "0  10150804  Evaluation of survival in medically treated pa...   \n",
       "1   9061841  Cohort versus cross-sectional design in large ...   \n",
       "2   9061840  Network analytic methods for epidemiological r...   \n",
       "3   9061838  Estimating age, period and cohort effects usin...   \n",
       "4  16353609  A perspective on the hormonal abnormalities of...   \n",
       "\n",
       "                                          journal      pubdate  \\\n",
       "0  Journal of insurance medicine (New York, N.Y.)   None-01-01   \n",
       "1                          Statistics in medicine  1994-Jan-15   \n",
       "2                          Statistics in medicine  1994-Jan-15   \n",
       "3                          Statistics in medicine  1994-Jan-15   \n",
       "4                                Obesity research  1994-Jan-01   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  General: Articles published in medical journal...   \n",
       "1  General: In planning large longitudinal field ...   \n",
       "2  General: The authors measure the efficacy of t...   \n",
       "3  General: To understand cancer aetiology better...   \n",
       "4  General: Studies in our laboratory and elsewhe...   \n",
       "\n",
       "                                             authors  \\\n",
       "0                                       Iacovino J R   \n",
       "1                          Feldman H A; McKinlay S M   \n",
       "2  Altmann M; Wee B C; Willard K; Peterson D; Gat...   \n",
       "3                    Holford T R; Zhang Z; McKay L A   \n",
       "4                               Zumoff B; Strain G W   \n",
       "\n",
       "                                        affiliations  \\\n",
       "0    New York Life Insurance Company, New York, USA.   \n",
       "1  New England Research Institute, Inc., Watertow...   \n",
       "2  Division of Health Computer Sciences, Universi...   \n",
       "3  Department of Epidemiology and Public Health, ...   \n",
       "4  Division of Endocrinology and Metabolism, Depa...   \n",
       "\n",
       "                                          mesh_terms keywords coi_statement  \\\n",
       "0  Adolescent; Adult; Age Distribution; Chelation...                    N/A   \n",
       "1  Analysis of Variance; Cluster Analysis; Cohort...                    N/A   \n",
       "2  Analysis of Variance; Communicable Disease Con...                    N/A   \n",
       "3  Age Factors; Cell Transformation, Neoplastic; ...                    N/A   \n",
       "4  Adolescent; Adult; Estradiol; Estrone; Female;...                    N/A   \n",
       "\n",
       "         date  \n",
       "0  1994-01-01  \n",
       "1  1994-01-01  \n",
       "2  1994-01-01  \n",
       "3  1994-01-01  \n",
       "4  1994-01-01  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data check (3 min)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"data/parquet_output/all_results.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: REPAIRING DATE + REMOVING DUPLICATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data/parquet_output\\all_results.parquet...\n",
      "Loaded 1460893 rows from data/parquet_output\\all_results.parquet.\n",
      "Total rows: 1460893\n",
      "Unique UIDs: 1059761\n",
      "Duplicate rows (by uid): 401132\n",
      "\n",
      "Example duplicates:\n",
      "['11250692' '11250688' '11094416' '11094415' '11094414' '10854520'\n",
      " '10770738' '10739772' '10739759' '10739758']\n",
      "Saved cleaned data to data/cleaned_parquet/final\\PubMedAbstracts_final.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_pubdate(pubdate_str: str):\n",
    "    \"\"\"\n",
    "    Convert a pubdate like '1994-Jan-15' or '1994-Jan' or '1994' into 'YYYY-MM-DD'.\n",
    "    If anything is missing or invalid, return None.\n",
    "    \"\"\"\n",
    "    if not pubdate_str:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns you might see:\n",
    "    # 1) YYYY-Mmm-DD  (e.g. 1994-Jan-15)\n",
    "    # 2) YYYY-Mmm      (e.g. 1994-Jan)\n",
    "    # 3) YYYY          (e.g. 1994)\n",
    "    # 4) Something like 'None-01-01' which is invalid\n",
    "    # 5) Possibly 'N/A', 'NA', etc.\n",
    "\n",
    "    # Quick check for 'None', 'NA', or obviously invalid\n",
    "    if 'None' in pubdate_str or 'NA' in pubdate_str:\n",
    "        return None\n",
    "\n",
    "    # We'll try multiple parsing patterns. The easiest is to replace\n",
    "    # short month name with numeric month via strptime if it follows \n",
    "    # the pattern 'YYYY-%b-%d' or 'YYYY-%b' etc.\n",
    "    # We'll guess day=01 if missing.\n",
    "\n",
    "    # 1) Attempt full pattern: YYYY-%b-%d (e.g. 1994-Jan-15)\n",
    "    # 2) If that fails, attempt YYYY-%b (e.g. 1994-Jan, assume day=01)\n",
    "    # 3) If that fails, attempt just YYYY (assume month=01, day=01)\n",
    "\n",
    "    # We'll do this in a small try/except chain:\n",
    "    for fmt in [\"%Y-%b-%d\", \"%Y-%b\", \"%Y\"]:\n",
    "        try:\n",
    "            dt = datetime.strptime(pubdate_str, fmt)\n",
    "            # If the format was just Year or Year+Month, \n",
    "            # dt will default day=1 or month=January if missing\n",
    "            return dt.strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # If everything fails, return None\n",
    "    return None\n",
    "\n",
    "def create_parsed_date_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a new column 'parsed_date' based on the logic:\n",
    "      - If pubdate is valid, parse it\n",
    "      - Otherwise, use the file-based date\n",
    "    \"\"\"\n",
    "    # We'll create a new column by applying parse_pubdate to each pubdate row\n",
    "    df[\"temp_parsed_pubdate\"] = df[\"pubdate\"].apply(parse_pubdate)\n",
    "\n",
    "    # Now we combine\n",
    "    # if temp_parsed_pubdate is None, use the 'date' column\n",
    "    # else use the parsed pubdate\n",
    "    def pick_date(row):\n",
    "        pubd = row[\"temp_parsed_pubdate\"]\n",
    "        file_d = row[\"date\"]  # e.g. '1994-01-01'\n",
    "        if pubd is not None:\n",
    "            return pubd\n",
    "        else:\n",
    "            return file_d\n",
    "    \n",
    "    df[\"parsed_date\"] = df.apply(pick_date, axis=1)\n",
    "\n",
    "    # You can drop the temp column\n",
    "    df.drop(columns=[\"temp_parsed_pubdate\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def check_duplicates(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Print duplicate stats based on 'uid'.\n",
    "    Doesn't remove them, just prints summary.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    unique_uids = df[\"uid\"].nunique()\n",
    "    dup_count = total_rows - unique_uids\n",
    "\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Unique UIDs: {unique_uids}\")\n",
    "    print(f\"Duplicate rows (by uid): {dup_count}\")\n",
    "\n",
    "    if dup_count > 0:\n",
    "        print(\"\\nExample duplicates:\")\n",
    "        # Find which UIDs are duplicated\n",
    "        duplicated_uids = df[\"uid\"][df[\"uid\"].duplicated()].unique()\n",
    "        print(duplicated_uids[:10])  # print first 10 duplicates for inspection\n",
    "\n",
    "def clean_parquet_file(input_parquet: str, output_parquet: str):\n",
    "    \"\"\"\n",
    "    Load a Parquet file, create 'parsed_date', check duplicates, then save result.\n",
    "    \"\"\"\n",
    "    print(f\"\\nReading {input_parquet}...\")\n",
    "    df = pd.read_parquet(input_parquet)\n",
    "    print(f\"Loaded {len(df)} rows from {input_parquet}.\")\n",
    "\n",
    "    # 1. Create the parsed date column\n",
    "    df = create_parsed_date_column(df)\n",
    "\n",
    "    # 2. Check duplicates\n",
    "    check_duplicates(df)\n",
    "\n",
    "    # 3. Remove duplicates by uid (keeping the last occurrence, for example)\n",
    "    df_dedup = df.drop_duplicates(subset=[\"uid\"], keep=\"last\") # after checking how deduplication works uid the same as \n",
    "\n",
    "    # 4. Save it back\n",
    "    df_dedup.to_parquet(output_parquet, index=False)\n",
    "    print(f\"Saved cleaned data to {output_parquet}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Suppose you have a single big Parquet from 1994-1998,\n",
    "    # or monthly Parquets. Example of whole approach:\n",
    "    import glob\n",
    "    input_folder = \"data/parquet_output\"\n",
    "    output_folder = \"data/cleaned_parquet/final\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Let’s process all .parquet in input_folder\n",
    "    parquet_files = glob.glob(os.path.join(input_folder, \"*.parquet\"))\n",
    "    \n",
    "    for infile in parquet_files:\n",
    "        fname = \"final\" #os.path.basename(infile)\n",
    "        outfile = os.path.join(output_folder, f\"PubMedAbstracts_{fname}.parquet\")\n",
    "        clean_parquet_file(infile, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>keywords</th>\n",
       "      <th>coi_statement</th>\n",
       "      <th>date</th>\n",
       "      <th>parsed_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10150804</td>\n",
       "      <td>Evaluation of survival in medically treated pa...</td>\n",
       "      <td>Journal of insurance medicine (New York, N.Y.)</td>\n",
       "      <td>None-01-01</td>\n",
       "      <td>General: Articles published in medical journal...</td>\n",
       "      <td>Iacovino J R</td>\n",
       "      <td>New York Life Insurance Company, New York, USA.</td>\n",
       "      <td>Adolescent; Adult; Age Distribution; Chelation...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>1994-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9061841</td>\n",
       "      <td>Cohort versus cross-sectional design in large ...</td>\n",
       "      <td>Statistics in medicine</td>\n",
       "      <td>1994-Jan-15</td>\n",
       "      <td>General: In planning large longitudinal field ...</td>\n",
       "      <td>Feldman H A; McKinlay S M</td>\n",
       "      <td>New England Research Institute, Inc., Watertow...</td>\n",
       "      <td>Analysis of Variance; Cluster Analysis; Cohort...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>1994-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9061840</td>\n",
       "      <td>Network analytic methods for epidemiological r...</td>\n",
       "      <td>Statistics in medicine</td>\n",
       "      <td>1994-Jan-15</td>\n",
       "      <td>General: The authors measure the efficacy of t...</td>\n",
       "      <td>Altmann M; Wee B C; Willard K; Peterson D; Gat...</td>\n",
       "      <td>Division of Health Computer Sciences, Universi...</td>\n",
       "      <td>Analysis of Variance; Communicable Disease Con...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>1994-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9061838</td>\n",
       "      <td>Estimating age, period and cohort effects usin...</td>\n",
       "      <td>Statistics in medicine</td>\n",
       "      <td>1994-Jan-15</td>\n",
       "      <td>General: To understand cancer aetiology better...</td>\n",
       "      <td>Holford T R; Zhang Z; McKay L A</td>\n",
       "      <td>Department of Epidemiology and Public Health, ...</td>\n",
       "      <td>Age Factors; Cell Transformation, Neoplastic; ...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>1994-01-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16353609</td>\n",
       "      <td>A perspective on the hormonal abnormalities of...</td>\n",
       "      <td>Obesity research</td>\n",
       "      <td>1994-Jan-01</td>\n",
       "      <td>General: Studies in our laboratory and elsewhe...</td>\n",
       "      <td>Zumoff B; Strain G W</td>\n",
       "      <td>Division of Endocrinology and Metabolism, Depa...</td>\n",
       "      <td>Adolescent; Adult; Estradiol; Estrone; Female;...</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1994-01-01</td>\n",
       "      <td>1994-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid                                              title  \\\n",
       "0  10150804  Evaluation of survival in medically treated pa...   \n",
       "1   9061841  Cohort versus cross-sectional design in large ...   \n",
       "2   9061840  Network analytic methods for epidemiological r...   \n",
       "3   9061838  Estimating age, period and cohort effects usin...   \n",
       "4  16353609  A perspective on the hormonal abnormalities of...   \n",
       "\n",
       "                                          journal      pubdate  \\\n",
       "0  Journal of insurance medicine (New York, N.Y.)   None-01-01   \n",
       "1                          Statistics in medicine  1994-Jan-15   \n",
       "2                          Statistics in medicine  1994-Jan-15   \n",
       "3                          Statistics in medicine  1994-Jan-15   \n",
       "4                                Obesity research  1994-Jan-01   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  General: Articles published in medical journal...   \n",
       "1  General: In planning large longitudinal field ...   \n",
       "2  General: The authors measure the efficacy of t...   \n",
       "3  General: To understand cancer aetiology better...   \n",
       "4  General: Studies in our laboratory and elsewhe...   \n",
       "\n",
       "                                             authors  \\\n",
       "0                                       Iacovino J R   \n",
       "1                          Feldman H A; McKinlay S M   \n",
       "2  Altmann M; Wee B C; Willard K; Peterson D; Gat...   \n",
       "3                    Holford T R; Zhang Z; McKay L A   \n",
       "4                               Zumoff B; Strain G W   \n",
       "\n",
       "                                        affiliations  \\\n",
       "0    New York Life Insurance Company, New York, USA.   \n",
       "1  New England Research Institute, Inc., Watertow...   \n",
       "2  Division of Health Computer Sciences, Universi...   \n",
       "3  Department of Epidemiology and Public Health, ...   \n",
       "4  Division of Endocrinology and Metabolism, Depa...   \n",
       "\n",
       "                                          mesh_terms keywords coi_statement  \\\n",
       "0  Adolescent; Adult; Age Distribution; Chelation...                    N/A   \n",
       "1  Analysis of Variance; Cluster Analysis; Cohort...                    N/A   \n",
       "2  Analysis of Variance; Communicable Disease Con...                    N/A   \n",
       "3  Age Factors; Cell Transformation, Neoplastic; ...                    N/A   \n",
       "4  Adolescent; Adult; Estradiol; Estrone; Female;...                    N/A   \n",
       "\n",
       "         date parsed_date  \n",
       "0  1994-01-01  1994-01-01  \n",
       "1  1994-01-01  1994-01-15  \n",
       "2  1994-01-01  1994-01-15  \n",
       "3  1994-01-01  1994-01-15  \n",
       "4  1994-01-01  1994-01-01  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"data/cleaned_parquet/final/PubMedAbstracts_final.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1059761"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: DEDUPLICATION CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading data/parquet_output\\all_results.parquet...\n",
      "Loaded 1460893 rows from data/parquet_output\\all_results.parquet.\n",
      "Total rows: 1460893\n",
      "Unique UIDs: 1059761\n",
      "Duplicate rows (by uid): 401132\n",
      "\n",
      "Example duplicates:\n",
      "['11250692' '11250688' '11094416' '11094415' '11094414' '10854520'\n",
      " '10770738' '10739772' '10739759' '10739758']\n",
      "Saved cleaned data to data/cleaned_parquet\\cleaned_nodedup_all_results.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_pubdate(pubdate_str: str):\n",
    "    \"\"\"\n",
    "    Convert a pubdate like '1994-Jan-15' or '1994-Jan' or '1994' into 'YYYY-MM-DD'.\n",
    "    If anything is missing or invalid, return None.\n",
    "    \"\"\"\n",
    "    if not pubdate_str:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns you might see:\n",
    "    # 1) YYYY-Mmm-DD  (e.g. 1994-Jan-15)\n",
    "    # 2) YYYY-Mmm      (e.g. 1994-Jan)\n",
    "    # 3) YYYY          (e.g. 1994)\n",
    "    # 4) Something like 'None-01-01' which is invalid\n",
    "    # 5) Possibly 'N/A', 'NA', etc.\n",
    "\n",
    "    # Quick check for 'None', 'NA', or obviously invalid\n",
    "    if 'None' in pubdate_str or 'NA' in pubdate_str:\n",
    "        return None\n",
    "\n",
    "    # We'll try multiple parsing patterns. The easiest is to replace\n",
    "    # short month name with numeric month via strptime if it follows \n",
    "    # the pattern 'YYYY-%b-%d' or 'YYYY-%b' etc.\n",
    "    # We'll guess day=01 if missing.\n",
    "\n",
    "    # 1) Attempt full pattern: YYYY-%b-%d (e.g. 1994-Jan-15)\n",
    "    # 2) If that fails, attempt YYYY-%b (e.g. 1994-Jan, assume day=01)\n",
    "    # 3) If that fails, attempt just YYYY (assume month=01, day=01)\n",
    "\n",
    "    # We'll do this in a small try/except chain:\n",
    "    for fmt in [\"%Y-%b-%d\", \"%Y-%b\", \"%Y\"]:\n",
    "        try:\n",
    "            dt = datetime.strptime(pubdate_str, fmt)\n",
    "            # If the format was just Year or Year+Month, \n",
    "            # dt will default day=1 or month=January if missing\n",
    "            return dt.strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # If everything fails, return None\n",
    "    return None\n",
    "\n",
    "def create_parsed_date_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a new column 'parsed_date' based on the logic:\n",
    "      - If pubdate is valid, parse it\n",
    "      - Otherwise, use the file-based date\n",
    "    \"\"\"\n",
    "    # We'll create a new column by applying parse_pubdate to each pubdate row\n",
    "    df[\"temp_parsed_pubdate\"] = df[\"pubdate\"].apply(parse_pubdate)\n",
    "\n",
    "    # Now we combine\n",
    "    # if temp_parsed_pubdate is None, use the 'date' column\n",
    "    # else use the parsed pubdate\n",
    "    def pick_date(row):\n",
    "        pubd = row[\"temp_parsed_pubdate\"]\n",
    "        file_d = row[\"date\"]  # e.g. '1994-01-01'\n",
    "        if pubd is not None:\n",
    "            return pubd\n",
    "        else:\n",
    "            return file_d\n",
    "    \n",
    "    df[\"parsed_date\"] = df.apply(pick_date, axis=1)\n",
    "\n",
    "    # You can drop the temp column\n",
    "    df.drop(columns=[\"temp_parsed_pubdate\"], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def check_duplicates(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Print duplicate stats based on 'uid'.\n",
    "    Doesn't remove them, just prints summary.\n",
    "    \"\"\"\n",
    "    total_rows = len(df)\n",
    "    unique_uids = df[\"uid\"].nunique()\n",
    "    dup_count = total_rows - unique_uids\n",
    "\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    print(f\"Unique UIDs: {unique_uids}\")\n",
    "    print(f\"Duplicate rows (by uid): {dup_count}\")\n",
    "\n",
    "    if dup_count > 0:\n",
    "        print(\"\\nExample duplicates:\")\n",
    "        # Find which UIDs are duplicated\n",
    "        duplicated_uids = df[\"uid\"][df[\"uid\"].duplicated()].unique()\n",
    "        print(duplicated_uids[:10])  # print first 10 duplicates for inspection\n",
    "\n",
    "def clean_parquet_file(input_parquet: str, output_parquet: str):\n",
    "    \"\"\"\n",
    "    Load a Parquet file, create 'parsed_date', check duplicates, then save result.\n",
    "    \"\"\"\n",
    "    print(f\"\\nReading {input_parquet}...\")\n",
    "    df = pd.read_parquet(input_parquet)\n",
    "    print(f\"Loaded {len(df)} rows from {input_parquet}.\")\n",
    "\n",
    "    # 1. Create the parsed date column\n",
    "    df = create_parsed_date_column(df)\n",
    "\n",
    "    # 2. Check duplicates\n",
    "    check_duplicates(df)\n",
    "\n",
    "    # 3. Remove duplicates by uid (keeping the last occurrence, for example)\n",
    "    #df_dedup = df.drop_duplicates(subset=[\"uid\"], keep=\"last\") #after checking how deduplication works \n",
    "\n",
    "    # 4. Save it back\n",
    "    df.to_parquet(output_parquet, index=False)\n",
    "    print(f\"Saved cleaned data to {output_parquet}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Suppose you have a single big Parquet from 1994-1998,\n",
    "    # or monthly Parquets. Example of whole approach:\n",
    "    import glob\n",
    "    input_folder = \"data/parquet_output\"\n",
    "    output_folder = \"data/cleaned_parquet\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Let’s process all .parquet in input_folder\n",
    "    parquet_files = glob.glob(os.path.join(input_folder, \"*.parquet\"))\n",
    "    \n",
    "    for infile in parquet_files:\n",
    "        fname = os.path.basename(infile)\n",
    "        outfile = os.path.join(output_folder, f\"cleaned_nodedup_{fname}\")\n",
    "        clean_parquet_file(infile, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: DUPLICATES CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1460893 rows from data/parquet_output/all_results.parquet.\n",
      "\n",
      "=== Dedup by [uid] only ===\n",
      "Rows before: 1460893\n",
      "Rows after:  1059761\n",
      "Rows removed: 401132\n",
      "\n",
      "Rows actually removed when deduplicating by [uid]: 401132\n",
      "\n",
      "=== Dedup by ['uid', 'abstract', 'title', 'pubdate', 'mesh_terms', 'journal'] ===\n",
      "Rows before: 1460893\n",
      "Rows after:  1059761\n",
      "Rows removed: 401132\n",
      "\n",
      "Rows actually removed when deduplicating by ['uid', 'abstract', 'title', 'pubdate', 'mesh_terms', 'journal']: 401132\n",
      "\n",
      "=== Comparison of removed rows ===\n",
      "Removed only by [uid] dedup:        0\n",
      "Removed only by ['uid', 'abstract', 'title', 'pubdate', 'mesh_terms', 'journal'] dedup: 0\n",
      "Removed by BOTH methods:            401132\n",
      "\n",
      "Example rows removed only by [uid] approach (first 5):\n",
      "Empty DataFrame\n",
      "Columns: [uid, title, journal, pubdate, abstract, authors, affiliations, mesh_terms, keywords, coi_statement, date]\n",
      "Index: []\n",
      "\n",
      "Example rows removed only by multi-col approach (first 5):\n",
      "Empty DataFrame\n",
      "Columns: [uid, title, journal, pubdate, abstract, authors, affiliations, mesh_terms, keywords, coi_statement, date]\n",
      "Index: []\n",
      "\n",
      "Example rows removed by BOTH (first 5):\n",
      "              uid                                              title  \\\n",
      "1048576  31216882  The Easier the Better? Comparing the Readabili...   \n",
      "1048577  31216874  Glucose-Responsive Insulin Through Bioconjugat...   \n",
      "1048578  31216871  \"Everyone thinks I am just lazy\": Legitimacy n...   \n",
      "1048582  31216602  Ecthyma gangrenosum in a 3-year-old boy post-h...   \n",
      "1048588  31216442  Improving homology-directed repair efficiency ...   \n",
      "\n",
      "                                                   journal      pubdate  \\\n",
      "1048576  Health education & behavior : the official pub...  2019-Oct-01   \n",
      "1048577         Journal of diabetes science and technology  2020-Mar-01   \n",
      "1048578                    Health (London, England : 1997)  2021-Jan-01   \n",
      "1048582  Transplant infectious disease : an official jo...  2019-Aug-01   \n",
      "1048588                        Methods (San Diego, Calif.)  2019-Jul-15   \n",
      "\n",
      "                                                  abstract  \\\n",
      "1048576  General: Online anti-vaccine articles contribu...   \n",
      "1048577  General: Although insulin analogs have markedl...   \n",
      "1048578  General: This qualitative study reflects an an...   \n",
      "1048582  General: Ecthyma gangrenosum (EG) is a serious...   \n",
      "1048588  General: The generation of induced pluripotent...   \n",
      "\n",
      "                                                   authors  \\\n",
      "1048576             Xu Zhan; Ellis Lauren; Umphrey Laura R   \n",
      "1048577  Disotuar Maria M; Chen Diao; Lin Nai-Pin; Chou...   \n",
      "1048578                                 Paxman Christina G   \n",
      "1048582  Fukui Kana Okazaki; Shoji Kensuke; Nagai Yusa;...   \n",
      "1048588  Skarnes William C; Pellegrino Enrica; McDonoug...   \n",
      "\n",
      "                                              affiliations  \\\n",
      "1048576  Northern Arizona University, Flagstaff, AZ, US...   \n",
      "1048577  Department of Biochemistry, University of Utah...   \n",
      "1048578                       Minot State University, USA.   \n",
      "1048582  Center of Postgraduate Education and Training,...   \n",
      "1048588  The Jackson Laboratory for Genomic Medicine, F...   \n",
      "\n",
      "                                                mesh_terms  \\\n",
      "1048576  Comprehension; Consumer Health Information; He...   \n",
      "1048577  Blood Glucose; Chemistry, Pharmaceutical; Diab...   \n",
      "1048578  Adult; Chronic Disease; Female; Fibromyalgia; ...   \n",
      "1048582  Anti-Bacterial Agents; Child, Preschool; Debri...   \n",
      "1048588  CRISPR-Cas Systems; Cell Culture Techniques; C...   \n",
      "\n",
      "                                                  keywords coi_statement  \\\n",
      "1048576  anti-vaccination; engagement; readability; vac...           N/A   \n",
      "1048577  bioconjugate; glucose-responsive insulin; glyc...           N/A   \n",
      "1048578  communication theory of identity; discourse; f...           N/A   \n",
      "1048582  Pseudomonas aeruginosa; ecthyma gangrenosum; h...           N/A   \n",
      "1048588                                                              N/A   \n",
      "\n",
      "               date  \n",
      "1048576  2019-06-01  \n",
      "1048577  2019-06-01  \n",
      "1048578  2019-06-01  \n",
      "1048582  2019-06-01  \n",
      "1048588  2019-06-01  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the dataset\n",
    "parquet_path = \"data/parquet_output/all_results.parquet\"\n",
    "df = pd.read_parquet(parquet_path)\n",
    "print(f\"Loaded {len(df)} rows from {parquet_path}.\")\n",
    "\n",
    "################################################################################\n",
    "# 2. Deduplicate by UID ONLY\n",
    "################################################################################\n",
    "df_dedup_uid = df.drop_duplicates(subset=[\"uid\"], keep=\"last\")\n",
    "rows_before = len(df)\n",
    "rows_after_uid = len(df_dedup_uid)\n",
    "removed_uid = rows_before - rows_after_uid\n",
    "\n",
    "print(\"\\n=== Dedup by [uid] only ===\")\n",
    "print(f\"Rows before: {rows_before}\")\n",
    "print(f\"Rows after:  {rows_after_uid}\")\n",
    "print(f\"Rows removed: {removed_uid}\")\n",
    "\n",
    "# Which rows were removed in the UID-only approach?\n",
    "removed_mask_uid = ~df.index.isin(df_dedup_uid.index)\n",
    "df_removed_uid = df[removed_mask_uid]\n",
    "print(f\"\\nRows actually removed when deduplicating by [uid]: {len(df_removed_uid)}\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# 3. Deduplicate by MULTI-COLUMNS\n",
    "################################################################################\n",
    "dup_columns = [\"uid\", \"abstract\", \"title\", \"pubdate\", \"mesh_terms\", \"journal\"]\n",
    "df_dedup_multi = df.drop_duplicates(subset=dup_columns, keep=\"last\")\n",
    "rows_after_multi = len(df_dedup_multi)\n",
    "removed_multi = rows_before - rows_after_multi\n",
    "\n",
    "print(f\"\\n=== Dedup by {dup_columns} ===\")\n",
    "print(f\"Rows before: {rows_before}\")\n",
    "print(f\"Rows after:  {rows_after_multi}\")\n",
    "print(f\"Rows removed: {removed_multi}\")\n",
    "\n",
    "# Which rows were removed in the multi-column approach?\n",
    "removed_mask_multi = ~df.index.isin(df_dedup_multi.index)\n",
    "df_removed_multi = df[removed_mask_multi]\n",
    "print(f\"\\nRows actually removed when deduplicating by {dup_columns}: {len(df_removed_multi)}\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# 4. Compare the Removed Sets\n",
    "################################################################################\n",
    "# - Rows removed by UID dedup only\n",
    "# - Rows removed by MULTI-COL dedup only\n",
    "# - Rows removed by BOTH approaches\n",
    "\n",
    "uid_removed_indices = set(df_removed_uid.index)\n",
    "multi_removed_indices = set(df_removed_multi.index)\n",
    "\n",
    "# Removed by UID but not by multi-col\n",
    "uid_only = uid_removed_indices - multi_removed_indices\n",
    "df_uid_only = df.loc[list(uid_only)]\n",
    "\n",
    "# Removed by multi-col but not by UID\n",
    "multi_only = multi_removed_indices - uid_removed_indices\n",
    "df_multi_only = df.loc[list(multi_only)]\n",
    "\n",
    "# Removed by BOTH\n",
    "both = uid_removed_indices & multi_removed_indices\n",
    "df_both = df.loc[list(both)]\n",
    "\n",
    "print(\"\\n=== Comparison of removed rows ===\")\n",
    "print(f\"Removed only by [uid] dedup:        {len(df_uid_only)}\")\n",
    "print(f\"Removed only by {dup_columns} dedup: {len(df_multi_only)}\")\n",
    "print(f\"Removed by BOTH methods:            {len(df_both)}\")\n",
    "\n",
    "# Optional: show a few examples\n",
    "print(\"\\nExample rows removed only by [uid] approach (first 5):\")\n",
    "print(df_uid_only.head(5))\n",
    "\n",
    "print(\"\\nExample rows removed only by multi-col approach (first 5):\")\n",
    "print(df_multi_only.head(5))\n",
    "\n",
    "print(\"\\nExample rows removed by BOTH (first 5):\")\n",
    "print(df_both.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
